<!doctype html>
<html>
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Voice Orb</title>
    <style>
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }

      html,
      body {
        width: 100%;
        height: 100%;
        background: transparent;
        overflow: hidden;
        /* Make transparent areas click-through to apps behind */
        pointer-events: none;
        /* Removed -webkit-app-region: drag - we handle dragging manually */
      }

      .orb-container {
        position: absolute;
        bottom: 20px;
        right: 20px;
        width: 80px;
        height: 80px;
        transition:
          top 0.25s ease,
          bottom 0.25s ease,
          left 0.25s ease,
          right 0.25s ease;
        /* Re-enable mouse events for the orb */
        pointer-events: auto;
      }

      /* Orb position variants for chat panel layout */
      .orb-container.orb-bottom-right {
        bottom: 20px;
        right: 20px;
        top: auto;
        left: auto;
      }

      .orb-container.orb-bottom-left {
        bottom: 20px;
        left: 20px;
        top: auto;
        right: auto;
      }

      .orb-container.orb-top-right {
        top: 20px;
        right: 20px;
        bottom: auto;
        left: auto;
      }

      .orb-container.orb-top-left {
        top: 20px;
        left: 20px;
        bottom: auto;
        right: auto;
      }

      .orb {
        width: 80px;
        height: 80px;
        border-radius: 50%;
        /* Use OneReach orb graphic (with gradient fallback if asset missing) */
        background: radial-gradient(circle at 35% 35%, #5b8def, #2a5cba 50%, #1a3a7a);
        background-image: url('assets/banner.png');
        background-size: cover;
        background-position: center;
        cursor: grab;
        display: flex;
        align-items: center;
        justify-content: center;
        -webkit-app-region: no-drag;
        position: relative;
        /* No glow by default - clean look */
        box-shadow: none;
        transition:
          transform 0.3s ease,
          filter 0.3s ease;
        animation: orb-float 6s ease-in-out infinite;
      }

      .orb:hover {
        transform: scale(1.1);
        filter: brightness(1.1);
      }

      .orb.listening {
        box-shadow: 0 0 8px rgba(234, 179, 8, 0.5);
        animation:
          orb-float 4s ease-in-out infinite,
          orb-breathe 1.5s ease-in-out infinite;
      }

      .orb.processing {
        box-shadow: 0 0 8px rgba(249, 115, 22, 0.5);
        animation:
          orb-float 3s ease-in-out infinite,
          orb-pulse 0.8s ease-in-out infinite;
      }

      .orb.connecting {
        box-shadow: 0 0 8px rgba(59, 130, 246, 0.5);
        animation:
          orb-float 4s ease-in-out infinite,
          orb-connect-pulse 1.2s ease-in-out infinite;
      }

      .orb.speaking {
        box-shadow: 0 0 10px rgba(147, 51, 234, 0.5);
        animation:
          orb-float 4s ease-in-out infinite,
          orb-speak-pulse 0.6s ease-in-out infinite;
      }

      .orb.awaiting {
        box-shadow: 0 0 8px rgba(245, 158, 11, 0.5);
        animation:
          orb-float 4s ease-in-out infinite,
          orb-await-blink 2s ease-in-out infinite;
      }

      .orb.error {
        box-shadow: 0 0 8px rgba(239, 68, 68, 0.5);
        animation: orb-shake 0.5s ease-in-out;
      }

      /* Gentle floating motion */
      @keyframes orb-float {
        0%,
        100% {
          transform: translate(0, 0);
        }
        25% {
          transform: translate(2px, -3px);
        }
        50% {
          transform: translate(-1px, -1px);
        }
        75% {
          transform: translate(-2px, -2px);
        }
      }

      /* Breathing glow when listening */
      @keyframes orb-breathe {
        0%,
        100% {
          box-shadow: 0 0 6px rgba(234, 179, 8, 0.4);
          filter: brightness(1);
        }
        50% {
          box-shadow: 0 0 12px rgba(234, 179, 8, 0.6);
          filter: brightness(1.1);
        }
      }

      /* Pulse when processing */
      @keyframes orb-pulse {
        0%,
        100% {
          transform: scale(1);
        }
        50% {
          transform: scale(1.05);
        }
      }

      /* Shake on error */
      @keyframes orb-shake {
        0%,
        100% {
          transform: translateX(0);
        }
        20% {
          transform: translateX(-4px);
        }
        40% {
          transform: translateX(4px);
        }
        60% {
          transform: translateX(-4px);
        }
        80% {
          transform: translateX(4px);
        }
      }

      /* Blue pulse when connecting */
      @keyframes orb-connect-pulse {
        0%,
        100% {
          box-shadow: 0 0 6px rgba(59, 130, 246, 0.3);
          filter: brightness(1);
        }
        50% {
          box-shadow: 0 0 14px rgba(59, 130, 246, 0.6);
          filter: brightness(1.05);
        }
      }

      /* Quick rhythmic pulse when speaking */
      @keyframes orb-speak-pulse {
        0%,
        100% {
          transform: scale(1);
          box-shadow: 0 0 8px rgba(147, 51, 234, 0.4);
        }
        50% {
          transform: scale(1.04);
          box-shadow: 0 0 14px rgba(147, 51, 234, 0.65);
        }
      }

      /* Slow blink when awaiting input */
      @keyframes orb-await-blink {
        0%,
        100% {
          box-shadow: 0 0 6px rgba(245, 158, 11, 0.3);
          filter: brightness(1);
        }
        50% {
          box-shadow: 0 0 12px rgba(245, 158, 11, 0.6);
          filter: brightness(1.08);
        }
      }

      /* Ring effects - hidden by default, only show when listening */
      .ring,
      .ring-secondary {
        display: none;
        position: absolute;
        top: 50%;
        left: 50%;
        transform: translate(-50%, -50%);
        width: 80px;
        height: 80px;
        border-radius: 50%;
        pointer-events: none;
      }

      .ring {
        border: 1px solid rgba(234, 179, 8, 0.4);
      }

      .orb.listening + .ring {
        display: block;
        border-color: rgba(234, 179, 8, 0.4);
        animation: ring-expand 2s ease-out infinite;
      }

      .orb.connecting + .ring {
        display: block;
        border-color: rgba(59, 130, 246, 0.4);
        animation: ring-expand 1.5s ease-out infinite;
      }

      .orb.speaking + .ring {
        display: block;
        border-color: rgba(147, 51, 234, 0.35);
        animation: ring-expand 1s ease-out infinite;
      }

      .orb.awaiting + .ring {
        display: block;
        border-color: rgba(245, 158, 11, 0.3);
        animation: ring-expand 3s ease-out infinite;
      }

      @keyframes ring-expand {
        0% {
          transform: translate(-50%, -50%) scale(1);
          opacity: 0.5;
        }
        100% {
          transform: translate(-50%, -50%) scale(1.6);
          opacity: 0;
        }
      }

      /* Mic overlay icon (subtle, appears on hover or when idle) */
      .mic-overlay {
        position: absolute;
        top: 50%;
        left: 50%;
        transform: translate(-50%, -50%);
        width: 24px;
        height: 24px;
        opacity: 0;
        transition: opacity 0.3s ease;
        pointer-events: none;
      }

      .mic-overlay svg {
        width: 100%;
        height: 100%;
        fill: rgba(255, 255, 255, 0.9);
        filter: drop-shadow(0 2px 4px rgba(0, 0, 0, 0.3));
      }

      .orb:hover .mic-overlay {
        opacity: 0.8;
      }

      .orb.listening .mic-overlay {
        opacity: 0;
      }

      /* Transcript tooltip - elegant floating text */
      /* Uses fixed positioning for reliable placement regardless of orb position */
      .transcript-tooltip {
        position: fixed;
        background: linear-gradient(135deg, rgba(15, 15, 20, 0.95) 0%, rgba(25, 25, 35, 0.9) 100%);
        color: rgba(255, 255, 255, 0.95);
        padding: 16px 22px;
        border-radius: 16px;
        font-size: 15px;
        font-weight: 400;
        letter-spacing: 0.2px;
        line-height: 1.6;
        font-family: -apple-system, BlinkMacSystemFont, 'SF Pro Text', 'Segoe UI', Roboto, sans-serif;
        max-width: 320px;
        max-height: 200px;
        overflow-y: hidden; /* Inner wrapper handles scrolling */
        text-align: left;
        white-space: normal;
        word-wrap: break-word;
        opacity: 0;
        /* Allow clicks through when hidden, enable when visible */
        pointer-events: none;
        -webkit-app-region: no-drag;
        /* Frosted glass effect */
        backdrop-filter: blur(20px);
        -webkit-backdrop-filter: blur(20px);
        /* Subtle border glow */
        border: 1px solid rgba(255, 255, 255, 0.1);
        box-shadow:
          0 8px 32px rgba(0, 0, 0, 0.4),
          0 0 0 1px rgba(255, 255, 255, 0.05),
          inset 0 1px 0 rgba(255, 255, 255, 0.1);
        /* Default position - will be overridden by JS */
        bottom: 120px;
        right: 20px;
        left: auto;
        top: auto;
        /* Animation properties */
        transform: translateY(10px);
        transition:
          opacity 0.4s ease,
          transform 0.4s cubic-bezier(0.4, 0, 0.2, 1);
        z-index: 100;
      }

      /* Inner scroll wrapper for auto-scroll effect */
      .transcript-scroll-inner {
        max-height: 168px; /* 200px container - 32px padding */
        overflow-y: auto;
        scrollbar-width: none; /* Firefox */
        -ms-overflow-style: none; /* IE/Edge */
        scroll-behavior: smooth;
      }

      .transcript-scroll-inner::-webkit-scrollbar {
        display: none;
      }

      /* Gradient fade at top when scrolled down */
      .transcript-tooltip.has-overflow-top::before {
        content: '';
        position: absolute;
        top: 0;
        left: 0;
        right: 0;
        height: 28px;
        background: linear-gradient(to bottom, rgba(15, 15, 20, 0.95) 0%, transparent 100%);
        border-radius: 16px 16px 0 0;
        pointer-events: none;
        z-index: 1;
      }

      /* Gradient fade at bottom when more content below */
      .transcript-tooltip.has-overflow-bottom::after {
        content: '';
        position: absolute;
        bottom: 0;
        left: 0;
        right: 0;
        height: 28px;
        background: linear-gradient(to top, rgba(15, 15, 20, 0.95) 0%, transparent 100%);
        border-radius: 0 0 16px 16px;
        pointer-events: none;
        z-index: 1;
      }

      .transcript-tooltip.visible {
        opacity: 1;
        transform: translateY(0);
      }

      /* Fading out state */
      .transcript-tooltip.fading {
        opacity: 0;
        transform: translateY(-10px);
        transition:
          opacity 0.8s ease-out,
          transform 0.8s ease-out;
      }

      .transcript-tooltip.interim {
        color: rgba(180, 180, 200, 0.9);
        font-style: italic;
        font-weight: 300;
      }

      /* Agent speaking - distinct style */
      .transcript-tooltip.agent-speaking {
        background: linear-gradient(135deg, rgba(20, 40, 60, 0.95) 0%, rgba(30, 50, 70, 0.9) 100%);
        border-left: 3px solid rgba(100, 180, 255, 0.7);
        color: rgba(200, 230, 255, 0.95);
      }

      /* Position below orb when near top of window */
      .transcript-tooltip.below {
        transform: translateY(-10px);
      }

      .transcript-tooltip.below.visible {
        transform: translateY(0);
      }

      .transcript-tooltip.below.fading {
        transform: translateY(10px);
      }

      /* Position to the left of orb (slides in from the right toward orb) */
      .transcript-tooltip.left-of-orb {
        transform: translateX(10px);
      }

      .transcript-tooltip.left-of-orb.visible {
        transform: translateX(0);
      }

      .transcript-tooltip.left-of-orb.fading {
        transform: translateX(10px);
        transition:
          opacity 0.8s ease-out,
          transform 0.8s ease-out;
      }

      /* Position to the right of orb (slides in from the left toward orb) */
      .transcript-tooltip.right-of-orb {
        transform: translateX(-10px);
      }

      .transcript-tooltip.right-of-orb.visible {
        transform: translateX(0);
      }

      .transcript-tooltip.right-of-orb.fading {
        transform: translateX(-10px);
        transition:
          opacity 0.8s ease-out,
          transform 0.8s ease-out;
      }

      /* ==================== CONTEXT MENU ==================== */
      .orb-context-menu {
        position: fixed;
        top: 50px;
        left: 50px;
        background: linear-gradient(135deg, rgba(20, 20, 30, 0.98) 0%, rgba(30, 30, 45, 0.95) 100%);
        border: 1px solid rgba(255, 255, 255, 0.1);
        border-radius: 12px;
        padding: 6px;
        min-width: 160px;
        box-shadow:
          0 12px 40px rgba(0, 0, 0, 0.5),
          0 0 0 1px rgba(255, 255, 255, 0.05),
          inset 0 1px 0 rgba(255, 255, 255, 0.1);
        backdrop-filter: blur(20px);
        -webkit-backdrop-filter: blur(20px);
        opacity: 0;
        transform: scale(0.95) translateY(-5px);
        transform-origin: top left;
        transition:
          opacity 0.15s ease,
          transform 0.15s ease;
        /* Click-through when hidden, enabled when visible */
        pointer-events: none;
        z-index: 1000;
        -webkit-app-region: no-drag;
      }

      .orb-context-menu.visible {
        opacity: 1;
        transform: scale(1) translateY(0);
        pointer-events: auto;
      }

      .context-menu-item {
        display: flex;
        align-items: center;
        gap: 10px;
        padding: 10px 14px;
        color: rgba(255, 255, 255, 0.9);
        font-size: 13px;
        font-family: -apple-system, BlinkMacSystemFont, 'SF Pro Text', sans-serif;
        border-radius: 8px;
        cursor: pointer;
        transition: background 0.15s ease;
        -webkit-app-region: no-drag;
      }

      .context-menu-item:hover {
        background: rgba(255, 255, 255, 0.1);
      }

      .context-menu-item svg {
        width: 16px;
        height: 16px;
        fill: currentColor;
        opacity: 0.8;
      }

      .context-menu-divider {
        height: 1px;
        background: rgba(255, 255, 255, 0.1);
        margin: 6px 8px;
      }

      /* ==================== TEXT CHAT PANEL ==================== */
      .text-chat-panel {
        position: fixed;
        top: 15px;
        left: 15px;
        right: 15px;
        bottom: 120px;
        background: linear-gradient(135deg, rgba(15, 15, 25, 0.98) 0%, rgba(25, 25, 40, 0.95) 100%);
        border: 1px solid rgba(255, 255, 255, 0.1);
        border-radius: 16px;
        box-shadow:
          0 20px 60px rgba(0, 0, 0, 0.5),
          0 0 0 1px rgba(255, 255, 255, 0.05),
          inset 0 1px 0 rgba(255, 255, 255, 0.1);
        backdrop-filter: blur(30px);
        -webkit-backdrop-filter: blur(30px);
        display: flex;
        flex-direction: column;
        opacity: 0;
        transform: scale(0.95) translateY(10px);
        transform-origin: bottom center;
        transition:
          opacity 0.25s ease,
          transform 0.25s cubic-bezier(0.4, 0, 0.2, 1);
        pointer-events: none;
        z-index: 500;
        -webkit-app-region: no-drag;
        overflow: hidden;
      }

      .text-chat-panel.visible {
        opacity: 1;
        transform: scale(1) translateY(0);
        pointer-events: auto;
      }

      .chat-header {
        display: flex;
        align-items: center;
        justify-content: space-between;
        padding: 16px 20px;
        border-bottom: 1px solid rgba(255, 255, 255, 0.08);
      }

      .chat-header-title {
        display: flex;
        align-items: center;
        gap: 10px;
        color: rgba(255, 255, 255, 0.95);
        font-size: 14px;
        font-weight: 500;
        font-family: -apple-system, BlinkMacSystemFont, 'SF Pro Text', sans-serif;
      }

      .chat-header-title svg {
        width: 18px;
        height: 18px;
        fill: currentColor;
        opacity: 0.8;
      }

      .chat-close-btn {
        width: 28px;
        height: 28px;
        border-radius: 8px;
        border: none;
        background: rgba(255, 255, 255, 0.05);
        color: rgba(255, 255, 255, 0.6);
        cursor: pointer;
        display: flex;
        align-items: center;
        justify-content: center;
        transition: all 0.15s ease;
      }

      .chat-close-btn:hover {
        background: rgba(255, 255, 255, 0.1);
        color: rgba(255, 255, 255, 0.9);
      }

      .chat-close-btn svg {
        width: 14px;
        height: 14px;
        fill: currentColor;
      }

      .chat-messages {
        flex: 1;
        overflow-y: auto;
        padding: 16px;
        display: flex;
        flex-direction: column;
        gap: 12px;
        min-height: 100px;
      }

      .chat-messages::-webkit-scrollbar {
        width: 6px;
      }

      .chat-messages::-webkit-scrollbar-track {
        background: transparent;
      }

      .chat-messages::-webkit-scrollbar-thumb {
        background: rgba(255, 255, 255, 0.15);
        border-radius: 3px;
      }

      .chat-message {
        padding: 12px 16px;
        border-radius: 16px;
        font-size: 13px;
        line-height: 1.5;
        font-family: -apple-system, BlinkMacSystemFont, 'SF Pro Text', sans-serif;
        max-width: 85%;
        animation: message-appear 0.2s ease;
      }

      @keyframes message-appear {
        from {
          opacity: 0;
          transform: translateY(8px);
        }
        to {
          opacity: 1;
          transform: translateY(0);
        }
      }

      .chat-message.user {
        align-self: flex-end;
        background: linear-gradient(135deg, #6366f1 0%, #8b5cf6 100%);
        color: white;
        border-bottom-right-radius: 6px;
      }

      .chat-message.assistant {
        align-self: flex-start;
        background: rgba(255, 255, 255, 0.08);
        color: rgba(255, 255, 255, 0.9);
        border-bottom-left-radius: 6px;
      }

      .chat-message.system {
        align-self: center;
        background: transparent;
        color: rgba(255, 255, 255, 0.5);
        font-size: 12px;
        padding: 8px;
      }

      .chat-input-container {
        padding: 16px;
        border-top: 1px solid rgba(255, 255, 255, 0.08);
      }

      .chat-input-wrapper {
        display: flex;
        align-items: center;
        gap: 10px;
        background: rgba(255, 255, 255, 0.06);
        border: 1px solid rgba(255, 255, 255, 0.1);
        border-radius: 14px;
        padding: 4px 4px 4px 16px;
        transition: all 0.15s ease;
      }

      .chat-input-wrapper:focus-within {
        border-color: rgba(139, 92, 246, 0.5);
        box-shadow: 0 0 0 3px rgba(139, 92, 246, 0.15);
      }

      .chat-input {
        flex: 1;
        background: transparent;
        border: none;
        outline: none;
        color: rgba(255, 255, 255, 0.95);
        font-size: 14px;
        font-family: -apple-system, BlinkMacSystemFont, 'SF Pro Text', sans-serif;
        padding: 10px 0;
      }

      .chat-input::placeholder {
        color: rgba(255, 255, 255, 0.35);
      }

      .chat-send-btn {
        width: 36px;
        height: 36px;
        border-radius: 10px;
        border: none;
        background: linear-gradient(135deg, #6366f1 0%, #8b5cf6 100%);
        color: white;
        cursor: pointer;
        display: flex;
        align-items: center;
        justify-content: center;
        transition: all 0.15s ease;
        flex-shrink: 0;
      }

      .chat-send-btn:hover {
        transform: scale(1.05);
        box-shadow: 0 4px 12px rgba(139, 92, 246, 0.4);
      }

      .chat-send-btn:active {
        transform: scale(0.95);
      }

      .chat-send-btn:disabled {
        opacity: 0.5;
        cursor: not-allowed;
        transform: none;
      }

      .chat-send-btn svg {
        width: 16px;
        height: 16px;
        fill: currentColor;
      }

      /* Position variants based on orb location on screen */
      /* Default: orb at bottom-right, chat appears above/left */
      .text-chat-panel.pos-top-left {
        /* Orb at bottom-right: chat above and to the left */
        top: 15px;
        left: 15px;
        right: 15px;
        bottom: 120px;
      }

      .text-chat-panel.pos-top-right {
        /* Orb at bottom-left: chat above and to the right */
        top: 15px;
        left: 15px;
        right: 15px;
        bottom: 120px;
      }

      .text-chat-panel.pos-bottom-left {
        /* Orb at top-right: chat below and to the left */
        top: 120px;
        left: 15px;
        right: 15px;
        bottom: 15px;
        transform-origin: top center;
      }

      .text-chat-panel.pos-bottom-left.visible {
        transform: scale(1) translateY(0);
      }

      .text-chat-panel.pos-bottom-right {
        /* Orb at top-left: chat below and to the right */
        top: 120px;
        left: 15px;
        right: 15px;
        bottom: 15px;
        transform-origin: top center;
      }

      .text-chat-panel.pos-bottom-right.visible {
        transform: scale(1) translateY(0);
      }

      /* Empty state */
      .chat-empty {
        display: flex;
        flex-direction: column;
        align-items: center;
        justify-content: center;
        height: 100%;
        padding: 40px 20px;
        color: rgba(255, 255, 255, 0.4);
        text-align: center;
      }

      .chat-empty svg {
        width: 48px;
        height: 48px;
        fill: currentColor;
        margin-bottom: 16px;
        opacity: 0.5;
      }

      .chat-empty-text {
        font-size: 13px;
        line-height: 1.6;
      }
    </style>
  </head>
  <body>
    <!-- Context Menu - outside container for proper positioning -->
    <div class="orb-context-menu" id="contextMenu">
      <div class="context-menu-item" id="menuTextChat">
        <svg viewBox="0 0 24 24">
          <path d="M20 2H4c-1.1 0-2 .9-2 2v18l4-4h14c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm0 14H6l-2 2V4h16v12z" />
        </svg>
        <span>Text Chat</span>
      </div>
      <div class="context-menu-item" id="menuVoice">
        <svg viewBox="0 0 24 24">
          <path
            d="M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3zm-1-9c0-.55.45-1 1-1s1 .45 1 1v6c0 .55-.45 1-1 1s-1-.45-1-1V5zm6 6c0 2.76-2.24 5-5 5s-5-2.24-5-5H5c0 3.53 2.61 6.43 6 6.92V21h2v-3.08c3.39-.49 6-3.39 6-6.92h-2z"
          />
        </svg>
        <span>Voice Mode</span>
      </div>
      <div class="context-menu-divider"></div>
      <div class="context-menu-item" id="menuSettings">
        <svg viewBox="0 0 24 24">
          <path
            d="M19.14 12.94c.04-.31.06-.63.06-.94 0-.31-.02-.63-.06-.94l2.03-1.58c.18-.14.23-.41.12-.61l-1.92-3.32c-.12-.22-.37-.29-.59-.22l-2.39.96c-.5-.38-1.03-.7-1.62-.94l-.36-2.54c-.04-.24-.24-.41-.48-.41h-3.84c-.24 0-.43.17-.47.41l-.36 2.54c-.59.24-1.13.57-1.62.94l-2.39-.96c-.22-.08-.47 0-.59.22L2.74 8.87c-.12.21-.08.47.12.61l2.03 1.58c-.04.31-.06.63-.06.94s.02.63.06.94l-2.03 1.58c-.18.14-.23.41-.12.61l1.92 3.32c.12.22.37.29.59.22l2.39-.96c.5.38 1.03.7 1.62.94l.36 2.54c.05.24.24.41.48.41h3.84c.24 0 .44-.17.47-.41l.36-2.54c.59-.24 1.13-.56 1.62-.94l2.39.96c.22.08.47 0 .59-.22l1.92-3.32c.12-.22.07-.47-.12-.61l-2.01-1.58zM12 15.6c-1.98 0-3.6-1.62-3.6-3.6s1.62-3.6 3.6-3.6 3.6 1.62 3.6 3.6-1.62 3.6-3.6 3.6z"
          />
        </svg>
        <span>Settings</span>
      </div>
    </div>

    <!-- Text Chat Panel - outside container for proper positioning -->

    <div class="orb-container">
      <div class="orb" id="orb" title="Click to start listening">
        <div class="mic-overlay">
          <svg viewBox="0 0 24 24">
            <path
              d="M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3zm-1-9c0-.55.45-1 1-1s1 .45 1 1v6c0 .55-.45 1-1 1s-1-.45-1-1V5zm6 6c0 2.76-2.24 5-5 5s-5-2.24-5-5H5c0 3.53 2.61 6.43 6 6.92V21h2v-3.08c3.39-.49 6-3.39 6-6.92h-2z"
            />
          </svg>
        </div>
      </div>
      <div class="ring"></div>
      <div class="ring-secondary"></div>
      <div class="transcript-tooltip" id="transcript">
        <div class="transcript-scroll-inner" id="transcript-inner"></div>
      </div>
    </div>

    <!-- Text Chat Panel -->
    <div class="text-chat-panel" id="textChatPanel">
      <div class="chat-header">
        <div class="chat-header-title">
          <svg viewBox="0 0 24 24">
            <path d="M20 2H4c-1.1 0-2 .9-2 2v18l4-4h14c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm0 14H6l-2 2V4h16v12z" />
          </svg>
          <span>Chat</span>
        </div>
        <button class="chat-close-btn" id="chatCloseBtn">
          <svg viewBox="0 0 24 24">
            <path
              d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"
            />
          </svg>
        </button>
      </div>
      <div class="chat-messages" id="chatMessages">
        <div class="chat-empty">
          <svg viewBox="0 0 24 24">
            <path d="M20 2H4c-1.1 0-2 .9-2 2v18l4-4h14c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm0 14H6l-2 2V4h16v12z" />
          </svg>
          <div class="chat-empty-text">
            Type a command or ask a question.<br />
            Same as voice, just quieter.
          </div>
        </div>
      </div>
      <div class="chat-input-container">
        <div class="chat-input-wrapper">
          <input type="text" class="chat-input" id="chatInput" placeholder="Type a message..." autocomplete="off" />
          <button class="chat-send-btn" id="chatSendBtn">
            <svg viewBox="0 0 24 24"><path d="M2.01 21L23 12 2.01 3 2 10l15 2-15 2z" /></svg>
          </button>
        </div>
      </div>
    </div>

    <!-- Extracted modules -->
    <script src="./lib/orb/orb-state.js"></script>
    <script src="./lib/orb/orb-audio.js"></script>
    <script src="./lib/orb/orb-event-router.js"></script>

    <script>
      // ==================== STATE (managed by OrbState v2) ====================
      const S = window.OrbState;

      // Audio capture resources (local to mic capture functions)
      let audioContext = null;
      let mediaStream = null;
      let processor = null;

      // Generation counter for stale-event detection (kept for debug logging)
      let _listenerGeneration = 0;

      // Wire OrbAudio speaking-end callback to transition state
      if (window.OrbAudio) {
        window.OrbAudio.setOnSpeakingEnd(() => {
          S.set('ttsEndTime', Date.now());
          const hasActiveWork = _activeTaskId || (S.get('pendingSubmitCount') || 0) > 0;
          // If awaiting input, transition to listening (multi-turn re-listen)
          if (S.isAwaitingInput) {
            S.transition('listening', 'tts-complete-followup');
          } else if (_pendingNeedsInput && S.isSpeaking) {
            // Clarification or multi-turn follow-up: wait for user response
            _pendingNeedsInput = false;
            S.transition('awaitingInput', 'needs-input');
            setTimeout(() => {
              if (S.isAwaitingInput) {
                S.transition('listening', 'followup-listen');
              }
            }, 500);
          } else if (S.isSpeaking && hasActiveWork) {
            // Task still executing or function call still processing
            // Go back to processing, don't disconnect
            console.log('[Orb] Interim speech done, work still active:', _activeTaskId || 'pending-submit');
            S.transition('processing', 'interim-speech-done');
          } else if (S.isSpeaking) {
            // No active task -- this was the final response
            S.transition('idle', 'tts-complete');
          }
        });
      }

      // Common noise/filler words to ignore (background noise often produces these)
      const NOISE_WORDS = new Set([
        'hmm',
        'hm',
        'um',
        'uh',
        'ah',
        'oh',
        'eh',
        'er',
        'mm',
        'mhm',
        'uh-huh',
        'the',
        'a',
        'i',
        'it',
        'is',
        'and',
        'but',
        'so',
        'like',
        'right',
        'alright',
        'well',
        'now',
        'just',
        // Common misheard noise
        'you',
        'me',
        'we',
        'he',
        'she',
        'they',
        'that',
        'this',
        'what',
        'huh',
        'wow',
        'ooh',
        'aah',
        'ugh',
      ]);

      // Valid short commands that should NOT be filtered (agents can handle these)
      const VALID_SHORT_COMMANDS = new Set([
        // Greetings (smalltalk agent)
        'hi',
        'hey',
        'hello',
        'yo',
        'sup',
        'bye',
        'goodbye',
        // Confirmations (multi-turn conversations)
        'yes',
        'yeah',
        'yep',
        'no',
        'nope',
        'ok',
        'okay',
        'sure',
        // Gratitude
        'thanks',
        'thank you',
        // Playback controls (DJ agent)
        'stop',
        'play',
        'pause',
        'skip',
        'next',
        'back',
        'mute',
        // Actions
        'help',
        'cancel',
        'undo',
        'done',
        'repeat',
      ]);

      // Check if transcript is likely just noise
      // NOTE: We now let more through - smalltalk agent handles unclear inputs naturally
      function isLikelyNoise(text) {
        if (!text) return true;

        const normalized = text.toLowerCase().trim();
        const stripped = normalized.replace(/[.,!?;:'"]/g, '').trim();

        // Allow valid short commands - these are intentional user input
        if (VALID_SHORT_COMMANDS.has(stripped)) return false;

        // Allow anything 4+ characters - let agents decide what to do with it
        // (smalltalk agent will handle gibberish with "Hmm?" type responses)
        if (stripped.length >= 4) return false;

        // Only filter truly empty or single-char noise
        if (stripped.length <= 1) return true;

        // Filter common 2-3 char noise that's definitely not intentional
        const definiteNoise = new Set(['um', 'uh', 'ah', 'eh', 'er', 'mm', 'hm']);
        if (definiteNoise.has(stripped)) return true;

        // Let everything else through to agents
        return false;
      }

      // Constants
      const FUNCTION_CALL_DEDUP_MS = 5000;
      const TTS_COOLDOWN_MS = 2500;
      const SILENCE_AFTER_SPEECH_MS = 5000;
      const NO_SPEECH_TIMEOUT_MS = 60000;

      // Speech timers (local, managed by start/clear functions)
      let silenceTimeoutId = null;
      let noSpeechTimeoutId = null;

      // Called when we detect the user is actively speaking (interim transcripts)
      function onSpeechDetected() {
        S.set('lastSpeechTime', Date.now());
        S.set('hasSpokenThisSession', true);
        clearSilenceTimeout();
        clearNoSpeechTimeout();
      }

      // Called when speech appears to have ended (final transcript received or silence)
      function onSpeechEnded() {
        clearSilenceTimeout();
        silenceTimeoutId = setTimeout(() => {
          if (S.isAwaitingInput) {
            console.log('[Orb] Silence timeout but awaiting input - keeping mic active');
            return;
          }
          if (S.isListening && !S.isSpeaking && S.get('hasSpokenThisSession')) {
            const timeSinceSpeech = Date.now() - (S.get('lastSpeechTime') || 0);
            if (timeSinceSpeech >= SILENCE_AFTER_SPEECH_MS) {
              console.log('[Orb] Silence after speech - auto-stopping');
              stopListening();
            }
          }
        }, SILENCE_AFTER_SPEECH_MS);
      }

      function clearSilenceTimeout() {
        if (silenceTimeoutId) {
          clearTimeout(silenceTimeoutId);
          silenceTimeoutId = null;
        }
      }

      // No speech at all timeout - in case user activates but never speaks
      function startNoSpeechTimeout() {
        clearNoSpeechTimeout();
        noSpeechTimeoutId = setTimeout(() => {
          if (S.isAwaitingInput) {
            console.log('[Orb] No-speech timeout but awaiting input - keeping mic active');
            return;
          }
          if (S.isListening && !S.isSpeaking && !S.get('hasSpokenThisSession')) {
            console.log('[Orb] No speech detected - auto-stopping');
            stopListening();
          }
        }, NO_SPEECH_TIMEOUT_MS);
      }

      function clearNoSpeechTimeout() {
        if (noSpeechTimeoutId) {
          clearTimeout(noSpeechTimeoutId);
          noSpeechTimeoutId = null;
        }
      }

      function clearAllSpeechTimers() {
        clearSilenceTimeout();
        clearNoSpeechTimeout();
      }

      // Audio functions delegated to OrbAudio module (lib/orb/orb-audio.js)
      const initTTSAudio = () => window.OrbAudio?.initTTSAudio();
      const ensureTTSAudioReady = () => window.OrbAudio?.ensureTTSAudioReady() ?? false;
      const base64ToFloat32 = (b64) => window.OrbAudio?.base64ToFloat32(b64);
      const playWAVAudio = (b64) => window.OrbAudio?.playWAVAudio(b64);
      const playTTSAudio = () => window.OrbAudio?.playTTSAudio();
      const playReadyChime = () => window.OrbAudio?.playReadyChime();

      // ==================== AUDIO CAPTURE FUNCTIONS ====================
      function startAudioCapture() {
        // Audio capture is initiated during the connect phase
        // and started here when we transition to listening
        // Mic stream is already set up - just enable processing
        if (processor && audioContext) {
          console.log('[Orb] Audio capture already active');
        }
      }

      function stopAudioCapture() {
        if (processor) {
          try {
            processor.disconnect();
          } catch (_) {}
          processor = null;
        }
        if (audioContext) {
          try {
            audioContext.close();
          } catch (_) {}
          audioContext = null;
        }
        if (mediaStream) {
          mediaStream.getTracks().forEach((track) => track.stop());
          mediaStream = null;
        }
      }

      // ==================== DISAMBIGUATION STATE ====================
      let pendingDisambiguation = null;
      let disambiguationListenerRemove = null;

      /**
       * Handle disambiguation flow
       * @param {Object} result - Classification result with clarification data
       * @param {string} originalTranscript - The original user transcript
       */
      async function handleDisambiguation(result, originalTranscript) {
        console.log('[Orb] Starting disambiguation flow:', result);

        // Create disambiguation state
        const disambiguationState = {
          id: 'disamb_' + Date.now(),
          originalTranscript: originalTranscript,
          question: result.clarificationQuestion || 'What did you mean?',
          options: result.clarificationOptions || [],
          createdAt: Date.now(),
          expiresAt: Date.now() + 30000, // 30 second timeout
        };

        pendingDisambiguation = disambiguationState;

        // NOTE: Backend speaks via exchange-bridge.js
        console.log('[Orb] Disambiguation question (backend speaks):', disambiguationState.question);

        // Show transcript with question
        showTranscript(disambiguationState.question, false);

        // Set up listener for disambiguation response
        setupDisambiguationListeners();
      }

      /**
       * Set up listeners for disambiguation responses
       */
      function setupDisambiguationListeners() {
        // Clean up previous listener if any
        if (disambiguationListenerRemove) {
          disambiguationListenerRemove();
          disambiguationListenerRemove = null;
        }

        // Listen for option selection via HUD API disambiguation events
        // (The onDisambiguation listener in the main event section also handles this,
        //  but we keep a specific listener for the active disambiguation state here)
        if (window.agentHUD?.onDisambiguation) {
          disambiguationListenerRemove = window.agentHUD.onDisambiguation((state) => {
            if (state.type === 'selected' || state.optionIndex !== undefined) {
              console.log('[Orb] Disambiguation option selected via HUD API:', state);
              resolveDisambiguation({
                optionIndex: state.optionIndex,
                option: state.option,
                mergedTranscript: state.mergedTranscript,
              });
            }
          });
        }

        // Set timeout to cancel disambiguation
        setTimeout(() => {
          if (pendingDisambiguation) {
            console.log('[Orb] Disambiguation timed out');
            cancelDisambiguation();
            // NOTE: Backend handles timeout feedback
          }
        }, 30000);
      }

      /**
       * Resolve disambiguation with selected option
       * @param {Object} selection - { optionIndex, option, mergedTranscript }
       */
      async function resolveDisambiguation(selection) {
        if (!pendingDisambiguation) return;

        console.log('[Orb] Resolving disambiguation:', selection);

        const state = pendingDisambiguation;
        pendingDisambiguation = null;

        // Clean up listeners
        if (disambiguationListenerRemove) {
          disambiguationListenerRemove();
          disambiguationListenerRemove = null;
        }

        // Hide transcript
        hideTranscript();

        // If we got a selected option with action, submit the clarified request
        if (selection.option && (selection.option.action || selection.mergedTranscript)) {
          console.log('[Orb] Disambiguation selected - submitting via HUD API');

          try {
            // Submit the clarified transcript through the full pipeline
            const clarifiedText =
              selection.mergedTranscript || `${state.originalTranscript} (${selection.option.label})`;
            const result = await window.agentHUD.submitTask(clarifiedText, {
              toolId: 'orb',
              skipFilter: true, // Already validated by disambiguation
              metadata: {
                disambiguationResolved: true,
                originalTranscript: state.originalTranscript,
                clarification: selection.option.label,
              },
            });

            console.log('[Orb] Disambiguation action submitted via HUD API:', result);
            // Backend speaks result via exchange-bridge.js
          } catch (err) {
            console.error('[Orb] Error submitting disambiguation action:', err);
            // NOTE: Backend handles error feedback
          }
        } else if (selection.mergedTranscript) {
          // Re-classify with merged transcript
          console.log('[Orb] Re-trying with merged transcript');
          await processVoiceCommand(selection.mergedTranscript);
        }
      }

      /**
       * Cancel pending disambiguation
       */
      function cancelDisambiguation() {
        if (!pendingDisambiguation) return;

        console.log('[Orb] Cancelling disambiguation');

        // Save stateId before nulling the reference
        const stateId = pendingDisambiguation.stateId;
        pendingDisambiguation = null;

        if (disambiguationListenerRemove) {
          disambiguationListenerRemove();
          disambiguationListenerRemove = null;
        }

        hideTranscript();

        // Cancel via HUD API if we have a stateId
        if (window.agentHUD?.cancelDisambiguation && stateId) {
          try {
            window.agentHUD.cancelDisambiguation(stateId);
          } catch (e) {
            console.warn('[Orb] HUD API cancel disambiguation error:', e);
          }
        }
      }

      /**
       * Check if we're waiting for disambiguation
       */
      function isAwaitingDisambiguation() {
        return pendingDisambiguation !== null && pendingDisambiguation.expiresAt > Date.now();
      }

      // ==================== VOICE COMMAND PROCESSING ====================

      // ==================== VOICE COMMAND PROCESSING (via HUD API) ====================

      let _processCallCount = 0;
      async function processVoiceCommand(transcript) {
        _processCallCount++;
        console.log('[Orb] Processing command:', transcript);

        // Check if Agent Composer is active - relay voice to it
        if (window.orbAPI?.isComposerActive) {
          try {
            const composerActive = await window.orbAPI.isComposerActive();
            if (composerActive) {
              console.log('[Orb] Relaying to Agent Composer:', transcript);
              if (window.orbAPI.cancelResponse) {
                await window.orbAPI.cancelResponse();
              }
              const relayed = await window.orbAPI.relayToComposer(transcript);
              if (relayed) return;
            }
          } catch (e) {
            console.warn('[Orb] Could not check composer status:', e);
          }
        }

        // Check if this is a response to a pending disambiguation
        if (isAwaitingDisambiguation()) {
          console.log('[Orb] Processing as disambiguation response');
          const state = pendingDisambiguation;
          const normalizedResponse = transcript.toLowerCase().trim();

          const numberWords = {
            one: 0,
            first: 0,
            1: 0,
            two: 1,
            second: 1,
            2: 1,
            three: 2,
            third: 2,
            3: 2,
            four: 3,
            fourth: 3,
            4: 3,
            five: 4,
            fifth: 4,
            5: 4,
          };

          let matchedIndex = -1;
          for (const [word, index] of Object.entries(numberWords)) {
            if (normalizedResponse.includes(word) && index < state.options.length) {
              matchedIndex = index;
              break;
            }
          }

          if (matchedIndex === -1) {
            matchedIndex = state.options.findIndex(
              (opt) =>
                normalizedResponse.includes(opt.label.toLowerCase()) ||
                opt.label.toLowerCase().includes(normalizedResponse)
            );
          }

          if (matchedIndex >= 0) {
            // Use HUD API to select disambiguation option
            if (state.stateId && window.agentHUD) {
              try {
                await window.agentHUD.selectDisambiguationOption(state.stateId, matchedIndex);
              } catch (e) {
                console.warn('[Orb] HUD API disambiguation select failed, using legacy:', e);
              }
            }
            resolveDisambiguation({
              optionIndex: matchedIndex,
              option: state.options[matchedIndex],
              mergedTranscript: `${state.originalTranscript} (clarification: ${state.options[matchedIndex].label})`,
            });
          } else {
            resolveDisambiguation({
              mergedTranscript: `${state.originalTranscript} (user clarified: ${transcript})`,
            });
          }
          return;
        }

        try {
          // ==================== SUBMIT VIA HUD API ====================
          // The HUD API routes through the full exchange bridge pipeline:
          //   transcript filter -> dedup -> Router -> critical commands ->
          //   disambiguation -> exchange auction -> agent execution -> voice cues
          // Task-tool mapping is tracked so events route back to 'orb'.

          if (!window.agentHUD || !window.agentHUD.submitTask) {
            console.error('[Orb] window.agentHUD not available - check preload-orb.js');
            return;
          }

          const result = await window.agentHUD.submitTask(transcript, {
            toolId: 'orb',
            skipFilter: false,
          });
          console.log('[Orb] HUD API submit result:', JSON.stringify(result));

          // Handle error returns
          if (!result || result.error) {
            console.warn('[Orb] submitTask returned error:', result?.error);
            return;
          }

          // Backend handled AND suppressed AI response (async exchange path)
          if (result.suppressAIResponse === true) {
            console.log('[Orb] Response suppressed by backend (exchange events will speak)');
            hideTranscript();
            return;
          }

          // Backend handled but didn't suppress -- backend already spoke
          if (result.handled && result.message && result.suppressAIResponse === false) {
            console.log('[Orb] Backend handled with message:', result.message);
            hideTranscript();
            return;
          }

          // Disambiguation needed
          if (result.clarificationNeeded && result.clarificationOptions?.length > 0) {
            await handleDisambiguation(result, transcript);
            return;
          }

          // Transcript was filtered as garbled
          if (result.needsClarification && result.filterReason) {
            console.log('[Orb] Transcript filtered:', result.filterReason);
            hideTranscript();
            return;
          }

          if (result.queued && result.taskId) {
            // Task successfully queued -- backend handles voice cues
            console.log('[Orb] Task queued:', result.taskId);
          } else if (result.handled) {
            // Agent handled directly (follow-up, etc.)
            console.log('[Orb] Task handled by agent, needsInput:', result.needsInput);

            if (result.needsInput) {
              _pendingNeedsInput = true;
              console.log('[Orb] Multi-turn continues: flagging for follow-up listen');
            } else {
              _pendingNeedsInput = false;
              console.log('[Orb] Multi-turn complete');
            }
          } else {
            // No action recognized -- backend already handled response
            console.log('[Orb] No specific action, transcript recorded');
          }
        } catch (err) {
          console.error('[Orb] Submit error:', err?.message || err);

          // Speak error via backend TTS
          try {
            await window.orbAPI.speak('Sorry, something went wrong. ' + (err?.message || ''));
          } catch (speakErr) {
            console.warn('[Orb] Could not speak error:', speakErr);
          }
        }
      }

      // ==================== EVENT LISTENERS (HUD API only) ====================
      // All events come through the centralized HUD API.
      // The orb filters by toolId === 'orb' for tool-scoped events.
      if (window.agentHUD) {
        console.log('[Orb] Setting up HUD API event listeners');

        // Disambiguation events
        window.agentHUD.onDisambiguation((state) => {
          console.log('[Orb] HUD API disambiguation event:', state);
          if (!state.toolId || state.toolId === 'orb') {
            if (typeof handleDisambiguation === 'function' && state.options?.length > 0) {
              handleDisambiguation(
                {
                  clarificationNeeded: true,
                  clarificationOptions: state.options.map((o, i) => ({
                    label: o.label,
                    description: o.description,
                    index: i,
                  })),
                  stateId: state.stateId,
                },
                ''
              );
            }
          }
        });

        // Needs-input events (agent follow-up questions)
        window.agentHUD.onNeedsInput((request) => {
          console.log('[Orb] HUD API needs-input:', request);
          if (!request.toolId || request.toolId === 'orb') {
            _pendingNeedsInput = true;
            console.log('[Orb] Multi-turn: flagging for follow-up listen');
          }
        });

        // Lifecycle events (task:queued, task:assigned, task:settled, etc.)
        window.agentHUD.onLifecycle((event) => {
          console.log('[Orb] HUD API lifecycle:', event.type, event.taskId || '');

          // Track active task to prevent premature disconnect during interim speech
          if (event.type === 'task:queued' || event.type === 'task:assigned') {
            if (event.taskId) _activeTaskId = event.taskId;
          }

          // Handle task completion -- clear active task
          // NOTE: Do NOT clear _pendingNeedsInput here. task:settled races with
          // onNeedsInput via IPC delivery order. The needsInput flag is correctly
          // cleared after TTS finishes (in onSpeakingEnd and audio_done handlers)
          // and on idle entry. Clearing here causes the mic to turn off prematurely
          // when an agent asks a follow-up question.
          if (event.type === 'task:settled' || event.type === 'task:completed') {
            console.log('[Orb] Task settled, clearing active task:', _activeTaskId);
            _activeTaskId = null;
          }
          // Handle task failure / dead letter -- clear active task
          if (event.type === 'task:dead_letter' || event.type === 'task:route_to_error_agent') {
            console.log('[Orb] Task failed, clearing active task:', _activeTaskId);
            _activeTaskId = null;
          }
        });

        // Result events (final result from agent execution)
        // When a result carries HTML (graphical panel like calendar), suppress
        // transcript text display -- the panel IS the visual result.
        window.agentHUD.onResult((result) => {
          console.log('[Orb] HUD API result:', result.taskId, result.success, result.html ? '(has panel)' : '');

          // Clear active task when result arrives (task:settled may not reach renderer)
          if (result.taskId && result.taskId === _activeTaskId) {
            console.log('[Orb] Result received for active task, clearing:', _activeTaskId);
            _activeTaskId = null;
          }

          if (result.html) {
            S.set('hasActivePanel', true);
          } else {
            S.set('hasActivePanel', false);
          }

          // If the result has needsInput, flag for multi-turn
          if (result.needsInput) {
            _pendingNeedsInput = true;
          }
        });

        // Speech state changes (centralized mic gating)
        window.agentHUD.onSpeechState((speechState) => {
          // Transition to speaking when TTS starts (if not already)
          if (speechState.isSpeaking && !S.isSpeaking && S.phase !== 'idle') {
            S.transition('speaking', 'hud-speech-start');
          }
          if (!speechState.isSpeaking) {
            S.set('ttsEndTime', Date.now());
          }
        });
      }

      // DOM elements
      const orb = document.getElementById('orb');
      const transcriptEl = document.getElementById('transcript');
      const transcriptInner = document.getElementById('transcript-inner');
      const contextMenu = document.getElementById('contextMenu');
      const textChatPanel = document.getElementById('textChatPanel');
      const chatMessages = document.getElementById('chatMessages');

      const chatInput = document.getElementById('chatInput');
      const chatSendBtn = document.getElementById('chatSendBtn');
      const chatCloseBtn = document.getElementById('chatCloseBtn');

      // Update overflow indicators when user manually scrolls the transcript
      transcriptInner.addEventListener('scroll', _updateOverflowIndicators);

      // Text chat state
      let isTextChatOpen = false;
      let chatHistory = [];
      let chatInactivityTimer = null;
      const CHAT_INACTIVITY_TIMEOUT = 30000; // 30 seconds

      // Reset the chat inactivity timer
      function resetChatInactivityTimer() {
        if (chatInactivityTimer) {
          clearTimeout(chatInactivityTimer);
          chatInactivityTimer = null;
        }

        // Only set timer if chat is open
        if (isTextChatOpen) {
          chatInactivityTimer = setTimeout(() => {
            console.log('[TextChat] Auto-closing due to inactivity');
            closeTextChat();
          }, CHAT_INACTIVITY_TIMEOUT);
        }
      }

      // Clear inactivity timer
      function clearChatInactivityTimer() {
        if (chatInactivityTimer) {
          clearTimeout(chatInactivityTimer);
          chatInactivityTimer = null;
        }
      }

      // ==================== CONTEXT MENU ====================

      function showContextMenu(x, y) {
        console.log('[Orb] Showing context menu at window coords:', x, y);

        const MIN_MARGIN = 8;

        // Window dimensions (the BrowserWindow client area)
        const windowWidth = window.innerWidth;
        const windowHeight = window.innerHeight;

        // Window position on physical screen
        const windowX = window.screenX;
        const windowY = window.screenY;

        // Physical screen dimensions
        const screenWidth = window.screen.availWidth;
        const screenHeight = window.screen.availHeight;

        // Measure actual menu size (show off-screen first to get dimensions)
        contextMenu.style.left = '-9999px';
        contextMenu.style.top = '-9999px';
        contextMenu.classList.add('visible');

        const rect = contextMenu.getBoundingClientRect();
        const menuWidth = rect.width;
        const menuHeight = rect.height;

        // Calculate where menu would appear in screen coordinates
        let screenMenuX = windowX + x;
        let screenMenuY = windowY + y;

        console.log(
          '[Orb] Screen coords - window:',
          windowX,
          windowY,
          'click:',
          screenMenuX,
          screenMenuY,
          'screen size:',
          screenWidth,
          screenHeight,
          'menu size:',
          menuWidth,
          menuHeight
        );

        // Horizontal: check if menu would go off right edge of screen
        if (screenMenuX + menuWidth > screenWidth - MIN_MARGIN) {
          // Flip to left of cursor
          screenMenuX = Math.max(MIN_MARGIN, screenMenuX - menuWidth);
        }
        // Ensure not off left edge of screen
        if (screenMenuX < MIN_MARGIN) {
          screenMenuX = MIN_MARGIN;
        }

        // Vertical: check if menu would go off bottom edge of screen
        if (screenMenuY + menuHeight > screenHeight - MIN_MARGIN) {
          // Flip to above cursor
          screenMenuY = screenMenuY - menuHeight;
        }
        // Ensure not off top edge of screen
        if (screenMenuY < MIN_MARGIN) {
          screenMenuY = MIN_MARGIN;
        }

        // Convert back to window-relative coordinates
        let finalX = screenMenuX - windowX;
        let finalY = screenMenuY - windowY;

        // Also ensure menu stays within window bounds (for very edge cases)
        finalX = Math.max(0, Math.min(finalX, windowWidth - menuWidth));
        finalY = Math.max(0, Math.min(finalY, windowHeight - menuHeight));

        // Apply final position
        contextMenu.style.position = 'fixed';
        contextMenu.style.left = finalX + 'px';
        contextMenu.style.top = finalY + 'px';
        contextMenu.style.bottom = 'auto';
        contextMenu.style.right = 'auto';

        console.log('[Orb] Context menu final position:', finalX, finalY);
      }

      function hideContextMenu() {
        contextMenu.classList.remove('visible');
      }

      // Context menu handlers
      document.getElementById('menuTextChat').addEventListener('click', () => {
        hideContextMenu();
        openTextChat();
      });

      document.getElementById('menuVoice').addEventListener('click', () => {
        hideContextMenu();
        closeTextChat();
        if (S.phase === 'idle') {
          startListening();
        }
      });

      document.getElementById('menuSettings').addEventListener('click', () => {
        hideContextMenu();
        // Open settings via IPC
        if (window.orbAPI?.openSettings) {
          window.orbAPI.openSettings();
        }
      });

      // Close menu on click outside
      document.addEventListener('click', (e) => {
        if (!contextMenu.contains(e.target) && e.target !== orb) {
          hideContextMenu();
        }
      });

      // ==================== TEXT CHAT ====================

      // Track current anchor for collapse
      let currentChatAnchor = 'bottom-right';

      async function openTextChat() {
        isTextChatOpen = true;

        // Determine which quadrant of the screen the orb is in
        const windowX = window.screenX;
        const windowY = window.screenY;
        const screenWidth = window.screen.availWidth;
        const screenHeight = window.screen.availHeight;

        // Calculate center points
        const windowCenterX = windowX + window.outerWidth / 2;
        const windowCenterY = windowY + window.outerHeight / 2;
        const screenCenterX = screenWidth / 2;
        const screenCenterY = screenHeight / 2;

        // Determine quadrant
        const isRight = windowCenterX > screenCenterX;
        const isBottom = windowCenterY > screenCenterY;

        // Remove existing position classes
        const orbContainer = document.querySelector('.orb-container');
        orbContainer.classList.remove('orb-bottom-right', 'orb-bottom-left', 'orb-top-right', 'orb-top-left');
        textChatPanel.classList.remove('pos-top-left', 'pos-top-right', 'pos-bottom-left', 'pos-bottom-right');

        // Determine anchor and apply classes based on quadrant
        // Chat panel appears opposite to orb position
        if (isBottom && isRight) {
          // Orb at bottom-right of screen: orb stays bottom-right, chat above
          currentChatAnchor = 'bottom-right';
          orbContainer.classList.add('orb-bottom-right');
          textChatPanel.classList.add('pos-top-left');
        } else if (isBottom && !isRight) {
          // Orb at bottom-left of screen: orb stays bottom-left, chat above
          currentChatAnchor = 'bottom-left';
          orbContainer.classList.add('orb-bottom-left');
          textChatPanel.classList.add('pos-top-right');
        } else if (!isBottom && isRight) {
          // Orb at top-right of screen: orb moves to top-right, chat below
          currentChatAnchor = 'top-right';
          orbContainer.classList.add('orb-top-right');
          textChatPanel.classList.add('pos-bottom-left');
        } else {
          // Orb at top-left of screen: orb moves to top-left, chat below
          currentChatAnchor = 'top-left';
          orbContainer.classList.add('orb-top-left');
          textChatPanel.classList.add('pos-bottom-right');
        }

        console.log(
          '[TextChat] Position:',
          isRight ? 'right' : 'left',
          isBottom ? 'bottom' : 'top',
          'anchor:',
          currentChatAnchor
        );

        // Expand the window to accommodate the chat panel
        if (window.orbAPI?.expandForChat) {
          try {
            await window.orbAPI.expandForChat(currentChatAnchor);
          } catch (e) {
            console.warn('[TextChat] Could not expand window:', e);
          }
        }

        // Show the chat panel
        textChatPanel.classList.add('visible');
        setTimeout(() => chatInput.focus(), 150);
        // Start inactivity timer
        resetChatInactivityTimer();
      }

      async function closeTextChat() {
        isTextChatOpen = false;
        textChatPanel.classList.remove('visible');
        // Clear inactivity timer
        clearChatInactivityTimer();

        // Collapse the window back to original size/position
        if (window.orbAPI?.collapseFromChat) {
          try {
            await window.orbAPI.collapseFromChat();
          } catch (e) {
            console.warn('[TextChat] Could not collapse window:', e);
          }
        }

        // Reset orb position after collapse, restoring the side-flip class
        const orbContainer = document.querySelector('.orb-container');
        orbContainer.classList.remove('orb-bottom-right', 'orb-bottom-left', 'orb-top-right', 'orb-top-left');
        textChatPanel.classList.remove('pos-top-left', 'pos-top-right', 'pos-bottom-left', 'pos-bottom-right');
        // Restore the correct side class (left or right) based on current screen position
        orbContainer.classList.add(currentOrbSide === 'left' ? 'orb-bottom-left' : 'orb-bottom-right');
        currentChatAnchor = currentOrbSide === 'left' ? 'bottom-left' : 'bottom-right';
      }

      chatCloseBtn.addEventListener('click', closeTextChat);

      // Reset inactivity timer on any chat panel interaction
      chatInput.addEventListener('input', resetChatInactivityTimer);
      chatInput.addEventListener('focus', resetChatInactivityTimer);
      chatMessages.addEventListener('scroll', resetChatInactivityTimer);
      textChatPanel.addEventListener('mousemove', resetChatInactivityTimer);

      function addChatMessage(type, text) {
        // Remove empty state if present
        const emptyState = chatMessages.querySelector('.chat-empty');
        if (emptyState) {
          emptyState.remove();
        }

        const msg = document.createElement('div');
        msg.className = `chat-message ${type}`;
        msg.textContent = text;
        chatMessages.appendChild(msg);
        chatMessages.scrollTop = chatMessages.scrollHeight;

        chatHistory.push({ type, text, timestamp: Date.now() });

        // Reset inactivity timer when messages are added
        resetChatInactivityTimer();
      }

      async function sendChatMessage() {
        const text = chatInput.value.trim();
        if (!text) return;

        // Reset inactivity timer on send
        resetChatInactivityTimer();

        // Clear input
        chatInput.value = '';
        chatSendBtn.disabled = true;

        // Add user message
        addChatMessage('user', text);

        // Process through the same command handler as voice
        try {
          // Show processing state
          addChatMessage('system', 'Processing...');

          // Use the same processVoiceCommand function
          await processVoiceCommand(text);

          // Remove processing message
          const processingMsg = chatMessages.querySelector('.chat-message.system:last-child');
          if (processingMsg && processingMsg.textContent === 'Processing...') {
            processingMsg.remove();
          }

          // Add confirmation
          addChatMessage('assistant', 'Got it. Working on that.');
        } catch (error) {
          console.error('[TextChat] Error:', error);
          addChatMessage('assistant', 'Sorry, something went wrong.');
        }

        chatSendBtn.disabled = false;
      }

      chatSendBtn.addEventListener('click', sendChatMessage);

      chatInput.addEventListener('keydown', (e) => {
        if (e.key === 'Enter' && !e.shiftKey) {
          e.preventDefault();
          sendChatMessage();
        }
        if (e.key === 'Escape') {
          closeTextChat();
        }
      });

      // Explicit paste support for frameless window (Edit menu may not work in frameless windows)
      chatInput.addEventListener('paste', (e) => {
        console.log('[TextChat] Paste event received');
        resetChatInactivityTimer();
      });

      // Manual paste handler for frameless window where native Cmd+V may not work
      chatInput.addEventListener('keydown', async (e) => {
        if ((e.metaKey || e.ctrlKey) && e.key.toLowerCase() === 'v') {
          // Prevent default to handle manually (native paste often fails in frameless windows)
          e.preventDefault();

          let text = null;

          // Try Electron clipboard API first (most reliable in Electron)
          if (window.clipboardAPI && window.clipboardAPI.readText) {
            try {
              text = window.clipboardAPI.readText();
              console.log('[TextChat] Got text from Electron clipboard API');
            } catch (err) {
              console.warn('[TextChat] Electron clipboard failed:', err.message);
            }
          }

          // Fall back to browser Clipboard API
          if (!text && navigator.clipboard && navigator.clipboard.readText) {
            try {
              text = await navigator.clipboard.readText();
              console.log('[TextChat] Got text from browser Clipboard API');
            } catch (err) {
              console.warn('[TextChat] Browser clipboard failed:', err.message);
            }
          }

          if (text) {
            const start = chatInput.selectionStart;
            const end = chatInput.selectionEnd;
            const value = chatInput.value;
            chatInput.value = value.substring(0, start) + text + value.substring(end);
            chatInput.selectionStart = chatInput.selectionEnd = start + text.length;
            chatInput.dispatchEvent(new Event('input', { bubbles: true }));
            console.log('[TextChat] Pasted text successfully');
          } else {
            console.warn('[TextChat] Could not read clipboard');
          }

          resetChatInactivityTimer();
        }
      });

      // Update UI based on state
      function updateUI(uiState, text = '') {
        orb.classList.remove('listening', 'processing', 'connecting', 'speaking', 'awaiting', 'error');
        // Note: transitions drive updateUI(), not the other way around.
        // No S.transition() call here -- that would cause infinite loops.

        switch (uiState) {
          case 'connecting':
            orb.classList.add('connecting');
            orb.title = 'Connecting...';
            break;
          case 'listening':
            orb.classList.add('listening');
            orb.title = 'Click to stop';
            break;
          case 'processing':
            orb.classList.add('processing');
            orb.title = 'Processing...';
            break;
          case 'speaking':
            orb.classList.add('speaking');
            orb.title = 'Click to stop';
            break;
          case 'awaiting':
            orb.classList.add('awaiting');
            orb.title = 'Waiting for your answer...';
            break;
          case 'error':
            orb.classList.add('error');
            showTranscript(text || 'Error occurred', false);
            setTimeout(() => {
              orb.classList.remove('error');
              hideTranscript();
            }, 3000);
            break;
          default: // idle
            orb.title = 'Click to start listening';
        }
      }

      let hideTranscriptTimeout = null;

      // Auto-scroll state
      let _autoScrollRAF = null;
      let _autoScrollTimeout = null;

      function _updateOverflowIndicators() {
        const el = transcriptInner;
        const hasOverflow = el.scrollHeight > el.clientHeight + 2;
        const scrolledFromTop = el.scrollTop > 4;
        const scrolledToBottom = el.scrollTop + el.clientHeight >= el.scrollHeight - 4;

        transcriptEl.classList.toggle('has-overflow-top', hasOverflow && scrolledFromTop);
        transcriptEl.classList.toggle('has-overflow-bottom', hasOverflow && !scrolledToBottom);
      }

      function _cancelAutoScroll() {
        if (_autoScrollRAF) {
          cancelAnimationFrame(_autoScrollRAF);
          _autoScrollRAF = null;
        }
        if (_autoScrollTimeout) {
          clearTimeout(_autoScrollTimeout);
          _autoScrollTimeout = null;
        }
      }

      function _startAutoScroll() {
        _cancelAutoScroll();
        const el = transcriptInner;
        const overflowAmount = el.scrollHeight - el.clientHeight;
        if (overflowAmount <= 0) return;

        // Start from top
        el.scrollTop = 0;
        _updateOverflowIndicators();

        // Wait a beat before starting to scroll (let user read the start)
        _autoScrollTimeout = setTimeout(() => {
          const pixelsPerSecond = 40; // Comfortable reading speed
          const startTime = performance.now();
          const startScroll = el.scrollTop;
          const distance = el.scrollHeight - el.clientHeight - startScroll;
          const duration = (distance / pixelsPerSecond) * 1000;

          function step(now) {
            const elapsed = now - startTime;
            const progress = Math.min(elapsed / duration, 1);
            // Ease-in-out for smooth feel
            const eased = progress < 0.5 ? 2 * progress * progress : 1 - Math.pow(-2 * progress + 2, 2) / 2;
            el.scrollTop = startScroll + distance * eased;
            _updateOverflowIndicators();
            if (progress < 1) {
              _autoScrollRAF = requestAnimationFrame(step);
            }
          }
          _autoScrollRAF = requestAnimationFrame(step);
        }, 800); // 800ms pause before scrolling starts
      }

      // Track whether window is currently expanded for tooltip
      let _windowExpandedForTooltip = false;
      let _collapseTimeout = null;
      const TOOLTIP_WINDOW_WIDTH = 400;
      const TOOLTIP_WINDOW_HEIGHT = 200;
      const COLLAPSED_WIDTH = 130;
      const COLLAPSED_HEIGHT = 130;

      function _getAnchor() {
        return currentOrbSide === 'left' ? 'bottom-left' : 'bottom-right';
      }

      function _expandForTooltip() {
        if (_collapseTimeout) {
          clearTimeout(_collapseTimeout);
          _collapseTimeout = null;
        }
        if (_windowExpandedForTooltip) return;
        _windowExpandedForTooltip = true;
        window.orbAPI.resizeWindow(TOOLTIP_WINDOW_WIDTH, TOOLTIP_WINDOW_HEIGHT, _getAnchor());
      }

      function _collapseFromTooltip() {
        if (_collapseTimeout) clearTimeout(_collapseTimeout);
        _collapseTimeout = setTimeout(() => {
          _collapseTimeout = null;
          if (_windowExpandedForTooltip && !transcriptEl.classList.contains('visible')) {
            _windowExpandedForTooltip = false;
            window.orbAPI.resizeWindow(COLLAPSED_WIDTH, COLLAPSED_HEIGHT, _getAnchor());
          }
        }, 900); // after fade animation (800ms) + small buffer
      }

      function showTranscript(text, isInterim = false) {
        // Clear any pending hide
        if (hideTranscriptTimeout) {
          clearTimeout(hideTranscriptTimeout);
          hideTranscriptTimeout = null;
        }

        // Expand window to fit tooltip (no-op if already expanded)
        _expandForTooltip();

        // Remove fading state if it was fading
        transcriptEl.classList.remove('fading', 'below', 'left-of-orb', 'right-of-orb');

        // Detect if this is a streaming append (agent speaking word-by-word)
        const isStreaming =
          transcriptEl.classList.contains('agent-speaking') && text.startsWith(transcriptInner.textContent);

        transcriptInner.textContent = text;
        transcriptEl.classList.toggle('interim', isInterim);

        if (isStreaming) {
          // Streaming: smooth-scroll to bottom to follow new words
          transcriptInner.scrollTo({ top: transcriptInner.scrollHeight, behavior: 'smooth' });
          _updateOverflowIndicators();
        } else {
          // New text: start auto-scroll from top after a brief pause
          transcriptInner.scrollTop = 0;
          // Use rAF to let layout settle before measuring overflow
          requestAnimationFrame(() => {
            _updateOverflowIndicators();
            const hasOverflow = transcriptInner.scrollHeight > transcriptInner.clientHeight + 2;
            if (hasOverflow && !isInterim) {
              _startAutoScroll();
            }
          });
        }

        // Position tooltip after a brief delay to let window resize settle
        requestAnimationFrame(() => {
          _positionTranscriptTooltip();
        });
      }

      function _positionTranscriptTooltip() {
        // Get the orb's position within the window
        const orbContainer = document.querySelector('.orb-container');
        const orbRect = orbContainer.getBoundingClientRect();
        const windowWidth = window.innerWidth;
        const windowHeight = window.innerHeight;

        // Window position on physical screen (same approach as context menu)
        const windowX = window.screenX;
        const screenWidth = window.screen.availWidth;

        const margin = 15;
        const gap = 15;

        // Orb's center in screen coordinates
        const orbScreenCenterX = windowX + orbRect.left + orbRect.width / 2;
        const isOrbOnRightHalf = orbScreenCenterX > screenWidth / 2;

        let left, right;

        if (isOrbOnRightHalf) {
          //  Orb is on the RIGHT half of screen 
          // Show tooltip to the LEFT of the orb
          transcriptEl.classList.add('left-of-orb');
          left = margin;
          right = windowWidth - orbRect.left + gap;
        } else {
          //  Orb is on the LEFT half of screen 
          // Show tooltip to the RIGHT of the orb
          transcriptEl.classList.add('right-of-orb');
          left = orbRect.right + gap;
          right = margin;
        }

        // Apply horizontal position first so width is set for height measurement
        transcriptEl.style.left = left + 'px';
        transcriptEl.style.right = right + 'px';
        transcriptEl.style.bottom = 'auto';

        // Measure actual rendered height (element is opacity:0 before .visible)
        const actualHeight = transcriptEl.offsetHeight;

        // Vertically center on the orb, clamped so tooltip stays fully within window
        const centerTop = orbRect.top + orbRect.height / 2 - actualHeight / 2;
        const top = Math.max(margin, Math.min(centerTop, windowHeight - actualHeight - margin));

        // Apply vertical position
        transcriptEl.style.top = top + 'px';

        // Now show it
        transcriptEl.classList.add('visible');
      }

      function hideTranscript() {
        // Stop any running auto-scroll
        _cancelAutoScroll();

        // Start elegant fade out
        transcriptEl.classList.add('fading');
        transcriptEl.classList.remove('visible');

        // Clean up classes and inline styles after animation completes
        hideTranscriptTimeout = setTimeout(() => {
          transcriptEl.classList.remove(
            'fading',
            'below',
            'left-of-orb',
            'right-of-orb',
            'interim',
            'has-overflow-top',
            'has-overflow-bottom',
            'agent-speaking'
          );
          // Reset inline styles so CSS defaults take over
          transcriptEl.style.top = '';
          transcriptEl.style.bottom = '';
          transcriptEl.style.left = '';
          transcriptEl.style.right = '';
          // Reset inner scroll position
          transcriptInner.scrollTop = 0;
        }, 800);

        // Collapse window back to orb-only size after fade
        _collapseFromTooltip();
      }

      // Convert Float32Array to base64 PCM16
      function floatTo16BitPCM(float32Array) {
        const buffer = new ArrayBuffer(float32Array.length * 2);
        const view = new DataView(buffer);
        for (let i = 0; i < float32Array.length; i++) {
          const s = Math.max(-1, Math.min(1, float32Array[i]));
          view.setInt16(i * 2, s < 0 ? s * 0x8000 : s * 0x7fff, true);
        }
        return btoa(String.fromCharCode.apply(null, new Uint8Array(buffer)));
      }

      // ==================== TRANSITION SIDE-EFFECT HANDLER ====================
      // One centralized place where state transitions trigger real-world effects.
      S.on('transition', ({ from, to, reason }) => {
        // --- Entering connecting: open WebSocket ---
        if (to === 'connecting') {
          (async () => {
            try {
              await window.orbAPI.requestMicPermission();
              const result = await window.orbAPI.connect();
              if (!result.success) throw new Error(result.error || 'Failed to connect');
              // session_updated event will trigger S.transition('listening')
            } catch (e) {
              console.error('[Orb] Connect failed:', e);
              S.endSession('connect-failed');
            }
          })();
          updateUI('connecting');
        }

        // --- Entering listening: start mic capture ---
        if (to === 'listening') {
          // Set up audio capture if not already active
          if (!processor || !audioContext) {
            (async () => {
              try {
                mediaStream = await navigator.mediaDevices.getUserMedia({
                  audio: {
                    channelCount: 1,
                    sampleRate: 24000,
                    echoCancellation: true,
                    noiseSuppression: true,
                  },
                });
                audioContext = new AudioContext({ sampleRate: 24000 });
                const source = audioContext.createMediaStreamSource(mediaStream);
                processor = audioContext.createScriptProcessor(4096, 1, 1);
                processor.onaudioprocess = (e) => {
                  if (!S.isListening) return;
                  const inputData = e.inputBuffer.getChannelData(0);
                  const base64Audio = floatTo16BitPCM(inputData);
                  window.orbAPI.sendAudio(base64Audio);
                };
                source.connect(processor);
                processor.connect(audioContext.destination);
                console.log('[Orb] Audio capture started');
              } catch (e) {
                console.error('[Orb] Audio capture failed:', e);
                S.endSession('mic-failed');
              }
            })();
          }
          startNoSpeechTimeout();
          if (from === 'connecting' || from === 'awaitingInput') {
            playReadyChime();
          }
          // Reset speech tracking for this listen cycle
          S.set('lastSpeechTime', 0);
          S.set('hasSpokenThisSession', false);
          updateUI('listening');
        }

        // --- Leaving listening: stop mic + timers ---
        if (from === 'listening' && to !== 'listening') {
          stopAudioCapture();
          clearAllSpeechTimers();
        }

        // --- Entering processing ---
        if (to === 'processing') {
          updateUI('processing');
          if (window.OrbAudio) window.OrbAudio.playAcknowledgeTone();
          // 30s timeout managed by OrbState automatically
        }

        // --- Entering speaking ---
        if (to === 'speaking') {
          updateUI('speaking');
        }

        // --- Entering awaitingInput ---
        if (to === 'awaitingInput') {
          updateUI('awaiting');
        }

        // --- Entering idle: full cleanup ---
        if (to === 'idle') {
          stopAudioCapture();
          // All timeouts cleared by OrbState automatically
          window.orbAPI.disconnect();

          const isError =
            reason &&
            reason !== 'user-stop' &&
            reason !== 'ws-disconnected' &&
            reason !== 'tts-complete' &&
            from !== 'speaking';
          if (isError) {
            showTranscript('Something went wrong. Try again.', false);
            setTimeout(hideTranscript, 3000);
          } else {
            // Hide any lingering transcript (agent text, streaming deltas, etc.)
            hideTranscript();
          }
          transcriptEl.classList.remove('agent-speaking');

          // Play completion tone when task finishes normally (not on error/user-stop)
          const isNormalCompletion = from === 'speaking' && reason === 'tts-complete';
          if (isNormalCompletion && window.OrbAudio) {
            window.OrbAudio.playCompletionTone();
          }

          _activeTaskId = null;
          _pendingNeedsInput = false;
          S.update({
            isSessionReady: false,
            pendingFunctionCallId: null,
            pendingSubmitCount: 0,
            pendingDisambiguation: null,
            hasActivePanel: false,
          });
          updateUI('idle');
        }
      });

      // ==================== EVENT ROUTER HANDLER FUNCTIONS ====================

      // Extract the most complex handler as a named function
      async function handleFunctionCallTranscript(event) {
        const transcript = event.transcript;
        const callId = event.callId;

        S.transition('processing', 'function-call');
        S.update({
          lastProcessedTranscript: transcript,
          lastProcessedTime: Date.now(),
          pendingFunctionCallId: callId,
          pendingSubmitCount: (S.get('pendingSubmitCount') || 0) + 1,
        });
        // Track for dedup with regular transcript event
        S.set('lastFunctionCallTranscript', transcript?.toLowerCase() || '');
        S.set('lastFunctionCallTime', Date.now());

        // Mark that speech ended
        onSpeechEnded();

        showTranscript(transcript, false);

        try {
          if (!window.agentHUD || !window.agentHUD.submitTask) {
            console.error('[Orb] window.agentHUD not available');
            await window.orbAPI.respondToFunction(callId, "I'm not ready yet. Try again in a moment.");
            S.set('pendingFunctionCallId', null);
            return;
          }

          const result = await window.agentHUD.submitTask(transcript, {
            toolId: 'orb',
            skipFilter: false,
          });
          console.log('[Orb] HUD API result:', JSON.stringify(result));

          // Handle error returns
          if (!result || result.error) {
            const errMsg = result?.error || 'Unknown error';
            console.warn('[Orb] submitTask returned error:', errMsg);
            const userMsg =
              errMsg.includes('Exchange not running') || errMsg.includes('not available')
                ? "I'm still starting up. Try again in a moment."
                : result?.message || 'I had trouble processing that. Try again.';
            await window.orbAPI.respondToFunction(callId, userMsg);
            S.set('pendingFunctionCallId', null);
            return;
          }

          // Async exchange path: events will handle TTS
          if (result.suppressAIResponse && result.queued) {
            console.log('[Orb] Async exchange path - events will speak result');
            await window.orbAPI.respondToFunction(callId, '');
            S.set('pendingFunctionCallId', null);
            return;
          }

          // Transcript was filtered
          if (result.needsClarification && result.filterReason) {
            console.log('[Orb] Transcript filtered:', result.filterReason);
            await window.orbAPI.respondToFunction(callId, result.message || '');
            S.set('pendingFunctionCallId', null);
            return;
          }

          // Check for HTML panel result
          if (result.html) {
            S.set('hasActivePanel', true);
          }

          // Multi-turn: agent needs input (includes NormalizeIntent clarification)
          if (result.needsInput || (result.needsClarification && !result.filterReason)) {
            console.log('[Orb] Agent needs input, entering awaitingInput');
            _pendingNeedsInput = true;
            const prompt = result.message || result.needsInput?.prompt || 'What would you like?';
            await window.orbAPI.respondToFunction(callId, prompt);
            S.set('pendingFunctionCallId', null);
            // Transition to awaitingInput - TTS will play the prompt,
            // then audio_done will transition to listening
            if (S.phase === 'processing') {
              S.transition('speaking', 'tts-followup');
            }
            // Note: We don't transition to awaitingInput here because TTS
            // hasn't started yet. The audio_done handler checks _pendingNeedsInput.
          } else {
            // Standard response
            let responseText = result.message || "I'm not sure how to help with that.";
            await window.orbAPI.respondToFunction(callId, responseText);
            S.set('pendingFunctionCallId', null);
          }
        } catch (err) {
          console.error('[Orb] Error processing function call:', err?.message || err);
          await window.orbAPI.respondToFunction(callId, 'Sorry, something went wrong. ' + (err?.message || ''));
          S.set('pendingFunctionCallId', null);
        } finally {
          const count = (S.get('pendingSubmitCount') || 1) - 1;
          S.set('pendingSubmitCount', count);
        }

        setTimeout(hideTranscript, 3000);
      }

      // Track if a _needsInput result was received (for audio_done to know)
      let _pendingNeedsInput = false;
      let _activeTaskId = null; // Track whether a task is still executing (queued but not yet settled)

      // Register the event router at page load
      if (window.OrbEventRouter && window.orbAPI) {
        let _firstAudioReceived = false;

        window.OrbEventRouter.start(
          window.orbAPI,
          {
            // --- OUTPUT (always active, from voice-speaker) ---
            audio_delta: (e) => {
              if (e.audio) {
                if (!_firstAudioReceived) {
                  _firstAudioReceived = true;
                  ensureTTSAudioReady();
                }
                const float32 = base64ToFloat32(e.audio);
                if (window.OrbAudio) window.OrbAudio.addChunk(float32);
                if (!S.isSpeaking && S.phase !== 'idle') {
                  S.transition('speaking', 'audio_delta');
                }
              }
            },
            audio_wav: (e) => {
              if (S.phase === 'idle') {
                console.log('[Orb] Ignoring audio_wav while idle (phantom audio blocked)');
                return;
              }
              if (e.audio) {
                if (!S.isSpeaking) {
                  S.transition('speaking', 'audio_wav');
                }
                window._agentResponseText = '';
                playWAVAudio(e.audio);
              }
            },
            audio_done: (e) => {
              if (S.phase === 'idle') {
                console.log('[Orb] Ignoring audio_done while idle (phantom audio blocked)');
                return;
              }
              window._agentResponseText = '';
              S.set('ttsEndTime', Date.now());

              const chunkCount = window.OrbAudio?.getChunkCount() || 0;
              if (chunkCount > 0) {
                playTTSAudio();
              }

              // Check if this was a follow-up question (agent needs more input)
              // If so, transition speaking -> awaitingInput -> listening
              if (_pendingNeedsInput) {
                _pendingNeedsInput = false;
                if (S.isSpeaking) {
                  S.transition('awaitingInput', 'needs-input');
                  // awaitingInput -> listening happens after a brief delay
                  // to let the user hear the question
                  setTimeout(() => {
                    if (S.isAwaitingInput) {
                      S.transition('listening', 'followup-listen');
                    }
                  }, 500);
                }
              } else if (S.isSpeaking && (_activeTaskId || (S.get('pendingSubmitCount') || 0) > 0)) {
                // Task still executing or function call processing -- interim speech
                console.log('[Orb] audio_done: interim speech, work active:', _activeTaskId || 'pending-submit');
                S.transition('processing', 'interim-speech-done');
              } else if (S.isSpeaking) {
                // Normal command complete -- no active task
                S.transition('idle', 'tts-complete');
              }
            },
            clear_audio_buffer: (e) => {
              console.log('[Orb] Clearing audio buffer');
              if (window.OrbAudio) window.OrbAudio.clearChunks();
            },
            speech_text_delta: (e) => {
              if (S.phase === 'idle') return; // Block phantom text while idle
              if (S.get('hasActivePanel')) return;
              if (!window._agentResponseText) window._agentResponseText = '';
              window._agentResponseText += e.text;
              transcriptEl.classList.add('agent-speaking');
              showTranscript(window._agentResponseText, false);
            },
            speech_text: (e) => {
              if (S.phase === 'idle') return; // Block phantom text while idle
              window._agentResponseText = '';
              if (S.get('hasActivePanel')) {
                S.set('hasActivePanel', false);
                return;
              }
              showTranscript(e.text, false);
              transcriptEl.classList.add('agent-speaking');
              const estimatedLines = Math.ceil((e.text || '').length / 50);
              const estimatedScrollTime = Math.max(4000, estimatedLines * 600 + 2000);
              setTimeout(() => {
                hideTranscript();
                transcriptEl.classList.remove('agent-speaking');
              }, estimatedScrollTime);
            },
            response_cancelled: (e) => {
              console.log('[Orb] Response cancelled');
              if (window.OrbAudio) window.OrbAudio.clearChunks();
              hideTranscript();
              transcriptEl.classList.remove('agent-speaking');
            },

            // --- INPUT (gated on phase + noise-filtered, from voice-listener) ---
            session_updated: (e) => {
              S.set('isSessionReady', true);
              S.transition('listening', 'session-ready');
            },
            transcript_delta: (e) => {
              onSpeechDetected();
              showTranscript(e.text, true);
            },
            function_call_transcript: handleFunctionCallTranscript,
            transcript: (e) => {
              const text = e.text?.trim();
              if (!text) return;

              // Skip if already processed via function call
              const stripPunctuation = (s) => s.replace(/[.,!?;:'"]/g, '').trim();
              const lastFCT = S.get('lastFunctionCallTranscript') || '';
              const lastFCTime = S.get('lastFunctionCallTime') || 0;
              const normalizedText = stripPunctuation(text.toLowerCase());
              const normalizedFuncCall = stripPunctuation(lastFCT);
              if (normalizedText === normalizedFuncCall && Date.now() - lastFCTime < FUNCTION_CALL_DEDUP_MS) {
                console.log('[Orb] Skipping transcript - already handled via function call:', text);
                return;
              }

              // Update dedup tracking
              S.set('lastProcessedTranscript', text);
              S.set('lastProcessedTime', Date.now());

              // Mark speech ended
              onSpeechEnded();

              showTranscript(text, false);
              processVoiceCommand(text);
              setTimeout(hideTranscript, 3000);
            },

            // --- LIFECYCLE (always active) ---
            reconnecting: (e) => {
              console.log('[Orb] Reconnecting, attempt:', e.attempt);
              showTranscript('Reconnecting...', false);
            },
            reconnected: (e) => {
              console.log('[Orb] Reconnected');
              S.set('isSessionReady', true);
              hideTranscript();
              updateUI('listening');
              S.set('hasSpokenThisSession', false);
              startNoSpeechTimeout();
            },
            disconnected: (e) => {
              console.warn('[Orb] Disconnected, code:', e.code);
              stopAudioCapture();
              S.endSession('ws-disconnected');
            },
            error: (e) => {
              console.error('[Orb] Speech error:', e.message || e);
              S.endSession('speech-error');
            },
          },
          {
            ttsCooldownMs: TTS_COOLDOWN_MS,
            dedupWindowMs: 3000,
            isLikelyNoise: isLikelyNoise,
          }
        );

        console.log('[Orb] Event router registered');
      } else {
        console.error('[Orb] OrbEventRouter or orbAPI not available!');
      }

      // ==================== LIFECYCLE FUNCTIONS ====================

      function startListening() {
        if (S.phase !== 'idle') return;
        _listenerGeneration++;
        S.startSession();
      }

      function stopListening() {
        S.endSession('user-stop');
      }

      // Legacy alias - no longer needed but keep for safety
      // (Legacy functions removed -- replaced by OrbState + event router above)

      // ==========================================================================
      // DRAG SUPPORT
      // ==========================================================================

      let isDragging = false;
      let dragStartX = 0;
      let dragStartY = 0;
      let windowStartX = 0;
      let windowStartY = 0;
      let hasMoved = false;

      // Track which side of the screen the orb is on
      // 'right' = orb at bottom-right of window (default), 'left' = orb at bottom-left
      let currentOrbSide = 'right';

      // Listen for initial side from main process (restores saved side on launch)
      if (window.orbAPI?.onInitialSide) {
        window.orbAPI.onInitialSide((side) => {
          if (side && side !== currentOrbSide) {
            currentOrbSide = side;
            const orbContainer = document.querySelector('.orb-container');
            orbContainer.classList.remove('orb-bottom-right', 'orb-bottom-left');
            orbContainer.classList.add(side === 'left' ? 'orb-bottom-left' : 'orb-bottom-right');
            console.log(`[Orb] Initial side set to: ${side}`);
          }
        });
      }

      orb.addEventListener('mousedown', (e) => {
        // Only drag on left-click (button 0), not right-click (button 2)
        if (e.button !== 0) return;

        isDragging = true;
        hasMoved = false;
        dragStartX = e.screenX;
        dragStartY = e.screenY;
        // Capture window position at start of drag
        windowStartX = window.screenX;
        windowStartY = window.screenY;
        orb.style.cursor = 'grabbing';
        e.preventDefault();
      });

      document.addEventListener('mousemove', (e) => {
        if (!isDragging) return;

        // Calculate total delta from drag start
        const deltaX = e.screenX - dragStartX;
        const deltaY = e.screenY - dragStartY;

        // Only start moving if we've moved more than 5px (to distinguish from clicks)
        if (Math.abs(deltaX) > 5 || Math.abs(deltaY) > 5) {
          hasMoved = true;

          // Calculate new position based on initial window position + total delta
          const newX = windowStartX + deltaX;
          const newY = windowStartY + deltaY;

          window.orbAPI.setPosition(newX, newY);
        }
      });

      document.addEventListener('mouseup', () => {
        if (isDragging) {
          isDragging = false;
          orb.style.cursor = 'pointer';

          // After drag ends, check if orb crossed the screen midpoint
          // Uses display-aware IPC for multi-monitor support
          if (hasMoved) {
            const orbContainer = document.querySelector('.orb-container');
            const orbRect = orbContainer.getBoundingClientRect();
            const orbScreenCenterX = window.screenX + orbRect.left + orbRect.width / 2;

            // Ask main process for the correct display bounds (multi-monitor aware)
            orbAPI
              .getDisplayForOrb()
              .then((displayInfo) => {
                const displayCenterX = displayInfo.x + displayInfo.width / 2;
                const isOnLeftHalf = orbScreenCenterX <= displayCenterX;
                const newSide = isOnLeftHalf ? 'left' : 'right';

                if (newSide !== currentOrbSide) {
                  currentOrbSide = newSide;
                  flipOrbToSide(newSide);
                }
              })
              .catch(() => {
                // Fallback: use window.screen (single-monitor, current behavior)
                const screenWidth = window.screen.availWidth;
                const isOnLeftHalf = orbScreenCenterX <= screenWidth / 2;
                const newSide = isOnLeftHalf ? 'left' : 'right';

                if (newSide !== currentOrbSide) {
                  currentOrbSide = newSide;
                  flipOrbToSide(newSide);
                }
              });
          }
        }
      });

      /**
       * Flip the orb's position within its window so the tooltip
       * can appear on the opposite side.
       * - 'right': orb at bottom-right of window (tooltip goes left)
       * - 'left':  orb at bottom-left of window (tooltip goes right)
       */
      function flipOrbToSide(side) {
        const orbContainer = document.querySelector('.orb-container');

        // Remove all position classes
        orbContainer.classList.remove('orb-bottom-right', 'orb-bottom-left', 'orb-top-right', 'orb-top-left');

        if (side === 'left') {
          orbContainer.classList.add('orb-bottom-left');
        } else {
          orbContainer.classList.add('orb-bottom-right');
        }

        // Tell main process to reposition window so orb stays visually anchored
        if (window.orbAPI?.flipSide) {
          window.orbAPI.flipSide(side);
        }
      }

      // Toggle listening on click (only if we didn't drag)
      orb.addEventListener('click', () => {
        if (hasMoved) {
          hasMoved = false;
          return;
        }

        // If text chat is open, close it and start voice
        if (isTextChatOpen) {
          closeTextChat();
        }

        if (S.phase === 'idle') {
          startListening();
        } else if (S.phase === 'speaking') {
          // Barge-in: cancel TTS and go back to listening
          console.log('[Orb] Barge-in: cancelling speech');
          if (window.orbAPI && window.orbAPI.cancelResponse) {
            window.orbAPI.cancelResponse();
          }
          if (window.OrbAudio) window.OrbAudio.clearChunks();
          S.transition('listening', 'barge-in');
        } else {
          stopListening();
        }
      });

      // Right-click to show context menu (only on the orb or its container)
      const orbContainer = document.querySelector('.orb-container');
      orbContainer.addEventListener('contextmenu', (e) => {
        e.preventDefault();
        e.stopPropagation();
        showContextMenu(e.clientX, e.clientY);
      });

      // Prevent default context menu on the rest of the document (but don't show our menu)
      document.addEventListener('contextmenu', (e) => {
        e.preventDefault();
      });

      // Double-click as alternative to open text chat directly
      orb.addEventListener('dblclick', (e) => {
        console.log('[Orb] Double-click detected');
        e.preventDefault();
        openTextChat();
      });

      // Handle window close
      window.addEventListener('beforeunload', () => {
        stopListening();
      });

      // Listen for plan summary from Agent Composer
      if (window.orbAPI?.onPlanSummary) {
        window.orbAPI.onPlanSummary(async (data) => {
          console.log('[Orb] Received plan summary:', data.type);

          if (data.type === 'plan-ready' && data.summary) {
            // Speak the plan summary via TTS (using backend)
            try {
              await window.orbAPI.speak(data.summary);
            } catch (e) {
              console.warn('[Orb] Could not speak plan summary:', e);
            }
          } else if (data.type === 'creation-complete' && data.agentName) {
            // Announce agent creation complete - conversational
            try {
              await window.orbAPI.speak(`All set! ${data.agentName} is ready to use.`);
            } catch (e) {
              console.warn('[Orb] Could not speak completion:', e);
            }
          }
        });
        console.log('[Orb] Agent Composer plan listener registered');
      }

      // =====================================================================
      // Click-through toggle: make transparent areas pass through to windows
      // behind (e.g. clipboard viewer) while keeping the orb itself clickable.
      // setIgnoreMouseEvents(true, { forward: true }) is set in main process
      // at window creation. Here we toggle it off when the cursor enters any
      // interactive element so clicks register, then back on when it leaves.
      // =====================================================================
      // Click-through: window starts collapsed (130x130, just the orb).
      // It expands dynamically for tooltip/chat and collapses when done.
      console.log('[Orb] Click-through: using dynamic window resizing');

      // #region agent log
      setInterval(() => {
        const chatPanel = document.getElementById('textChatPanel');
        const transcript = document.getElementById('transcript');
        const ctxMenu = document.querySelector('.orb-context-menu');
        const d = {
          chatPanelVisible: chatPanel?.classList.contains('visible'),
          chatPanelOpacity: chatPanel ? getComputedStyle(chatPanel).opacity : null,
          chatPanelPointerEvents: chatPanel ? getComputedStyle(chatPanel).pointerEvents : null,
          transcriptVisible: transcript?.classList.contains('visible'),
          transcriptOpacity: transcript ? getComputedStyle(transcript).opacity : null,
          ctxMenuVisible: ctxMenu?.classList.contains('visible'),
          bodyPointerEvents: getComputedStyle(document.body).pointerEvents,
          bodyBackground: getComputedStyle(document.body).background,
        };
        fetch('http://127.0.0.1:7242/ingest/54746cc5-c924-4bb5-9e76-3f6b729e6870', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            location: 'orb.html:periodic',
            message: 'Orb element visibility poll',
            data: d,
            timestamp: Date.now(),
            hypothesisId: 'H3,H4',
          }),
        }).catch(() => {});
      }, 5000);
      // #endregion

      // Log ready state
      console.log('[Orb] Voice Orb initialized with drag support and Agent Composer integration');
    </script>
  </body>
</html>
