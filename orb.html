<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Orb</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        html, body {
            width: 100%;
            height: 100%;
            background: transparent;
            overflow: hidden;
            /* Make transparent areas click-through to apps behind */
            pointer-events: none;
            /* Removed -webkit-app-region: drag - we handle dragging manually */
        }
        
        .orb-container {
            position: absolute;
            bottom: 20px;
            right: 20px;
            width: 80px;
            height: 80px;
            transition: top 0.25s ease, bottom 0.25s ease, left 0.25s ease, right 0.25s ease;
            /* Re-enable mouse events for the orb */
            pointer-events: auto;
        }
        
        /* Orb position variants for chat panel layout */
        .orb-container.orb-bottom-right {
            bottom: 20px;
            right: 20px;
            top: auto;
            left: auto;
        }
        
        .orb-container.orb-bottom-left {
            bottom: 20px;
            left: 20px;
            top: auto;
            right: auto;
        }
        
        .orb-container.orb-top-right {
            top: 20px;
            right: 20px;
            bottom: auto;
            left: auto;
        }
        
        .orb-container.orb-top-left {
            top: 20px;
            left: 20px;
            bottom: auto;
            right: auto;
        }
        
        .orb {
            width: 80px;
            height: 80px;
            border-radius: 50%;
            /* Use OneReach orb graphic */
            background-image: url('assets/banner.png');
            background-size: cover;
            background-position: center;
            cursor: grab;
            display: flex;
            align-items: center;
            justify-content: center;
            -webkit-app-region: no-drag;
            position: relative;
            /* No glow by default - clean look */
            box-shadow: none;
            transition: transform 0.3s ease, filter 0.3s ease;
            animation: orb-float 6s ease-in-out infinite;
        }
        
        .orb:hover {
            transform: scale(1.1);
            filter: brightness(1.1);
        }
        
        .orb.listening {
            box-shadow: 0 0 8px rgba(234, 179, 8, 0.5);
            animation: orb-float 4s ease-in-out infinite, orb-breathe 1.5s ease-in-out infinite;
        }
        
        .orb.processing {
            box-shadow: 0 0 8px rgba(249, 115, 22, 0.5);
            animation: orb-float 3s ease-in-out infinite, orb-pulse 0.8s ease-in-out infinite;
        }
        
        .orb.error {
            box-shadow: 0 0 8px rgba(239, 68, 68, 0.5);
            animation: orb-shake 0.5s ease-in-out;
        }
        
        /* Gentle floating motion */
        @keyframes orb-float {
            0%, 100% { 
                transform: translate(0, 0); 
            }
            25% { 
                transform: translate(2px, -3px); 
            }
            50% { 
                transform: translate(-1px, -1px); 
            }
            75% { 
                transform: translate(-2px, -2px); 
            }
        }
        
        /* Breathing glow when listening */
        @keyframes orb-breathe {
            0%, 100% { 
                box-shadow: 0 0 6px rgba(234, 179, 8, 0.4);
                filter: brightness(1);
            }
            50% { 
                box-shadow: 0 0 12px rgba(234, 179, 8, 0.6);
                filter: brightness(1.1);
            }
        }
        
        /* Pulse when processing */
        @keyframes orb-pulse {
            0%, 100% { 
                transform: scale(1);
            }
            50% { 
                transform: scale(1.05);
            }
        }
        
        /* Shake on error */
        @keyframes orb-shake {
            0%, 100% { transform: translateX(0); }
            20% { transform: translateX(-4px); }
            40% { transform: translateX(4px); }
            60% { transform: translateX(-4px); }
            80% { transform: translateX(4px); }
        }
        
        /* Ring effects - hidden by default, only show when listening */
        .ring, .ring-secondary {
            display: none;
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            width: 80px;
            height: 80px;
            border-radius: 50%;
            pointer-events: none;
        }
        
        .ring {
            border: 1px solid rgba(234, 179, 8, 0.4);
        }
        
        .orb.listening + .ring {
            display: block;
            animation: ring-expand 2s ease-out infinite;
        }
        
        @keyframes ring-expand {
            0% { 
                transform: translate(-50%, -50%) scale(1); 
                opacity: 0.5;
            }
            100% { 
                transform: translate(-50%, -50%) scale(1.6); 
                opacity: 0;
            }
        }
        
        /* Mic overlay icon (subtle, appears on hover or when idle) */
        .mic-overlay {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            width: 24px;
            height: 24px;
            opacity: 0;
            transition: opacity 0.3s ease;
            pointer-events: none;
        }
        
        .mic-overlay svg {
            width: 100%;
            height: 100%;
            fill: rgba(255, 255, 255, 0.9);
            filter: drop-shadow(0 2px 4px rgba(0, 0, 0, 0.3));
        }
        
        .orb:hover .mic-overlay {
            opacity: 0.8;
        }
        
        .orb.listening .mic-overlay {
            opacity: 0;
        }
        
        /* Transcript tooltip - elegant floating text */
        /* Uses fixed positioning for reliable placement regardless of orb position */
        .transcript-tooltip {
            position: fixed;
            background: linear-gradient(135deg, rgba(15, 15, 20, 0.95) 0%, rgba(25, 25, 35, 0.9) 100%);
            color: rgba(255, 255, 255, 0.95);
            padding: 14px 20px;
            border-radius: 16px;
            font-size: 14px;
            font-weight: 400;
            letter-spacing: 0.3px;
            line-height: 1.5;
            font-family: -apple-system, BlinkMacSystemFont, 'SF Pro Text', 'Segoe UI', Roboto, sans-serif;
            max-width: 320px;
            max-height: 150px;
            overflow-y: auto;
            text-align: center;
            white-space: normal;
            word-wrap: break-word;
            opacity: 0;
            /* Allow clicks through when hidden, enable when visible */
            pointer-events: none;
            -webkit-app-region: no-drag;
            /* Frosted glass effect */
            backdrop-filter: blur(20px);
            -webkit-backdrop-filter: blur(20px);
            /* Subtle border glow */
            border: 1px solid rgba(255, 255, 255, 0.1);
            box-shadow: 
                0 8px 32px rgba(0, 0, 0, 0.4),
                0 0 0 1px rgba(255, 255, 255, 0.05),
                inset 0 1px 0 rgba(255, 255, 255, 0.1);
            /* Default position - will be overridden by JS */
            bottom: 120px;
            right: 20px;
            left: auto;
            top: auto;
            /* Animation properties */
            transform: translateY(10px);
            transition: opacity 0.4s ease, transform 0.4s cubic-bezier(0.4, 0, 0.2, 1);
            z-index: 100;
        }
        
        .transcript-tooltip.visible {
            opacity: 1;
            transform: translateY(0);
        }
        
        /* Fading out state */
        .transcript-tooltip.fading {
            opacity: 0;
            transform: translateY(-10px);
            transition: opacity 0.8s ease-out, transform 0.8s ease-out;
        }
        
        .transcript-tooltip.interim {
            color: rgba(180, 180, 200, 0.9);
            font-style: italic;
            font-weight: 300;
        }
        
        /* Position below orb when near top of window */
        .transcript-tooltip.below {
            transform: translateY(-10px);
        }
        
        .transcript-tooltip.below.visible {
            transform: translateY(0);
        }
        
        .transcript-tooltip.below.fading {
            transform: translateY(10px);
        }
        
        /* ==================== CONTEXT MENU ==================== */
        .orb-context-menu {
            position: fixed;
            top: 50px;
            left: 50px;
            background: linear-gradient(135deg, rgba(20, 20, 30, 0.98) 0%, rgba(30, 30, 45, 0.95) 100%);
            border: 1px solid rgba(255, 255, 255, 0.1);
            border-radius: 12px;
            padding: 6px;
            min-width: 160px;
            box-shadow: 
                0 12px 40px rgba(0, 0, 0, 0.5),
                0 0 0 1px rgba(255, 255, 255, 0.05),
                inset 0 1px 0 rgba(255, 255, 255, 0.1);
            backdrop-filter: blur(20px);
            -webkit-backdrop-filter: blur(20px);
            opacity: 0;
            transform: scale(0.95) translateY(-5px);
            transform-origin: top left;
            transition: opacity 0.15s ease, transform 0.15s ease;
            /* Click-through when hidden, enabled when visible */
            pointer-events: none;
            z-index: 1000;
            -webkit-app-region: no-drag;
        }
        
        .orb-context-menu.visible {
            opacity: 1;
            transform: scale(1) translateY(0);
            pointer-events: auto;
        }
        
        .context-menu-item {
            display: flex;
            align-items: center;
            gap: 10px;
            padding: 10px 14px;
            color: rgba(255, 255, 255, 0.9);
            font-size: 13px;
            font-family: -apple-system, BlinkMacSystemFont, 'SF Pro Text', sans-serif;
            border-radius: 8px;
            cursor: pointer;
            transition: background 0.15s ease;
            -webkit-app-region: no-drag;
        }
        
        .context-menu-item:hover {
            background: rgba(255, 255, 255, 0.1);
        }
        
        .context-menu-item svg {
            width: 16px;
            height: 16px;
            fill: currentColor;
            opacity: 0.8;
        }
        
        .context-menu-divider {
            height: 1px;
            background: rgba(255, 255, 255, 0.1);
            margin: 6px 8px;
        }
        
        /* ==================== TEXT CHAT PANEL ==================== */
        .text-chat-panel {
            position: fixed;
            top: 15px;
            left: 15px;
            right: 15px;
            bottom: 120px;
            background: linear-gradient(135deg, rgba(15, 15, 25, 0.98) 0%, rgba(25, 25, 40, 0.95) 100%);
            border: 1px solid rgba(255, 255, 255, 0.1);
            border-radius: 16px;
            box-shadow: 
                0 20px 60px rgba(0, 0, 0, 0.5),
                0 0 0 1px rgba(255, 255, 255, 0.05),
                inset 0 1px 0 rgba(255, 255, 255, 0.1);
            backdrop-filter: blur(30px);
            -webkit-backdrop-filter: blur(30px);
            display: flex;
            flex-direction: column;
            opacity: 0;
            transform: scale(0.95) translateY(10px);
            transform-origin: bottom center;
            transition: opacity 0.25s ease, transform 0.25s cubic-bezier(0.4, 0, 0.2, 1);
            pointer-events: none;
            z-index: 500;
            -webkit-app-region: no-drag;
            overflow: hidden;
        }
        
        .text-chat-panel.visible {
            opacity: 1;
            transform: scale(1) translateY(0);
            pointer-events: auto;
        }
        
        .chat-header {
            display: flex;
            align-items: center;
            justify-content: space-between;
            padding: 16px 20px;
            border-bottom: 1px solid rgba(255, 255, 255, 0.08);
        }
        
        .chat-header-title {
            display: flex;
            align-items: center;
            gap: 10px;
            color: rgba(255, 255, 255, 0.95);
            font-size: 14px;
            font-weight: 500;
            font-family: -apple-system, BlinkMacSystemFont, 'SF Pro Text', sans-serif;
        }
        
        .chat-header-title svg {
            width: 18px;
            height: 18px;
            fill: currentColor;
            opacity: 0.8;
        }
        
        .chat-close-btn {
            width: 28px;
            height: 28px;
            border-radius: 8px;
            border: none;
            background: rgba(255, 255, 255, 0.05);
            color: rgba(255, 255, 255, 0.6);
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: all 0.15s ease;
        }
        
        .chat-close-btn:hover {
            background: rgba(255, 255, 255, 0.1);
            color: rgba(255, 255, 255, 0.9);
        }
        
        .chat-close-btn svg {
            width: 14px;
            height: 14px;
            fill: currentColor;
        }
        
        .chat-messages {
            flex: 1;
            overflow-y: auto;
            padding: 16px;
            display: flex;
            flex-direction: column;
            gap: 12px;
            min-height: 100px;
        }
        
        .chat-messages::-webkit-scrollbar {
            width: 6px;
        }
        
        .chat-messages::-webkit-scrollbar-track {
            background: transparent;
        }
        
        .chat-messages::-webkit-scrollbar-thumb {
            background: rgba(255, 255, 255, 0.15);
            border-radius: 3px;
        }
        
        .chat-message {
            padding: 12px 16px;
            border-radius: 16px;
            font-size: 13px;
            line-height: 1.5;
            font-family: -apple-system, BlinkMacSystemFont, 'SF Pro Text', sans-serif;
            max-width: 85%;
            animation: message-appear 0.2s ease;
        }
        
        @keyframes message-appear {
            from {
                opacity: 0;
                transform: translateY(8px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }
        
        .chat-message.user {
            align-self: flex-end;
            background: linear-gradient(135deg, #6366f1 0%, #8b5cf6 100%);
            color: white;
            border-bottom-right-radius: 6px;
        }
        
        .chat-message.assistant {
            align-self: flex-start;
            background: rgba(255, 255, 255, 0.08);
            color: rgba(255, 255, 255, 0.9);
            border-bottom-left-radius: 6px;
        }
        
        .chat-message.system {
            align-self: center;
            background: transparent;
            color: rgba(255, 255, 255, 0.5);
            font-size: 12px;
            padding: 8px;
        }
        
        .chat-input-container {
            padding: 16px;
            border-top: 1px solid rgba(255, 255, 255, 0.08);
        }
        
        .chat-input-wrapper {
            display: flex;
            align-items: center;
            gap: 10px;
            background: rgba(255, 255, 255, 0.06);
            border: 1px solid rgba(255, 255, 255, 0.1);
            border-radius: 14px;
            padding: 4px 4px 4px 16px;
            transition: all 0.15s ease;
        }
        
        .chat-input-wrapper:focus-within {
            border-color: rgba(139, 92, 246, 0.5);
            box-shadow: 0 0 0 3px rgba(139, 92, 246, 0.15);
        }
        
        .chat-input {
            flex: 1;
            background: transparent;
            border: none;
            outline: none;
            color: rgba(255, 255, 255, 0.95);
            font-size: 14px;
            font-family: -apple-system, BlinkMacSystemFont, 'SF Pro Text', sans-serif;
            padding: 10px 0;
        }
        
        .chat-input::placeholder {
            color: rgba(255, 255, 255, 0.35);
        }
        
        .chat-send-btn {
            width: 36px;
            height: 36px;
            border-radius: 10px;
            border: none;
            background: linear-gradient(135deg, #6366f1 0%, #8b5cf6 100%);
            color: white;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: all 0.15s ease;
            flex-shrink: 0;
        }
        
        .chat-send-btn:hover {
            transform: scale(1.05);
            box-shadow: 0 4px 12px rgba(139, 92, 246, 0.4);
        }
        
        .chat-send-btn:active {
            transform: scale(0.95);
        }
        
        .chat-send-btn:disabled {
            opacity: 0.5;
            cursor: not-allowed;
            transform: none;
        }
        
        .chat-send-btn svg {
            width: 16px;
            height: 16px;
            fill: currentColor;
        }
        
        /* Position variants based on orb location on screen */
        /* Default: orb at bottom-right, chat appears above/left */
        .text-chat-panel.pos-top-left {
            /* Orb at bottom-right: chat above and to the left */
            top: 15px;
            left: 15px;
            right: 15px;
            bottom: 120px;
        }
        
        .text-chat-panel.pos-top-right {
            /* Orb at bottom-left: chat above and to the right */
            top: 15px;
            left: 15px;
            right: 15px;
            bottom: 120px;
        }
        
        .text-chat-panel.pos-bottom-left {
            /* Orb at top-right: chat below and to the left */
            top: 120px;
            left: 15px;
            right: 15px;
            bottom: 15px;
            transform-origin: top center;
        }
        
        .text-chat-panel.pos-bottom-left.visible {
            transform: scale(1) translateY(0);
        }
        
        .text-chat-panel.pos-bottom-right {
            /* Orb at top-left: chat below and to the right */
            top: 120px;
            left: 15px;
            right: 15px;
            bottom: 15px;
            transform-origin: top center;
        }
        
        .text-chat-panel.pos-bottom-right.visible {
            transform: scale(1) translateY(0);
        }
        
        /* Empty state */
        .chat-empty {
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100%;
            padding: 40px 20px;
            color: rgba(255, 255, 255, 0.4);
            text-align: center;
        }
        
        .chat-empty svg {
            width: 48px;
            height: 48px;
            fill: currentColor;
            margin-bottom: 16px;
            opacity: 0.5;
        }
        
        .chat-empty-text {
            font-size: 13px;
            line-height: 1.6;
        }
    </style>
</head>
<body>
    <!-- Context Menu - outside container for proper positioning -->
    <div class="orb-context-menu" id="contextMenu">
        <div class="context-menu-item" id="menuTextChat">
            <svg viewBox="0 0 24 24"><path d="M20 2H4c-1.1 0-2 .9-2 2v18l4-4h14c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm0 14H6l-2 2V4h16v12z"/></svg>
            <span>Text Chat</span>
        </div>
        <div class="context-menu-item" id="menuVoice">
            <svg viewBox="0 0 24 24"><path d="M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3zm-1-9c0-.55.45-1 1-1s1 .45 1 1v6c0 .55-.45 1-1 1s-1-.45-1-1V5zm6 6c0 2.76-2.24 5-5 5s-5-2.24-5-5H5c0 3.53 2.61 6.43 6 6.92V21h2v-3.08c3.39-.49 6-3.39 6-6.92h-2z"/></svg>
            <span>Voice Mode</span>
        </div>
        <div class="context-menu-divider"></div>
        <div class="context-menu-item" id="menuSettings">
            <svg viewBox="0 0 24 24"><path d="M19.14 12.94c.04-.31.06-.63.06-.94 0-.31-.02-.63-.06-.94l2.03-1.58c.18-.14.23-.41.12-.61l-1.92-3.32c-.12-.22-.37-.29-.59-.22l-2.39.96c-.5-.38-1.03-.7-1.62-.94l-.36-2.54c-.04-.24-.24-.41-.48-.41h-3.84c-.24 0-.43.17-.47.41l-.36 2.54c-.59.24-1.13.57-1.62.94l-2.39-.96c-.22-.08-.47 0-.59.22L2.74 8.87c-.12.21-.08.47.12.61l2.03 1.58c-.04.31-.06.63-.06.94s.02.63.06.94l-2.03 1.58c-.18.14-.23.41-.12.61l1.92 3.32c.12.22.37.29.59.22l2.39-.96c.5.38 1.03.7 1.62.94l.36 2.54c.05.24.24.41.48.41h3.84c.24 0 .44-.17.47-.41l.36-2.54c.59-.24 1.13-.56 1.62-.94l2.39.96c.22.08.47 0 .59-.22l1.92-3.32c.12-.22.07-.47-.12-.61l-2.01-1.58zM12 15.6c-1.98 0-3.6-1.62-3.6-3.6s1.62-3.6 3.6-3.6 3.6 1.62 3.6 3.6-1.62 3.6-3.6 3.6z"/></svg>
            <span>Settings</span>
        </div>
    </div>
    
    <!-- Text Chat Panel - outside container for proper positioning -->

    <div class="orb-container">
        <div class="orb" id="orb" title="Click to start listening">
            <div class="mic-overlay">
                <svg viewBox="0 0 24 24">
                    <path d="M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3zm-1-9c0-.55.45-1 1-1s1 .45 1 1v6c0 .55-.45 1-1 1s-1-.45-1-1V5zm6 6c0 2.76-2.24 5-5 5s-5-2.24-5-5H5c0 3.53 2.61 6.43 6 6.92V21h2v-3.08c3.39-.49 6-3.39 6-6.92h-2z"/>
                </svg>
            </div>
        </div>
        <div class="ring"></div>
        <div class="ring-secondary"></div>
        <div class="transcript-tooltip" id="transcript"></div>
    </div>
        
    <!-- Text Chat Panel -->
        <div class="text-chat-panel" id="textChatPanel">
            <div class="chat-header">
                <div class="chat-header-title">
                    <svg viewBox="0 0 24 24"><path d="M20 2H4c-1.1 0-2 .9-2 2v18l4-4h14c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm0 14H6l-2 2V4h16v12z"/></svg>
                    <span>Chat</span>
                </div>
                <button class="chat-close-btn" id="chatCloseBtn">
                    <svg viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
                </button>
            </div>
            <div class="chat-messages" id="chatMessages">
                <div class="chat-empty">
                    <svg viewBox="0 0 24 24"><path d="M20 2H4c-1.1 0-2 .9-2 2v18l4-4h14c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm0 14H6l-2 2V4h16v12z"/></svg>
                    <div class="chat-empty-text">
                        Type a command or ask a question.<br/>
                        Same as voice, just quieter.
                    </div>
                </div>
            </div>
            <div class="chat-input-container">
                <div class="chat-input-wrapper">
                    <input type="text" class="chat-input" id="chatInput" placeholder="Type a message..." autocomplete="off" />
                    <button class="chat-send-btn" id="chatSendBtn">
                        <svg viewBox="0 0 24 24"><path d="M2.01 21L23 12 2.01 3 2 10l15 2-15 2z"/></svg>
                    </button>
                </div>
            </div>
        </div>
    
    <script>
        // State
        let isListening = false;
        let isConnected = false;
        let isSessionReady = false;
        // NOTE: pendingSpeak removed - all TTS handled by backend
        let removeEventListener = null;
        let audioContext = null;
        let mediaStream = null;
        let processor = null;
        let ttsAudio = null;
        
        // Audio playback for OpenAI Realtime TTS
        let ttsAudioContext = null;
        let ttsAudioChunks = [];
        let isSpeaking = false;
        
        // Deduplication and feedback loop prevention
        let lastProcessedTranscript = '';
        let lastProcessedTime = 0;
        const DEDUP_WINDOW_MS = 2000; // Ignore same transcript within 2 seconds
        let ttsEndTime = 0;
        
        // Common noise/filler words to ignore (background noise often produces these)
        const NOISE_WORDS = new Set([
            'hmm', 'hm', 'um', 'uh', 'ah', 'oh', 'eh', 'er', 'mm',
            'yeah', 'yep', 'yes', 'no', 'nope', 'ok', 'okay', 'mhm', 'uh-huh',
            'the', 'a', 'i', 'it', 'is', 'and', 'but', 'so', 'like',
            'bye', 'hi', 'hey', 'hello', 'thanks', 'thank you',
            'right', 'alright', 'sure', 'well', 'now', 'just',
            // Common misheard noise
            'you', 'me', 'we', 'he', 'she', 'they', 'that', 'this',
            'what', 'huh', 'wow', 'ooh', 'aah', 'ugh'
        ]);
        
        // Check if transcript is likely just noise
        function isLikelyNoise(text) {
            if (!text) return true;
            
            const normalized = text.toLowerCase().trim();
            
            // Too short
            if (normalized.length <= 3) return true;
            
            // Single noise word
            if (NOISE_WORDS.has(normalized)) return true;
            
            // Remove punctuation and check again
            const stripped = normalized.replace(/[.,!?;:'"]/g, '').trim();
            if (NOISE_WORDS.has(stripped)) return true;
            
            // Check if it's just repeated characters (like "aaaah" or "mmmm")
            if (/^(.)\1{2,}$/.test(stripped)) return true;
            
            // Very short after stripping (like "I." or "A!")
            if (stripped.length <= 2) return true;
            
            // Two-word combos that are likely noise (e.g., "oh yeah", "uh huh")
            const words = stripped.split(/\s+/);
            if (words.length <= 2 && words.every(w => NOISE_WORDS.has(w) || w.length <= 2)) {
                return true;
            }
            
            return false;
        }
        
        // Track function call transcripts to avoid double-processing
        // When using function calling, we get the transcript via function_call_transcript
        // AND via the regular transcript event - we must skip the duplicate
        let lastFunctionCallTranscript = '';
        let lastFunctionCallTime = 0;
        const FUNCTION_CALL_DEDUP_MS = 5000; // Skip regular transcript if function call handled it within 5s
        const TTS_COOLDOWN_MS = 4000; // Ignore transcripts for 4s after TTS ends (prevent echo/ambient pickup)
        // NOTE: pendingSpeakQueue removed - all TTS handled by backend speechQueue
        
        // Store pending function call ID for multi-turn conversations
        // When agent returns needsInput, we use this to respond via the proper TTS channel
        let pendingFunctionCallId = null;
        
        // Multi-turn conversation state - when true, keep mic active after TTS
        // Set when agent returns needsInput, cleared when follow-up is processed
        let isWaitingForUserInput = false;
        
        // Speech detection and silence timeout
        // The idea: detect when user STARTS speaking, then wait for silence AFTER speech
        let lastSpeechTime = 0;           // When we last detected user speaking (interim transcript)
        let hasSpokenThisSession = false; // Has the user said anything meaningful?
        let silenceTimeoutId = null;
        let noSpeechTimeoutId = null;
        
        const SILENCE_AFTER_SPEECH_MS = 5000;  // 5 seconds of silence after speech = done
        const NO_SPEECH_TIMEOUT_MS = 60000;    // 60 seconds with no speech at all = auto-stop
        
        // Called when we detect the user is actively speaking (interim transcripts)
        function onSpeechDetected() {
            lastSpeechTime = Date.now();
            hasSpokenThisSession = true;
            
            // Clear the silence timeout while they're speaking
            clearSilenceTimeout();
            
            // Clear the no-speech timeout since they are speaking
            clearNoSpeechTimeout();
        }
        
        // Called when speech appears to have ended (final transcript received or silence)
        function onSpeechEnded() {
            // Start silence timeout - if no more speech, consider them done
            clearSilenceTimeout();
            silenceTimeoutId = setTimeout(() => {
                // CRITICAL: Don't auto-stop if we're waiting for user input (multi-turn conversation)
                if (isWaitingForUserInput) {
                    console.log('[Orb] Silence timeout but waiting for user input - keeping mic active');
                    return;
                }
                if (isListening && !isSpeaking && hasSpokenThisSession) {
                    const timeSinceSpeech = Date.now() - lastSpeechTime;
                    if (timeSinceSpeech >= SILENCE_AFTER_SPEECH_MS) {
                        console.log('[Orb] Silence after speech - auto-stopping');
                        stopListening();
                    }
                }
            }, SILENCE_AFTER_SPEECH_MS);
        }
        
        function clearSilenceTimeout() {
            if (silenceTimeoutId) {
                clearTimeout(silenceTimeoutId);
                silenceTimeoutId = null;
            }
        }
        
        // No speech at all timeout - in case user activates but never speaks
        function startNoSpeechTimeout() {
            clearNoSpeechTimeout();
            noSpeechTimeoutId = setTimeout(() => {
                // CRITICAL: Don't auto-stop if we're waiting for user input (multi-turn conversation)
                if (isWaitingForUserInput) {
                    console.log('[Orb] No-speech timeout but waiting for user input - keeping mic active');
                    return;
                }
                if (isListening && !isSpeaking && !hasSpokenThisSession) {
                    console.log('[Orb] No speech detected - auto-stopping');
                    stopListening();
                }
            }, NO_SPEECH_TIMEOUT_MS);
        }
        
        function clearNoSpeechTimeout() {
            if (noSpeechTimeoutId) {
                clearTimeout(noSpeechTimeoutId);
                noSpeechTimeoutId = null;
            }
        }
        
        function clearAllSpeechTimers() {
            clearSilenceTimeout();
            clearNoSpeechTimeout();
        }
        
        // Initialize audio context for TTS playback
        function initTTSAudio() {
            if (!ttsAudioContext) {
                ttsAudioContext = new AudioContext({ sampleRate: 24000 });
            }
            return ttsAudioContext;
        }
        
        // Convert base64 PCM16 to Float32 for playback
        function base64ToFloat32(base64) {
            const binaryString = atob(base64);
            const bytes = new Uint8Array(binaryString.length);
            for (let i = 0; i < binaryString.length; i++) {
                bytes[i] = binaryString.charCodeAt(i);
            }
            const pcm16 = new Int16Array(bytes.buffer);
            const float32 = new Float32Array(pcm16.length);
            for (let i = 0; i < pcm16.length; i++) {
                float32[i] = pcm16[i] / 32768;
            }
            return float32;
        }
        
        // Play a pleasant ready chime using Web Audio API
        // This is a short tone that won't trigger function calling
        async function playReadyChime() {
            try {
                const ctx = new AudioContext();
                
                // Resume if suspended (autoplay policy)
                if (ctx.state === 'suspended') {
                    await ctx.resume();
                }
                
                const now = ctx.currentTime;
                
                // Create a pleasant two-tone ascending chime
                const frequencies = [523.25, 659.25]; // C5 and E5 - pleasant major third
                const duration = 0.12; // Very short
                const gap = 0.08;
                
                frequencies.forEach((freq, i) => {
                    const osc = ctx.createOscillator();
                    const gain = ctx.createGain();
                    
                    osc.type = 'sine';
                    osc.frequency.value = freq;
                    
                    // Gentle envelope - louder for audibility
                    const startTime = now + (i * (duration + gap));
                    gain.gain.setValueAtTime(0, startTime);
                    gain.gain.linearRampToValueAtTime(0.3, startTime + 0.02); // Louder
                    gain.gain.exponentialRampToValueAtTime(0.01, startTime + duration);
                    
                    osc.connect(gain);
                    gain.connect(ctx.destination);
                    
                    osc.start(startTime);
                    osc.stop(startTime + duration);
                });
                
                // Close context after chime finishes
                setTimeout(() => ctx.close(), 500);
                console.log('[Orb] Ready chime played');
            } catch (e) {
                console.warn('[Orb] Could not play ready chime:', e);
            }
        }
        
        // Play accumulated audio chunks
        async function playTTSAudio() {
            if (ttsAudioChunks.length === 0) return;
            
            const ctx = initTTSAudio();
            if (ctx.state === 'suspended') {
                await ctx.resume();
            }
            
            // Combine all chunks
            const totalLength = ttsAudioChunks.reduce((sum, chunk) => sum + chunk.length, 0);
            const combined = new Float32Array(totalLength);
            let offset = 0;
            for (const chunk of ttsAudioChunks) {
                combined.set(chunk, offset);
                offset += chunk.length;
            }
            
            // Create buffer and play
            const buffer = ctx.createBuffer(1, combined.length, 24000);
            buffer.copyToChannel(combined, 0);
            
            const source = ctx.createBufferSource();
            source.buffer = buffer;
            source.connect(ctx.destination);
            source.start();
            
            console.log('[Orb] Playing TTS audio, samples:', combined.length);
            ttsAudioChunks = [];
        }
        
        // ==================== PERSISTENT AUDIO LISTENER ====================
        // CRITICAL: This listener receives audio even when orb has "disconnected"
        // Handles the case where backend speaks after orb stopped listening
        (function initPersistentAudioListener() {
            if (window.orbAPI?.onEvent) {
                window.orbAPI.onEvent((event) => {
                    // Only handle audio events in this persistent listener
                    if (event.type === 'audio_delta' && event.audio) {
                        console.log('[Orb] Persistent: audio_delta received');
                        const float32 = base64ToFloat32(event.audio);
                        ttsAudioChunks.push(float32);
                        isSpeaking = true;
                    } else if (event.type === 'audio_done') {
                        console.log('[Orb] Persistent: audio_done, playing', ttsAudioChunks.length, 'chunks');
                        if (ttsAudioChunks.length > 0) {
                            playTTSAudio();
                        }
                        isSpeaking = false;
                        ttsEndTime = Date.now();
                    } else if (event.type === 'clear_audio_buffer') {
                        console.log('[Orb] Persistent: clearing audio buffer');
                        ttsAudioChunks = [];
                        isSpeaking = false;
                    }
                });
                console.log('[Orb] Persistent audio listener registered');
            }
        })();
        // ============================================================
        
        // ==================== FRONTEND TTS REMOVED ====================
        // ALL TTS is now handled by the backend via exchange-bridge.js -> realtimeSpeech.speak()
        // The frontend only tracks TTS state for UI purposes (cooldown, etc.)
        // This eliminates duplicate TTS paths and race conditions.
        // ============================================================
        
        // ==================== DISAMBIGUATION STATE ====================
        let pendingDisambiguation = null;
        let disambiguationListenerRemove = null;
        
        /**
         * Handle disambiguation flow
         * @param {Object} result - Classification result with clarification data
         * @param {string} originalTranscript - The original user transcript
         */
        async function handleDisambiguation(result, originalTranscript) {
            console.log('[Orb] Starting disambiguation flow:', result);
            
            // Create disambiguation state
            const disambiguationState = {
                id: 'disamb_' + Date.now(),
                originalTranscript: originalTranscript,
                question: result.clarificationQuestion || 'What did you mean?',
                options: result.clarificationOptions || [],
                createdAt: Date.now(),
                expiresAt: Date.now() + 30000, // 30 second timeout
            };
            
            pendingDisambiguation = disambiguationState;
            
            // NOTE: Backend speaks via exchange-bridge.js
            console.log('[Orb] Disambiguation question (backend speaks):', disambiguationState.question);
            
            // Show disambiguation in HUD
            if (window.orbAPI.showDisambiguation) {
                try {
                    await window.orbAPI.showDisambiguation(disambiguationState);
                } catch (e) {
                    console.warn('[Orb] Could not show disambiguation HUD:', e);
                }
            }
            
            // Show transcript with question
            showTranscript(disambiguationState.question, false);
            
            // Set up listener for disambiguation response
            setupDisambiguationListeners();
        }
        
        /**
         * Set up listeners for disambiguation responses
         */
        function setupDisambiguationListeners() {
            // Clean up previous listener if any
            if (disambiguationListenerRemove) {
                disambiguationListenerRemove();
                disambiguationListenerRemove = null;
            }
            
            // Listen for option selection from HUD
            if (window.orbAPI.onDisambiguationSelected) {
                disambiguationListenerRemove = window.orbAPI.onDisambiguationSelected((selection) => {
                    console.log('[Orb] Disambiguation option selected:', selection);
                    resolveDisambiguation(selection);
                });
            }
            
            // Set timeout to cancel disambiguation
            setTimeout(() => {
                if (pendingDisambiguation) {
                    console.log('[Orb] Disambiguation timed out');
                    cancelDisambiguation();
                    // NOTE: Backend handles timeout feedback
                }
            }, 30000);
        }
        
        /**
         * Resolve disambiguation with selected option
         * @param {Object} selection - { optionIndex, option, mergedTranscript }
         */
        async function resolveDisambiguation(selection) {
            if (!pendingDisambiguation) return;
            
            console.log('[Orb] Resolving disambiguation:', selection);
            
            const state = pendingDisambiguation;
            pendingDisambiguation = null;
            
            // Clean up listeners
            if (disambiguationListenerRemove) {
                disambiguationListenerRemove();
                disambiguationListenerRemove = null;
            }
            
            // Hide transcript
            hideTranscript();
            
            // If we got a selected option with action, submit it directly
            if (selection.option && selection.option.action) {
                console.log('[Orb] Disambiguation selected - backend handles response');
                
                // Submit the selected action directly
                try {
                    const result = await window.orbAPI.submitAction({
                        action: selection.option.action,
                        params: selection.option.params || {},
                        originalTranscript: state.originalTranscript,
                        clarification: selection.option.label,
                    });
                    
                    console.log('[Orb] Disambiguation action submitted:', result);
                    // NOTE: Backend speaks result via exchange-bridge.js
                } catch (err) {
                    console.error('[Orb] Error submitting disambiguation action:', err);
                    // NOTE: Backend handles error feedback
                }
            } else if (selection.mergedTranscript) {
                // Re-classify with merged transcript
                console.log('[Orb] Re-trying with merged transcript');
                await processVoiceCommand(selection.mergedTranscript);
            }
        }
        
        /**
         * Cancel pending disambiguation
         */
        function cancelDisambiguation() {
            if (!pendingDisambiguation) return;
            
            console.log('[Orb] Cancelling disambiguation');
            
            pendingDisambiguation = null;
            
            if (disambiguationListenerRemove) {
                disambiguationListenerRemove();
                disambiguationListenerRemove = null;
            }
            
            hideTranscript();
            
            if (window.orbAPI.cancelDisambiguation) {
                window.orbAPI.cancelDisambiguation();
            }
        }
        
        /**
         * Check if we're waiting for disambiguation
         */
        function isAwaitingDisambiguation() {
            return pendingDisambiguation !== null && 
                   pendingDisambiguation.expiresAt > Date.now();
        }
        
        // ==================== VOICE COMMAND PROCESSING ====================
        
        // Process voice command - classify and queue via SDK
        async function processVoiceCommand(transcript) {
            console.log('[Orb] Processing command:', transcript);
            
            // Check if Agent Composer is active - relay voice to it
            if (window.orbAPI?.isComposerActive) {
                try {
                    const composerActive = await window.orbAPI.isComposerActive();
                    if (composerActive) {
                        console.log('[Orb] Relaying to Agent Composer:', transcript);
                        
                        // Cancel any AI response since we're handling locally
                        if (window.orbAPI.cancelResponse) {
                            await window.orbAPI.cancelResponse();
                        }
                        
                        const relayed = await window.orbAPI.relayToComposer(transcript);
                        if (relayed) {
                            // Don't process further - Composer will handle it
                            return;
                        }
                    }
                } catch (e) {
                    console.warn('[Orb] Could not check composer status:', e);
                }
            }
            
            // Check if this is a response to a pending disambiguation
            if (isAwaitingDisambiguation()) {
                console.log('[Orb] Processing as disambiguation response');
                
                // Try to match to an option
                const state = pendingDisambiguation;
                const normalizedResponse = transcript.toLowerCase().trim();
                
                // Try number matching
                const numberWords = {
                    'one': 0, 'first': 0, '1': 0,
                    'two': 1, 'second': 1, '2': 1,
                    'three': 2, 'third': 2, '3': 2,
                    'four': 3, 'fourth': 3, '4': 3,
                    'five': 4, 'fifth': 4, '5': 4,
                };
                
                let matchedIndex = -1;
                for (const [word, index] of Object.entries(numberWords)) {
                    if (normalizedResponse.includes(word) && index < state.options.length) {
                        matchedIndex = index;
                        break;
                    }
                }
                
                // Try label matching if number didn't work
                if (matchedIndex === -1) {
                    matchedIndex = state.options.findIndex(opt =>
                        normalizedResponse.includes(opt.label.toLowerCase()) ||
                        opt.label.toLowerCase().includes(normalizedResponse)
                    );
                }
                
                if (matchedIndex >= 0) {
                    resolveDisambiguation({
                        optionIndex: matchedIndex,
                        option: state.options[matchedIndex],
                        mergedTranscript: `${state.originalTranscript} (clarification: ${state.options[matchedIndex].label})`,
                    });
                } else {
                    // Use the voice response as additional context
                    resolveDisambiguation({
                        mergedTranscript: `${state.originalTranscript} (user clarified: ${transcript})`,
                    });
                }
                return;
            }
            
            try {
                // Submit to SDK for classification and queueing
                // The SDK will:
                // 1. Classify the transcript
                // 2. Create a task
                // 3. Queue it for execution
                // 4. Dispatch to an agent
                // 5. Broadcast events (which update HUD automatically)
                const result = await window.orbAPI.submit(transcript);
                console.log('[Orb] SDK submit result:', result);
                
                // Check if backend already handled AND suppressed AI response
                // If suppressAIResponse is false, we should speak the message even if handled
                if (result.suppressAIResponse === true) {
                    console.log('[Orb] Response suppressed by backend, skipping frontend speech');
                    hideTranscript();
                    return;
                }
                
                // If backend handled it but didn't suppress, backend already spoke
                if (result.handled && result.message && result.suppressAIResponse === false) {
                    console.log('[Orb] Backend handled with message:', result.message);
                    // NOTE: Backend speaks via exchange-bridge.js - don't duplicate
                    hideTranscript();
                    return;
                }
                
                // Check if disambiguation is needed
                if (result.clarificationNeeded && result.clarificationOptions?.length > 0) {
                    await handleDisambiguation(result, transcript);
                    return;
                }
                
                if (result.queued && result.task) {
                    // Task was successfully queued
                    // NOTE: Backend speaks acknowledgment via exchange-bridge.js
                    console.log('[Orb] Task queued:', result.task.id, 'action:', result.action);
                    
                } else if (result.handled) {
                    // Agent already handled this (follow-up conversation, etc.)
                    // Don't speak - the agent already spoke via direct TTS
                    console.log('[Orb] Task handled by agent, action:', result.action, 'needsInput:', result.needsInput);
                    
                    // Just update HUD if needed
                    if (result.needsInput) {
                        // Agent needs more input - keep waiting
                        isWaitingForUserInput = true;
                        console.log('[Orb] Multi-turn continues: still waiting for input');
                        try {
                            await window.orbAPI.sendHUDResult({
                                success: true,
                                message: result.message,
                                needsInput: true,
                            });
                        } catch (e) {
                            console.warn('[Orb] Could not update HUD:', e);
                        }
                    } else {
                        // Follow-up completed - clear multi-turn state
                        isWaitingForUserInput = false;
                        console.log('[Orb] Multi-turn complete: follow-up handled');
                    }
                    
                } else if (result.classified && result.action) {
                    // Classified but not queued (SDK not fully running?)
                    // Fall back to showing result in HUD manually
                    const task = {
                        id: 'task_' + Date.now(),
                        transcript: transcript,
                        action: result.action,
                        params: result.params || {},
                        confidence: result.confidence,
                        status: 'running',
                        timestamp: Date.now()
                    };
                    
                    try {
                        await window.orbAPI.showHUD(task);
                    } catch (e) {
                        console.warn('[Orb] Could not show HUD:', e);
                    }
                    
                    // NOTE: This is a fallback path - backend should handle TTS
                    // If we reach here, the exchange may not be running
                    console.log('[Orb] Fallback path - classified but not queued:', result.action);
                    
                    // Just update HUD after delay
                    setTimeout(async () => {
                        try {
                            await window.orbAPI.sendHUDResult({
                                success: true,
                                message: `Action "${result.action}" completed`,
                            });
                        } catch (e) {
                            console.warn('[Orb] Could not send HUD result:', e);
                        }
                    }, 1000);
                    
                } else {
                    // No action recognized
                    const task = {
                        id: 'task_' + Date.now(),
                        transcript: transcript,
                        action: 'Unknown',
                        status: 'completed',
                        timestamp: Date.now()
                    };
                    
                    try {
                        await window.orbAPI.showHUD(task);
                        await window.orbAPI.sendHUDResult({
                            success: true,
                            message: 'No specific action recognized. Transcript recorded.',
                        });
                    } catch (e) {
                        console.warn('[Orb] Could not update HUD:', e);
                    }
                    
                    // NOTE: Backend handles acknowledgments - transcript was recorded
                    console.log('[Orb] No action recognized, transcript recorded');
                }
                
            } catch (err) {
                console.error('[Orb] Submit error:', err);
                
                // Show error in HUD
                try {
                    await window.orbAPI.sendHUDResult({
                        success: false,
                        error: err.message || 'Processing failed',
                    });
                } catch (e) {
                    console.warn('[Orb] Could not send HUD error:', e);
                }
                
                // NOTE: For errors, we should speak - use backend API
                try {
                    await window.orbAPI.speak('Sorry, something went wrong');
                } catch (speakErr) {
                    console.warn('[Orb] Could not speak error:', speakErr);
                }
            }
        }
        
        // Listen for retry requests from HUD
        if (window.orbAPI.onHUDRetry) {
            window.orbAPI.onHUDRetry((task) => {
                console.log('[Orb] Retry requested for:', task.transcript);
                if (task.transcript) {
                    processVoiceCommand(task.transcript);
                }
            });
        }
        
        // Listen for task events (needs-input, completed, failed)
        // NOTE: exchange-bridge now speaks directly via realtimeSpeech.speak()
        // so we DON'T speak here to avoid duplicates
        if (window.orbAPI.onTaskEvent) {
            window.orbAPI.onTaskEvent(async (event) => {
                // Handle multi-turn conversation prompts
                if (event.type === 'needs-input') {
                    console.log('[Orb] Agent needs input, prompt:', event.prompt);
                    // DON'T speak here - exchange-bridge already speaks directly via realtimeSpeech
                    
                    // CRITICAL: Mark that we're waiting for user input
                    // This prevents disconnection after TTS completes
                    isWaitingForUserInput = true;
                    console.log('[Orb] Multi-turn: waiting for user input, keeping mic active');
                    
                    // If we're not listening, start listening again for the follow-up
                    if (!isListening && !isSpeaking) {
                        console.log('[Orb] Restarting listening for multi-turn conversation');
                        startListening();
                    }
                }
                // Handle task completion (from async exchange path)
                else if (event.type === 'completed') {
                    console.log('[Orb] Task completed, result:', event.result);
                    // DON'T speak here - exchange-bridge already speaks directly via realtimeSpeech
                    
                    // Clear multi-turn state
                    isWaitingForUserInput = false;
                }
                // Handle task failure
                else if (event.type === 'failed') {
                    console.log('[Orb] Task failed:', event.reason || event.error);
                    // DON'T speak here - exchange-bridge already speaks directly via realtimeSpeech
                    
                    // Clear multi-turn state
                    isWaitingForUserInput = false;
                }
            });
        }
        
        // DOM elements
        const orb = document.getElementById('orb');
        const transcriptEl = document.getElementById('transcript');
        const contextMenu = document.getElementById('contextMenu');
        const textChatPanel = document.getElementById('textChatPanel');
        const chatMessages = document.getElementById('chatMessages');
        
        // Debug: check if elements exist
        console.log('[Orb] Elements found:', {
            orb: !!orb,
            contextMenu: !!contextMenu,
            textChatPanel: !!textChatPanel
        });
        const chatInput = document.getElementById('chatInput');
        const chatSendBtn = document.getElementById('chatSendBtn');
        const chatCloseBtn = document.getElementById('chatCloseBtn');
        
        // Text chat state
        let isTextChatOpen = false;
        let chatHistory = [];
        let chatInactivityTimer = null;
        const CHAT_INACTIVITY_TIMEOUT = 30000; // 30 seconds
        
        // Reset the chat inactivity timer
        function resetChatInactivityTimer() {
            if (chatInactivityTimer) {
                clearTimeout(chatInactivityTimer);
                chatInactivityTimer = null;
            }
            
            // Only set timer if chat is open
            if (isTextChatOpen) {
                chatInactivityTimer = setTimeout(() => {
                    console.log('[TextChat] Auto-closing due to inactivity');
                    closeTextChat();
                }, CHAT_INACTIVITY_TIMEOUT);
            }
        }
        
        // Clear inactivity timer
        function clearChatInactivityTimer() {
            if (chatInactivityTimer) {
                clearTimeout(chatInactivityTimer);
                chatInactivityTimer = null;
            }
        }
        
        // ==================== CONTEXT MENU ====================
        
        function showContextMenu(x, y) {
            console.log('[Orb] Showing context menu at window coords:', x, y);
            
            const MIN_MARGIN = 8;
            
            // Window dimensions (the BrowserWindow client area)
            const windowWidth = window.innerWidth;
            const windowHeight = window.innerHeight;
            
            // Window position on physical screen
            const windowX = window.screenX;
            const windowY = window.screenY;
            
            // Physical screen dimensions
            const screenWidth = window.screen.availWidth;
            const screenHeight = window.screen.availHeight;
            
            // Measure actual menu size (show off-screen first to get dimensions)
            contextMenu.style.left = '-9999px';
            contextMenu.style.top = '-9999px';
            contextMenu.classList.add('visible');
            
            const rect = contextMenu.getBoundingClientRect();
            const menuWidth = rect.width;
            const menuHeight = rect.height;
            
            // #region agent log
            fetch('http://127.0.0.1:7242/ingest/54746cc5-c924-4bb5-9e76-3f6b729e6870',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'orb.html:showContextMenu',message:'Context menu dimensions',data:{clickX:x,clickY:y,windowWidth,windowHeight,windowX,windowY,screenWidth,screenHeight,menuWidth,menuHeight,spaceRightInWindow:windowWidth-x,spaceBottomInWindow:windowHeight-y},timestamp:Date.now(),sessionId:'debug-session',hypothesisId:'A,B,C'})}).catch(()=>{});
            // #endregion
            
            // Calculate where menu would appear in screen coordinates
            let screenMenuX = windowX + x;
            let screenMenuY = windowY + y;
            
            console.log('[Orb] Screen coords - window:', windowX, windowY, 'click:', screenMenuX, screenMenuY, 'screen size:', screenWidth, screenHeight, 'menu size:', menuWidth, menuHeight);
            
            // Horizontal: check if menu would go off right edge of screen
            if (screenMenuX + menuWidth > screenWidth - MIN_MARGIN) {
                // Flip to left of cursor
                screenMenuX = Math.max(MIN_MARGIN, screenMenuX - menuWidth);
            }
            // Ensure not off left edge of screen
            if (screenMenuX < MIN_MARGIN) {
                screenMenuX = MIN_MARGIN;
            }
            
            // Vertical: check if menu would go off bottom edge of screen
            if (screenMenuY + menuHeight > screenHeight - MIN_MARGIN) {
                // Flip to above cursor
                screenMenuY = screenMenuY - menuHeight;
            }
            // Ensure not off top edge of screen
            if (screenMenuY < MIN_MARGIN) {
                screenMenuY = MIN_MARGIN;
            }
            
            // Convert back to window-relative coordinates
            let finalX = screenMenuX - windowX;
            let finalY = screenMenuY - windowY;
            
            // #region agent log
            fetch('http://127.0.0.1:7242/ingest/54746cc5-c924-4bb5-9e76-3f6b729e6870',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'orb.html:showContextMenu:afterScreenCalc',message:'After screen boundary calc',data:{screenMenuX,screenMenuY,finalXBeforeClamp:finalX,finalYBeforeClamp:finalY},timestamp:Date.now(),sessionId:'debug-session',hypothesisId:'A,B'})}).catch(()=>{});
            // #endregion
            
            // Also ensure menu stays within window bounds (for very edge cases)
            finalX = Math.max(0, Math.min(finalX, windowWidth - menuWidth));
            finalY = Math.max(0, Math.min(finalY, windowHeight - menuHeight));
            
            // #region agent log
            const wouldBeClippedRight = (finalX + menuWidth) > windowWidth;
            const wouldBeClippedBottom = (finalY + menuHeight) > windowHeight;
            fetch('http://127.0.0.1:7242/ingest/54746cc5-c924-4bb5-9e76-3f6b729e6870',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'orb.html:showContextMenu:finalPosition',message:'Final position and clipping check',data:{finalX,finalY,menuRightEdge:finalX+menuWidth,menuBottomEdge:finalY+menuHeight,windowWidth,windowHeight,wouldBeClippedRight,wouldBeClippedBottom,menuFitsInWindow:(menuWidth<=windowWidth && menuHeight<=windowHeight)},timestamp:Date.now(),sessionId:'debug-session',hypothesisId:'C,D'})}).catch(()=>{});
            // #endregion
            
            // Apply final position
            contextMenu.style.position = 'fixed';
            contextMenu.style.left = finalX + 'px';
            contextMenu.style.top = finalY + 'px';
            contextMenu.style.bottom = 'auto';
            contextMenu.style.right = 'auto';
            
            console.log('[Orb] Context menu final position:', finalX, finalY);
        }
        
        function hideContextMenu() {
            contextMenu.classList.remove('visible');
        }
        
        // Context menu handlers
        document.getElementById('menuTextChat').addEventListener('click', () => {
            hideContextMenu();
            openTextChat();
        });
        
        document.getElementById('menuVoice').addEventListener('click', () => {
            hideContextMenu();
            closeTextChat();
            if (!isListening) {
                startListening();
            }
        });
        
        document.getElementById('menuSettings').addEventListener('click', () => {
            hideContextMenu();
            // Open settings via IPC
            if (window.orbAPI?.openSettings) {
                window.orbAPI.openSettings();
            }
        });
        
        // Close menu on click outside
        document.addEventListener('click', (e) => {
            if (!contextMenu.contains(e.target) && e.target !== orb) {
                hideContextMenu();
            }
        });
        
        // ==================== TEXT CHAT ====================
        
        // Track current anchor for collapse
        let currentChatAnchor = 'bottom-right';
        
        async function openTextChat() {
            isTextChatOpen = true;
            
            // Determine which quadrant of the screen the orb is in
            const windowX = window.screenX;
            const windowY = window.screenY;
            const screenWidth = window.screen.availWidth;
            const screenHeight = window.screen.availHeight;
            
            // Calculate center points
            const windowCenterX = windowX + (window.outerWidth / 2);
            const windowCenterY = windowY + (window.outerHeight / 2);
            const screenCenterX = screenWidth / 2;
            const screenCenterY = screenHeight / 2;
            
            // Determine quadrant
            const isRight = windowCenterX > screenCenterX;
            const isBottom = windowCenterY > screenCenterY;
            
            // Remove existing position classes
            const orbContainer = document.querySelector('.orb-container');
            orbContainer.classList.remove('orb-bottom-right', 'orb-bottom-left', 'orb-top-right', 'orb-top-left');
            textChatPanel.classList.remove('pos-top-left', 'pos-top-right', 'pos-bottom-left', 'pos-bottom-right');
            
            // Determine anchor and apply classes based on quadrant
            // Chat panel appears opposite to orb position
            if (isBottom && isRight) {
                // Orb at bottom-right of screen: orb stays bottom-right, chat above
                currentChatAnchor = 'bottom-right';
                orbContainer.classList.add('orb-bottom-right');
                textChatPanel.classList.add('pos-top-left');
            } else if (isBottom && !isRight) {
                // Orb at bottom-left of screen: orb stays bottom-left, chat above
                currentChatAnchor = 'bottom-left';
                orbContainer.classList.add('orb-bottom-left');
                textChatPanel.classList.add('pos-top-right');
            } else if (!isBottom && isRight) {
                // Orb at top-right of screen: orb moves to top-right, chat below
                currentChatAnchor = 'top-right';
                orbContainer.classList.add('orb-top-right');
                textChatPanel.classList.add('pos-bottom-left');
            } else {
                // Orb at top-left of screen: orb moves to top-left, chat below
                currentChatAnchor = 'top-left';
                orbContainer.classList.add('orb-top-left');
                textChatPanel.classList.add('pos-bottom-right');
            }
            
            console.log('[TextChat] Position:', isRight ? 'right' : 'left', isBottom ? 'bottom' : 'top', 'anchor:', currentChatAnchor);
            
            // Expand the window to accommodate the chat panel
            if (window.orbAPI?.expandForChat) {
                try {
                    await window.orbAPI.expandForChat(currentChatAnchor);
                } catch (e) {
                    console.warn('[TextChat] Could not expand window:', e);
                }
            }
            
            // Show the chat panel
            textChatPanel.classList.add('visible');
            setTimeout(() => chatInput.focus(), 150);
            // Start inactivity timer
            resetChatInactivityTimer();
        }
        
        async function closeTextChat() {
            isTextChatOpen = false;
            textChatPanel.classList.remove('visible');
            // Clear inactivity timer
            clearChatInactivityTimer();
            
            // Collapse the window back to original size/position
            if (window.orbAPI?.collapseFromChat) {
                try {
                    await window.orbAPI.collapseFromChat();
                } catch (e) {
                    console.warn('[TextChat] Could not collapse window:', e);
                }
            }
            
            // Reset orb to default position after collapse
            const orbContainer = document.querySelector('.orb-container');
            orbContainer.classList.remove('orb-bottom-right', 'orb-bottom-left', 'orb-top-right', 'orb-top-left');
            textChatPanel.classList.remove('pos-top-left', 'pos-top-right', 'pos-bottom-left', 'pos-bottom-right');
            currentChatAnchor = 'bottom-right'; // Reset to default
        }
        
        chatCloseBtn.addEventListener('click', closeTextChat);
        
        // Reset inactivity timer on any chat panel interaction
        chatInput.addEventListener('input', resetChatInactivityTimer);
        chatInput.addEventListener('focus', resetChatInactivityTimer);
        chatMessages.addEventListener('scroll', resetChatInactivityTimer);
        textChatPanel.addEventListener('mousemove', resetChatInactivityTimer);
        
        function addChatMessage(type, text) {
            // Remove empty state if present
            const emptyState = chatMessages.querySelector('.chat-empty');
            if (emptyState) {
                emptyState.remove();
            }
            
            const msg = document.createElement('div');
            msg.className = `chat-message ${type}`;
            msg.textContent = text;
            chatMessages.appendChild(msg);
            chatMessages.scrollTop = chatMessages.scrollHeight;
            
            chatHistory.push({ type, text, timestamp: Date.now() });
            
            // Reset inactivity timer when messages are added
            resetChatInactivityTimer();
        }
        
        async function sendChatMessage() {
            const text = chatInput.value.trim();
            if (!text) return;
            
            // Reset inactivity timer on send
            resetChatInactivityTimer();
            
            // Clear input
            chatInput.value = '';
            chatSendBtn.disabled = true;
            
            // Add user message
            addChatMessage('user', text);
            
            // Process through the same command handler as voice
            try {
                // Show processing state
                addChatMessage('system', 'Processing...');
                
                // Use the same processVoiceCommand function
                await processVoiceCommand(text);
                
                // Remove processing message
                const processingMsg = chatMessages.querySelector('.chat-message.system:last-child');
                if (processingMsg && processingMsg.textContent === 'Processing...') {
                    processingMsg.remove();
                }
                
                // Add confirmation
                addChatMessage('assistant', 'Got it. Working on that.');
                
            } catch (error) {
                console.error('[TextChat] Error:', error);
                addChatMessage('assistant', 'Sorry, something went wrong.');
            }
            
            chatSendBtn.disabled = false;
        }
        
        chatSendBtn.addEventListener('click', sendChatMessage);
        
        chatInput.addEventListener('keydown', (e) => {
            if (e.key === 'Enter' && !e.shiftKey) {
                e.preventDefault();
                sendChatMessage();
            }
            if (e.key === 'Escape') {
                closeTextChat();
            }
        });
        
        // Update UI based on state
        function updateUI(state, text = '') {
            orb.classList.remove('listening', 'processing', 'error');
            
            switch (state) {
                case 'listening':
                    orb.classList.add('listening');
                    orb.title = 'Click to stop';
                    break;
                case 'processing':
                    orb.classList.add('processing');
                    break;
                case 'error':
                    orb.classList.add('error');
                    showTranscript(text || 'Error occurred', false);
                    setTimeout(() => {
                        orb.classList.remove('error');
                        hideTranscript();
                    }, 3000);
                    break;
                default:
                    orb.title = 'Click to start listening';
            }
        }
        
        let hideTranscriptTimeout = null;
        
        function showTranscript(text, isInterim = false) {
            // Clear any pending hide
            if (hideTranscriptTimeout) {
                clearTimeout(hideTranscriptTimeout);
                hideTranscriptTimeout = null;
            }
            
            // Remove fading state if it was fading
            transcriptEl.classList.remove('fading', 'below');
            
            transcriptEl.textContent = text;
            transcriptEl.classList.toggle('interim', isInterim);
            
            // Get the orb's position within the window
            const orbContainer = document.querySelector('.orb-container');
            const orbRect = orbContainer.getBoundingClientRect();
            const windowWidth = window.innerWidth;
            const windowHeight = window.innerHeight;
            
            // Tooltip dimensions (estimate before visible)
            const tooltipMaxWidth = 320;
            const tooltipEstimatedHeight = Math.min(150, 50 + text.length * 0.5); // rough estimate
            const margin = 15;
            const gap = 15; // gap between orb and tooltip
            
            // Determine if orb is in top or bottom half of window
            const orbCenterY = orbRect.top + orbRect.height / 2;
            const orbCenterX = orbRect.left + orbRect.width / 2;
            const showBelow = orbCenterY < windowHeight / 2;
            
            // Calculate vertical position
            let top, bottom;
            if (showBelow) {
                // Position below the orb
                top = orbRect.bottom + gap;
                bottom = 'auto';
                transcriptEl.classList.add('below');
            } else {
                // Position above the orb
                top = 'auto';
                bottom = windowHeight - orbRect.top + gap;
            }
            
            // Calculate horizontal position - try to center on orb, but stay within bounds
            let left, right;
            const tooltipCenterX = orbCenterX;
            const tooltipLeft = tooltipCenterX - tooltipMaxWidth / 2;
            const tooltipRight = tooltipCenterX + tooltipMaxWidth / 2;
            
            if (tooltipLeft < margin) {
                // Would overflow left edge - align to left
                left = margin;
                right = 'auto';
            } else if (tooltipRight > windowWidth - margin) {
                // Would overflow right edge - align to right
                left = 'auto';
                right = margin;
            } else {
                // Center on orb
                left = tooltipLeft;
                right = 'auto';
            }
            
            // Apply position
            transcriptEl.style.top = typeof top === 'number' ? top + 'px' : top;
            transcriptEl.style.bottom = typeof bottom === 'number' ? bottom + 'px' : bottom;
            transcriptEl.style.left = typeof left === 'number' ? left + 'px' : left;
            transcriptEl.style.right = typeof right === 'number' ? right + 'px' : right;
            
            // Now show it
            transcriptEl.classList.add('visible');
        }
        
        function hideTranscript() {
            // Start elegant fade out
            transcriptEl.classList.add('fading');
            transcriptEl.classList.remove('visible');
            
            // Clean up classes and inline styles after animation completes
            hideTranscriptTimeout = setTimeout(() => {
                transcriptEl.classList.remove('fading', 'below', 'interim');
                // Reset inline styles so CSS defaults take over
                transcriptEl.style.top = '';
                transcriptEl.style.bottom = '';
                transcriptEl.style.left = '';
                transcriptEl.style.right = '';
            }, 800);
        }
        
        // Convert Float32Array to base64 PCM16
        function floatTo16BitPCM(float32Array) {
            const buffer = new ArrayBuffer(float32Array.length * 2);
            const view = new DataView(buffer);
            for (let i = 0; i < float32Array.length; i++) {
                const s = Math.max(-1, Math.min(1, float32Array[i]));
                view.setInt16(i * 2, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
            }
            return btoa(String.fromCharCode.apply(null, new Uint8Array(buffer)));
        }
        
        // Start listening
        async function startListening() {
            if (isListening) return;
            
            // Reset dedup state for new session
            lastProcessedTranscript = '';
            lastProcessedTime = 0;
            ttsEndTime = 0;
            // NOTE: pendingSpeakQueue removed - all TTS handled by backend speechQueue
            
            try {
                // Request mic permission
                await window.orbAPI.requestMicPermission();
                
                // Connect to realtime speech API
                const result = await window.orbAPI.connect();
                if (!result.success) {
                    throw new Error(result.error || 'Failed to connect');
                }
                isConnected = true;
                
                // Set up event listener for transcripts and audio
                removeEventListener = window.orbAPI.onEvent(async (event) => {
                    if (event.type === 'session_updated') {
                        // Session is ready
                        isSessionReady = true;
                        console.log('[Orb] Session ready');
                        // NOTE: pendingSpeak removed - all TTS handled by backend
                    } else if (event.type === 'transcript_delta') {
                        // User is actively speaking - reset speech detection
                        onSpeechDetected();
                        showTranscript(event.text, true);
                    } else if (event.type === 'function_call_transcript') {
                        // FUNCTION CALLING: AI called our function with the transcript
                        // Process through our agents and respond
                        const transcript = event.transcript;
                        const callId = event.callId;
                        const now = Date.now();
                        
                        // NOISE CHECK: Ignore background noise that triggered function call
                        if (isLikelyNoise(transcript)) {
                            console.log('[Orb] Ignoring noise in function call:', transcript);
                            // Respond with empty to acknowledge but not speak
                            try {
                                await window.orbAPI.respondToFunction(callId, "");
                            } catch (e) { /* ignore */ }
                            return;
                        }
                        
                        // COOLDOWN CHECK: Ignore function calls right after TTS
                        // This prevents the microphone from picking up echo/ambient noise
                        const timeSinceTTS = now - ttsEndTime;
                        if (timeSinceTTS < TTS_COOLDOWN_MS) {
                            console.log('[Orb] Ignoring function call during TTS cooldown:', transcript, 'time since TTS:', timeSinceTTS);
                            // Respond with empty to acknowledge but not speak
                            try {
                                await window.orbAPI.respondToFunction(callId, "");
                            } catch (e) { /* ignore */ }
                            return;
                        }
                        
                        // Track this to prevent duplicate processing via regular transcript event
                        lastFunctionCallTranscript = transcript?.toLowerCase() || '';
                        lastFunctionCallTime = now;
                        
                        // Mark that speech ended (final transcript received)
                        onSpeechEnded();
                        
                        console.log('[Orb] Function call received:', transcript, 'callId:', callId);
                        
                        // Store callId for potential multi-turn conversation (needsInput)
                        pendingFunctionCallId = callId;
                        
                        // Show the transcript
                        showTranscript(transcript, false);
                        
                        // Process through exchange-bridge (our agents)
                        try {
                            const result = await window.orbAPI.submit(transcript);
                            console.log('[Orb] Agent result:', result);
                            
                            // Check if we should suppress the response (async exchange handling)
                            // When suppressAIResponse is true, events (needs-input, completed) will handle TTS
                            if (result.suppressAIResponse && result.queued) {
                                console.log('[Orb] Async exchange path - responding with brief ack, events will speak result');
                                // Respond with brief acknowledgment to complete the function call
                                // The actual result (or needs-input prompt) will be spoken by backend
                                await window.orbAPI.respondToFunction(callId, "");
                                // Keep pendingFunctionCallId null since we already responded
                                // Backend speaks via exchange-bridge.js -> realtimeSpeech.speak()
                                pendingFunctionCallId = null;
                                return;
                            }
                            
                            // Check if agent needs input (multi-turn conversation, synchronous path)
                            if (result.needsInput) {
                                console.log('[Orb] Agent needs input (sync)');
                                // Respond with the prompt
                                const prompt = result.message || result.needsInput.prompt || "What would you like?";
                                await window.orbAPI.respondToFunction(callId, prompt);
                                pendingFunctionCallId = null;
                            } else {
                                // Get the response message
                                let responseText = result.message || "I'm not sure how to help with that.";
                                
                                // Respond to the function call so AI speaks the result
                                await window.orbAPI.respondToFunction(callId, responseText);
                                pendingFunctionCallId = null;
                            }
                        } catch (err) {
                            console.error('[Orb] Error processing function call:', err);
                            // Respond with error message
                            await window.orbAPI.respondToFunction(callId, "Sorry, something went wrong.");
                            pendingFunctionCallId = null;
                        }
                        
                        // Hide transcript after a delay
                        setTimeout(hideTranscript, 3000);
                        return; // Don't process as regular transcript
                    } else if (event.type === 'transcript') {
                        const text = event.text?.trim();
                        const now = Date.now();
                        
                        // Skip if empty or likely background noise
                        if (isLikelyNoise(text)) {
                            console.log('[Orb] Ignoring noise/filler:', text);
                            return;
                        }
                        
                        // CRITICAL: Skip if already processed via function call
                        // With function calling enabled, we receive transcripts twice:
                        // 1. Via function_call_transcript (which we handle)
                        // 2. Via regular transcript (which we must skip)
                        // Normalize by removing punctuation for comparison (e.g., "Play music." vs "play music")
                        const stripPunctuation = (s) => s.replace(/[.,!?;:'"]/g, '').trim();
                        const normalizedFuncCall = stripPunctuation(lastFunctionCallTranscript);
                        const normalizedText = stripPunctuation(text.toLowerCase());
                        if (normalizedText === normalizedFuncCall && (now - lastFunctionCallTime) < FUNCTION_CALL_DEDUP_MS) {
                            console.log('[Orb] Skipping transcript - already handled via function call:', text);
                            return;
                        }
                        
                        // Skip if within TTS cooldown (prevents feedback loop)
                        if (now - ttsEndTime < TTS_COOLDOWN_MS) {
                            console.log('[Orb] Ignoring transcript during TTS cooldown:', text);
                            return;
                        }
                        
                        // Skip if duplicate within dedup window
                        // normalizedText already defined above
                        const normalizedLast = lastProcessedTranscript.toLowerCase();
                        if (normalizedText === normalizedLast && (now - lastProcessedTime) < DEDUP_WINDOW_MS) {
                            console.log('[Orb] Ignoring duplicate transcript:', text);
                            return;
                        }
                        
                        // Update dedup tracking
                        lastProcessedTranscript = text;
                        lastProcessedTime = now;
                        
                        // Mark that speech ended (final transcript received)
                        onSpeechEnded();
                        
                        showTranscript(text, false);
                        // Submit for classification and show HUD
                        processVoiceCommand(text);
                        // Hide transcript after a delay
                        setTimeout(hideTranscript, 3000);
                    } else if (event.type === 'response_cancelled' || event.type === 'clear_audio_buffer') {
                        // FIX: When a response is cancelled OR we're about to speak something new,
                        // clear any accumulated audio chunks to prevent mixing responses
                        console.log('[Orb] Clearing audio buffer, reason:', event.type);
                        ttsAudioChunks = [];
                        
                        // Clear safety timeout
                        if (speakingTimeoutId) {
                            clearTimeout(speakingTimeoutId);
                            speakingTimeoutId = null;
                        }
                        
                        // FIX: Also reset speaking state when response is cancelled
                        // This prevents getting stuck in speaking state
                        if (isSpeaking) {
                            console.log('[Orb] Resetting speaking state after cancellation');
                            isSpeaking = false;
                            ttsEndTime = Date.now();
                            
                            // NOTE: Backend speech queue handles queuing - no frontend queue
                            
                            // If listening was stopped, now disconnect
                            // BUT if we're in a multi-turn conversation, keep listening instead
                            if (!isListening && isConnected) {
                                if (isWaitingForUserInput) {
                                    console.log('[Orb] Response cancelled but waiting for user input - restarting listening');
                                    startListening();
                                } else {
                                    console.log('[Orb] Response cancelled, now disconnecting...');
                                    finishAndDisconnect();
                                    updateUI('idle');
                                }
                            }
                        }
                    } else if (event.type === 'audio_delta') {
                        // TTS audio chunk from OpenAI
                        // FIX: Always collect audio chunks (backend may speak without frontend knowing)
                        if (event.audio) {
                            const float32 = base64ToFloat32(event.audio);
                            ttsAudioChunks.push(float32);
                        }
                    } else if (event.type === 'audio_done') {
                        // TTS audio complete, play it
                        if (ttsAudioChunks.length > 0) {
                            playTTSAudio();
                        }
                        
                        // ALWAYS update ttsEndTime when audio finishes
                        // This handles speech from backend (exchange-bridge) via realtimeSpeech.speak()
                        ttsEndTime = Date.now();
                        console.log('[Orb] TTS ended, cooldown active until:', ttsEndTime + TTS_COOLDOWN_MS);
                        
                        // Reset speaking state
                        if (isSpeaking) {
                            isSpeaking = false;
                            
                            // After TTS, start listening for user's next speech
                            // Reset hasSpokenThisSession so we wait for new speech
                            hasSpokenThisSession = false;
                            startNoSpeechTimeout();
                            
                            // NOTE: Backend speech queue handles queuing - no frontend queue
                        }
                        
                        // If listening was stopped while TTS was playing, now disconnect
                        // BUT if we're in a multi-turn conversation, keep listening instead
                        if (!isListening && isConnected) {
                            if (isWaitingForUserInput) {
                                console.log('[Orb] TTS complete but waiting for user input - restarting listening');
                                startListening();
                            } else {
                                console.log('[Orb] TTS complete, now disconnecting...');
                                finishAndDisconnect();
                                updateUI('idle');
                            }
                        }
                    } else if (event.type === 'error') {
                        updateUI('error', event.message || 'Speech error');
                        stopListening();
                    }
                });
                
                // Set up audio capture
                mediaStream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        channelCount: 1,
                        sampleRate: 24000,
                        echoCancellation: true,
                        noiseSuppression: true
                    }
                });
                
                audioContext = new AudioContext({ sampleRate: 24000 });
                const source = audioContext.createMediaStreamSource(mediaStream);
                processor = audioContext.createScriptProcessor(4096, 1, 1);
                
                processor.onaudioprocess = (e) => {
                    if (!isListening) return;
                    const inputData = e.inputBuffer.getChannelData(0);
                    const base64Audio = floatTo16BitPCM(inputData);
                    window.orbAPI.sendAudio(base64Audio);
                };
                
                source.connect(processor);
                processor.connect(audioContext.destination);
                
                isListening = true;
                updateUI('listening');
                console.log('[Orb] Started listening');
                
                // Reset speech tracking for this session
                lastSpeechTime = 0;
                hasSpokenThisSession = false;
                
                // Start no-speech timeout (in case user never speaks)
                startNoSpeechTimeout();
                
                // Play ready chime - a short tone that won't trigger function calling
                // (unlike spoken "Ready" which caused feedback loops)
                playReadyChime();
                
            } catch (error) {
                console.error('[Orb] Start error:', error);
                updateUI('error', error.message);
                stopListening();
            }
        }
        
        // Helper to finish cleanup and disconnect
        async function finishAndDisconnect() {
            // Clean up event listener
            if (removeEventListener) {
                removeEventListener();
                removeEventListener = null;
            }
            
            // Disconnect from API
            if (isConnected) {
                try {
                    await window.orbAPI.disconnect();
                } catch (e) {
                    console.error('[Orb] Disconnect error:', e);
                }
                isConnected = false;
            }
            
            isSessionReady = false;
        }
        
        // Stop listening
        async function stopListening() {
            if (!isListening && !isConnected) return;
            
            isListening = false;
            // NOTE: pendingSpeak and pendingSpeakQueue removed - all TTS handled by backend
            
            // Clear all timers
            clearAllSpeechTimers();
            
            // Clean up audio input (microphone)
            if (processor) {
                processor.disconnect();
                processor = null;
            }
            if (audioContext) {
                audioContext.close();
                audioContext = null;
            }
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
                mediaStream = null;
            }
            
            // If TTS is in progress, delay disconnect until it's done
            // But still update UI immediately to show we're not listening anymore
            if (isSpeaking) {
                console.log('[Orb] TTS in progress, delaying disconnect...');
                // Event listener stays active to receive audio_done
                // Disconnect will happen via finishAndDisconnect()
                updateUI('processing'); // Show processing state while TTS finishes
                return;
            }
            
            // Clean up event listener and disconnect
            finishAndDisconnect();
            
            updateUI('idle');
            console.log('[Orb] Stopped listening');
        }
        
        // ==========================================================================
        // DRAG SUPPORT
        // ==========================================================================
        
        let isDragging = false;
        let dragStartX = 0;
        let dragStartY = 0;
        let windowStartX = 0;
        let windowStartY = 0;
        let hasMoved = false;
        
        orb.addEventListener('mousedown', (e) => {
            // Only drag on left-click (button 0), not right-click (button 2)
            if (e.button !== 0) return;
            
            isDragging = true;
            hasMoved = false;
            dragStartX = e.screenX;
            dragStartY = e.screenY;
            // Capture window position at start of drag
            windowStartX = window.screenX;
            windowStartY = window.screenY;
            orb.style.cursor = 'grabbing';
            e.preventDefault();
        });
        
        document.addEventListener('mousemove', (e) => {
            if (!isDragging) return;
            
            // Calculate total delta from drag start
            const deltaX = e.screenX - dragStartX;
            const deltaY = e.screenY - dragStartY;
            
            // Only start moving if we've moved more than 5px (to distinguish from clicks)
            if (Math.abs(deltaX) > 5 || Math.abs(deltaY) > 5) {
                hasMoved = true;
                
                // Calculate new position based on initial window position + total delta
                const newX = windowStartX + deltaX;
                const newY = windowStartY + deltaY;
                
                window.orbAPI.setPosition(newX, newY);
            }
        });
        
        document.addEventListener('mouseup', () => {
            if (isDragging) {
                isDragging = false;
                orb.style.cursor = 'pointer';
            }
        });
        
        // Toggle listening on click (only if we didn't drag)
        orb.addEventListener('click', () => {
            if (hasMoved) {
                hasMoved = false;
                return;
            }
            
            // If text chat is open, close it and start voice
            if (isTextChatOpen) {
                closeTextChat();
            }
            
            if (isListening) {
                stopListening();
            } else {
                startListening();
            }
        });
        
        // Right-click to show context menu (only on the orb or its container)
        const orbContainer = document.querySelector('.orb-container');
        orbContainer.addEventListener('contextmenu', (e) => {
            console.log('[Orb] Right-click on orb container at', e.clientX, e.clientY);
            // #region agent log
            fetch('http://127.0.0.1:7242/ingest/54746cc5-c924-4bb5-9e76-3f6b729e6870',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'orb.html:contextmenu',message:'Right-click event on orb',data:{clientX:e.clientX,clientY:e.clientY,screenX:e.screenX,screenY:e.screenY,target:e.target?.id||e.target?.className},timestamp:Date.now(),sessionId:'debug-session',hypothesisId:'E'})}).catch(()=>{});
            // #endregion
            e.preventDefault();
            e.stopPropagation();
            showContextMenu(e.clientX, e.clientY);
        });
        
        // Prevent default context menu on the rest of the document (but don't show our menu)
        document.addEventListener('contextmenu', (e) => {
            e.preventDefault();
        });
        
        // Double-click as alternative to open text chat directly
        orb.addEventListener('dblclick', (e) => {
            console.log('[Orb] Double-click detected');
            e.preventDefault();
            openTextChat();
        });
        
        // Handle window close
        window.addEventListener('beforeunload', () => {
            stopListening();
        });
        
        // Listen for plan summary from Agent Composer
        if (window.orbAPI?.onPlanSummary) {
            window.orbAPI.onPlanSummary(async (data) => {
                console.log('[Orb] Received plan summary:', data.type);
                
                if (data.type === 'plan-ready' && data.summary) {
                    // Speak the plan summary via TTS (using backend)
                    try {
                        await window.orbAPI.speak(data.summary);
                    } catch (e) {
                        console.warn('[Orb] Could not speak plan summary:', e);
                    }
                } else if (data.type === 'creation-complete' && data.agentName) {
                    // Announce agent creation complete - conversational
                    try {
                        await window.orbAPI.speak(`All set! ${data.agentName} is ready to use.`);
                    } catch (e) {
                        console.warn('[Orb] Could not speak completion:', e);
                    }
                }
            });
            console.log('[Orb] Agent Composer plan listener registered');
        }
        
        // Log ready state
        console.log('[Orb] Voice Orb initialized with drag support and Agent Composer integration');
    </script>
</body>
</html>
