<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Orb</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        html, body {
            width: 100%;
            height: 100%;
            background: transparent;
            overflow: hidden;
            /* Make transparent areas click-through to apps behind */
            pointer-events: none;
            /* Removed -webkit-app-region: drag - we handle dragging manually */
        }
        
        .orb-container {
            position: absolute;
            bottom: 20px;
            right: 20px;
            width: 80px;
            height: 80px;
            transition: top 0.25s ease, bottom 0.25s ease, left 0.25s ease, right 0.25s ease;
            /* Re-enable mouse events for the orb */
            pointer-events: auto;
        }
        
        /* Orb position variants for chat panel layout */
        .orb-container.orb-bottom-right {
            bottom: 20px;
            right: 20px;
            top: auto;
            left: auto;
        }
        
        .orb-container.orb-bottom-left {
            bottom: 20px;
            left: 20px;
            top: auto;
            right: auto;
        }
        
        .orb-container.orb-top-right {
            top: 20px;
            right: 20px;
            bottom: auto;
            left: auto;
        }
        
        .orb-container.orb-top-left {
            top: 20px;
            left: 20px;
            bottom: auto;
            right: auto;
        }
        
        .orb {
            width: 80px;
            height: 80px;
            border-radius: 50%;
            /* Use OneReach orb graphic (with gradient fallback if asset missing) */
            background: radial-gradient(circle at 35% 35%, #5b8def, #2a5cba 50%, #1a3a7a);
            background-image: url('assets/banner.png');
            background-size: cover;
            background-position: center;
            cursor: grab;
            display: flex;
            align-items: center;
            justify-content: center;
            -webkit-app-region: no-drag;
            position: relative;
            /* No glow by default - clean look */
            box-shadow: none;
            transition: transform 0.3s ease, filter 0.3s ease;
            animation: orb-float 6s ease-in-out infinite;
        }
        
        .orb:hover {
            transform: scale(1.1);
            filter: brightness(1.1);
        }
        
        .orb.listening {
            box-shadow: 0 0 8px rgba(234, 179, 8, 0.5);
            animation: orb-float 4s ease-in-out infinite, orb-breathe 1.5s ease-in-out infinite;
        }
        
        .orb.processing {
            box-shadow: 0 0 8px rgba(249, 115, 22, 0.5);
            animation: orb-float 3s ease-in-out infinite, orb-pulse 0.8s ease-in-out infinite;
        }
        
        .orb.error {
            box-shadow: 0 0 8px rgba(239, 68, 68, 0.5);
            animation: orb-shake 0.5s ease-in-out;
        }
        
        /* Gentle floating motion */
        @keyframes orb-float {
            0%, 100% { 
                transform: translate(0, 0); 
            }
            25% { 
                transform: translate(2px, -3px); 
            }
            50% { 
                transform: translate(-1px, -1px); 
            }
            75% { 
                transform: translate(-2px, -2px); 
            }
        }
        
        /* Breathing glow when listening */
        @keyframes orb-breathe {
            0%, 100% { 
                box-shadow: 0 0 6px rgba(234, 179, 8, 0.4);
                filter: brightness(1);
            }
            50% { 
                box-shadow: 0 0 12px rgba(234, 179, 8, 0.6);
                filter: brightness(1.1);
            }
        }
        
        /* Pulse when processing */
        @keyframes orb-pulse {
            0%, 100% { 
                transform: scale(1);
            }
            50% { 
                transform: scale(1.05);
            }
        }
        
        /* Shake on error */
        @keyframes orb-shake {
            0%, 100% { transform: translateX(0); }
            20% { transform: translateX(-4px); }
            40% { transform: translateX(4px); }
            60% { transform: translateX(-4px); }
            80% { transform: translateX(4px); }
        }
        
        /* Ring effects - hidden by default, only show when listening */
        .ring, .ring-secondary {
            display: none;
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            width: 80px;
            height: 80px;
            border-radius: 50%;
            pointer-events: none;
        }
        
        .ring {
            border: 1px solid rgba(234, 179, 8, 0.4);
        }
        
        .orb.listening + .ring {
            display: block;
            animation: ring-expand 2s ease-out infinite;
        }
        
        @keyframes ring-expand {
            0% { 
                transform: translate(-50%, -50%) scale(1); 
                opacity: 0.5;
            }
            100% { 
                transform: translate(-50%, -50%) scale(1.6); 
                opacity: 0;
            }
        }
        
        /* Mic overlay icon (subtle, appears on hover or when idle) */
        .mic-overlay {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            width: 24px;
            height: 24px;
            opacity: 0;
            transition: opacity 0.3s ease;
            pointer-events: none;
        }
        
        .mic-overlay svg {
            width: 100%;
            height: 100%;
            fill: rgba(255, 255, 255, 0.9);
            filter: drop-shadow(0 2px 4px rgba(0, 0, 0, 0.3));
        }
        
        .orb:hover .mic-overlay {
            opacity: 0.8;
        }
        
        .orb.listening .mic-overlay {
            opacity: 0;
        }
        
        /* Transcript tooltip - elegant floating text */
        /* Uses fixed positioning for reliable placement regardless of orb position */
        .transcript-tooltip {
            position: fixed;
            background: linear-gradient(135deg, rgba(15, 15, 20, 0.95) 0%, rgba(25, 25, 35, 0.9) 100%);
            color: rgba(255, 255, 255, 0.95);
            padding: 16px 22px;
            border-radius: 16px;
            font-size: 15px;
            font-weight: 400;
            letter-spacing: 0.2px;
            line-height: 1.6;
            font-family: -apple-system, BlinkMacSystemFont, 'SF Pro Text', 'Segoe UI', Roboto, sans-serif;
            max-width: 320px;
            max-height: 200px;
            overflow-y: auto;
            text-align: left;
            white-space: normal;
            /* Hide scrollbar but keep scroll functionality */
            scrollbar-width: none; /* Firefox */
            -ms-overflow-style: none; /* IE/Edge */
            word-wrap: break-word;
            opacity: 0;
            /* Allow clicks through when hidden, enable when visible */
            pointer-events: none;
            -webkit-app-region: no-drag;
            /* Frosted glass effect */
            backdrop-filter: blur(20px);
            -webkit-backdrop-filter: blur(20px);
            /* Subtle border glow */
            border: 1px solid rgba(255, 255, 255, 0.1);
            box-shadow: 
                0 8px 32px rgba(0, 0, 0, 0.4),
                0 0 0 1px rgba(255, 255, 255, 0.05),
                inset 0 1px 0 rgba(255, 255, 255, 0.1);
            /* Default position - will be overridden by JS */
            bottom: 120px;
            right: 20px;
            left: auto;
            top: auto;
            /* Animation properties */
            transform: translateY(10px);
            transition: opacity 0.4s ease, transform 0.4s cubic-bezier(0.4, 0, 0.2, 1);
            z-index: 100;
        }
        
        /* Hide scrollbar for WebKit browsers (Chrome, Safari) */
        .transcript-tooltip::-webkit-scrollbar {
            display: none;
        }
        
        .transcript-tooltip.visible {
            opacity: 1;
            transform: translateY(0);
        }
        
        /* Fading out state */
        .transcript-tooltip.fading {
            opacity: 0;
            transform: translateY(-10px);
            transition: opacity 0.8s ease-out, transform 0.8s ease-out;
        }
        
        .transcript-tooltip.interim {
            color: rgba(180, 180, 200, 0.9);
            font-style: italic;
            font-weight: 300;
        }
        
        /* Agent speaking - distinct style */
        .transcript-tooltip.agent-speaking {
            background: linear-gradient(135deg, rgba(20, 40, 60, 0.95) 0%, rgba(30, 50, 70, 0.9) 100%);
            border-left: 3px solid rgba(100, 180, 255, 0.7);
            color: rgba(200, 230, 255, 0.95);
        }
        
        /* Position below orb when near top of window */
        .transcript-tooltip.below {
            transform: translateY(-10px);
        }
        
        .transcript-tooltip.below.visible {
            transform: translateY(0);
        }
        
        .transcript-tooltip.below.fading {
            transform: translateY(10px);
        }
        
        /* Position to the left of orb (slides in from the right toward orb) */
        .transcript-tooltip.left-of-orb {
            transform: translateX(10px);
        }
        
        .transcript-tooltip.left-of-orb.visible {
            transform: translateX(0);
        }
        
        .transcript-tooltip.left-of-orb.fading {
            transform: translateX(10px);
            transition: opacity 0.8s ease-out, transform 0.8s ease-out;
        }
        
        /* ==================== CONTEXT MENU ==================== */
        .orb-context-menu {
            position: fixed;
            top: 50px;
            left: 50px;
            background: linear-gradient(135deg, rgba(20, 20, 30, 0.98) 0%, rgba(30, 30, 45, 0.95) 100%);
            border: 1px solid rgba(255, 255, 255, 0.1);
            border-radius: 12px;
            padding: 6px;
            min-width: 160px;
            box-shadow: 
                0 12px 40px rgba(0, 0, 0, 0.5),
                0 0 0 1px rgba(255, 255, 255, 0.05),
                inset 0 1px 0 rgba(255, 255, 255, 0.1);
            backdrop-filter: blur(20px);
            -webkit-backdrop-filter: blur(20px);
            opacity: 0;
            transform: scale(0.95) translateY(-5px);
            transform-origin: top left;
            transition: opacity 0.15s ease, transform 0.15s ease;
            /* Click-through when hidden, enabled when visible */
            pointer-events: none;
            z-index: 1000;
            -webkit-app-region: no-drag;
        }
        
        .orb-context-menu.visible {
            opacity: 1;
            transform: scale(1) translateY(0);
            pointer-events: auto;
        }
        
        .context-menu-item {
            display: flex;
            align-items: center;
            gap: 10px;
            padding: 10px 14px;
            color: rgba(255, 255, 255, 0.9);
            font-size: 13px;
            font-family: -apple-system, BlinkMacSystemFont, 'SF Pro Text', sans-serif;
            border-radius: 8px;
            cursor: pointer;
            transition: background 0.15s ease;
            -webkit-app-region: no-drag;
        }
        
        .context-menu-item:hover {
            background: rgba(255, 255, 255, 0.1);
        }
        
        .context-menu-item svg {
            width: 16px;
            height: 16px;
            fill: currentColor;
            opacity: 0.8;
        }
        
        .context-menu-divider {
            height: 1px;
            background: rgba(255, 255, 255, 0.1);
            margin: 6px 8px;
        }
        
        /* ==================== TEXT CHAT PANEL ==================== */
        .text-chat-panel {
            position: fixed;
            top: 15px;
            left: 15px;
            right: 15px;
            bottom: 120px;
            background: linear-gradient(135deg, rgba(15, 15, 25, 0.98) 0%, rgba(25, 25, 40, 0.95) 100%);
            border: 1px solid rgba(255, 255, 255, 0.1);
            border-radius: 16px;
            box-shadow: 
                0 20px 60px rgba(0, 0, 0, 0.5),
                0 0 0 1px rgba(255, 255, 255, 0.05),
                inset 0 1px 0 rgba(255, 255, 255, 0.1);
            backdrop-filter: blur(30px);
            -webkit-backdrop-filter: blur(30px);
            display: flex;
            flex-direction: column;
            opacity: 0;
            transform: scale(0.95) translateY(10px);
            transform-origin: bottom center;
            transition: opacity 0.25s ease, transform 0.25s cubic-bezier(0.4, 0, 0.2, 1);
            pointer-events: none;
            z-index: 500;
            -webkit-app-region: no-drag;
            overflow: hidden;
        }
        
        .text-chat-panel.visible {
            opacity: 1;
            transform: scale(1) translateY(0);
            pointer-events: auto;
        }
        
        .chat-header {
            display: flex;
            align-items: center;
            justify-content: space-between;
            padding: 16px 20px;
            border-bottom: 1px solid rgba(255, 255, 255, 0.08);
        }
        
        .chat-header-title {
            display: flex;
            align-items: center;
            gap: 10px;
            color: rgba(255, 255, 255, 0.95);
            font-size: 14px;
            font-weight: 500;
            font-family: -apple-system, BlinkMacSystemFont, 'SF Pro Text', sans-serif;
        }
        
        .chat-header-title svg {
            width: 18px;
            height: 18px;
            fill: currentColor;
            opacity: 0.8;
        }
        
        .chat-close-btn {
            width: 28px;
            height: 28px;
            border-radius: 8px;
            border: none;
            background: rgba(255, 255, 255, 0.05);
            color: rgba(255, 255, 255, 0.6);
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: all 0.15s ease;
        }
        
        .chat-close-btn:hover {
            background: rgba(255, 255, 255, 0.1);
            color: rgba(255, 255, 255, 0.9);
        }
        
        .chat-close-btn svg {
            width: 14px;
            height: 14px;
            fill: currentColor;
        }
        
        .chat-messages {
            flex: 1;
            overflow-y: auto;
            padding: 16px;
            display: flex;
            flex-direction: column;
            gap: 12px;
            min-height: 100px;
        }
        
        .chat-messages::-webkit-scrollbar {
            width: 6px;
        }
        
        .chat-messages::-webkit-scrollbar-track {
            background: transparent;
        }
        
        .chat-messages::-webkit-scrollbar-thumb {
            background: rgba(255, 255, 255, 0.15);
            border-radius: 3px;
        }
        
        .chat-message {
            padding: 12px 16px;
            border-radius: 16px;
            font-size: 13px;
            line-height: 1.5;
            font-family: -apple-system, BlinkMacSystemFont, 'SF Pro Text', sans-serif;
            max-width: 85%;
            animation: message-appear 0.2s ease;
        }
        
        @keyframes message-appear {
            from {
                opacity: 0;
                transform: translateY(8px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }
        
        .chat-message.user {
            align-self: flex-end;
            background: linear-gradient(135deg, #6366f1 0%, #8b5cf6 100%);
            color: white;
            border-bottom-right-radius: 6px;
        }
        
        .chat-message.assistant {
            align-self: flex-start;
            background: rgba(255, 255, 255, 0.08);
            color: rgba(255, 255, 255, 0.9);
            border-bottom-left-radius: 6px;
        }
        
        .chat-message.system {
            align-self: center;
            background: transparent;
            color: rgba(255, 255, 255, 0.5);
            font-size: 12px;
            padding: 8px;
        }
        
        .chat-input-container {
            padding: 16px;
            border-top: 1px solid rgba(255, 255, 255, 0.08);
        }
        
        .chat-input-wrapper {
            display: flex;
            align-items: center;
            gap: 10px;
            background: rgba(255, 255, 255, 0.06);
            border: 1px solid rgba(255, 255, 255, 0.1);
            border-radius: 14px;
            padding: 4px 4px 4px 16px;
            transition: all 0.15s ease;
        }
        
        .chat-input-wrapper:focus-within {
            border-color: rgba(139, 92, 246, 0.5);
            box-shadow: 0 0 0 3px rgba(139, 92, 246, 0.15);
        }
        
        .chat-input {
            flex: 1;
            background: transparent;
            border: none;
            outline: none;
            color: rgba(255, 255, 255, 0.95);
            font-size: 14px;
            font-family: -apple-system, BlinkMacSystemFont, 'SF Pro Text', sans-serif;
            padding: 10px 0;
        }
        
        .chat-input::placeholder {
            color: rgba(255, 255, 255, 0.35);
        }
        
        .chat-send-btn {
            width: 36px;
            height: 36px;
            border-radius: 10px;
            border: none;
            background: linear-gradient(135deg, #6366f1 0%, #8b5cf6 100%);
            color: white;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: all 0.15s ease;
            flex-shrink: 0;
        }
        
        .chat-send-btn:hover {
            transform: scale(1.05);
            box-shadow: 0 4px 12px rgba(139, 92, 246, 0.4);
        }
        
        .chat-send-btn:active {
            transform: scale(0.95);
        }
        
        .chat-send-btn:disabled {
            opacity: 0.5;
            cursor: not-allowed;
            transform: none;
        }
        
        .chat-send-btn svg {
            width: 16px;
            height: 16px;
            fill: currentColor;
        }
        
        /* Position variants based on orb location on screen */
        /* Default: orb at bottom-right, chat appears above/left */
        .text-chat-panel.pos-top-left {
            /* Orb at bottom-right: chat above and to the left */
            top: 15px;
            left: 15px;
            right: 15px;
            bottom: 120px;
        }
        
        .text-chat-panel.pos-top-right {
            /* Orb at bottom-left: chat above and to the right */
            top: 15px;
            left: 15px;
            right: 15px;
            bottom: 120px;
        }
        
        .text-chat-panel.pos-bottom-left {
            /* Orb at top-right: chat below and to the left */
            top: 120px;
            left: 15px;
            right: 15px;
            bottom: 15px;
            transform-origin: top center;
        }
        
        .text-chat-panel.pos-bottom-left.visible {
            transform: scale(1) translateY(0);
        }
        
        .text-chat-panel.pos-bottom-right {
            /* Orb at top-left: chat below and to the right */
            top: 120px;
            left: 15px;
            right: 15px;
            bottom: 15px;
            transform-origin: top center;
        }
        
        .text-chat-panel.pos-bottom-right.visible {
            transform: scale(1) translateY(0);
        }
        
        /* Empty state */
        .chat-empty {
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100%;
            padding: 40px 20px;
            color: rgba(255, 255, 255, 0.4);
            text-align: center;
        }
        
        .chat-empty svg {
            width: 48px;
            height: 48px;
            fill: currentColor;
            margin-bottom: 16px;
            opacity: 0.5;
        }
        
        .chat-empty-text {
            font-size: 13px;
            line-height: 1.6;
        }
    </style>
</head>
<body>
    <!-- Context Menu - outside container for proper positioning -->
    <div class="orb-context-menu" id="contextMenu">
        <div class="context-menu-item" id="menuTextChat">
            <svg viewBox="0 0 24 24"><path d="M20 2H4c-1.1 0-2 .9-2 2v18l4-4h14c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm0 14H6l-2 2V4h16v12z"/></svg>
            <span>Text Chat</span>
        </div>
        <div class="context-menu-item" id="menuVoice">
            <svg viewBox="0 0 24 24"><path d="M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3zm-1-9c0-.55.45-1 1-1s1 .45 1 1v6c0 .55-.45 1-1 1s-1-.45-1-1V5zm6 6c0 2.76-2.24 5-5 5s-5-2.24-5-5H5c0 3.53 2.61 6.43 6 6.92V21h2v-3.08c3.39-.49 6-3.39 6-6.92h-2z"/></svg>
            <span>Voice Mode</span>
        </div>
        <div class="context-menu-divider"></div>
        <div class="context-menu-item" id="menuSettings">
            <svg viewBox="0 0 24 24"><path d="M19.14 12.94c.04-.31.06-.63.06-.94 0-.31-.02-.63-.06-.94l2.03-1.58c.18-.14.23-.41.12-.61l-1.92-3.32c-.12-.22-.37-.29-.59-.22l-2.39.96c-.5-.38-1.03-.7-1.62-.94l-.36-2.54c-.04-.24-.24-.41-.48-.41h-3.84c-.24 0-.43.17-.47.41l-.36 2.54c-.59.24-1.13.57-1.62.94l-2.39-.96c-.22-.08-.47 0-.59.22L2.74 8.87c-.12.21-.08.47.12.61l2.03 1.58c-.04.31-.06.63-.06.94s.02.63.06.94l-2.03 1.58c-.18.14-.23.41-.12.61l1.92 3.32c.12.22.37.29.59.22l2.39-.96c.5.38 1.03.7 1.62.94l.36 2.54c.05.24.24.41.48.41h3.84c.24 0 .44-.17.47-.41l.36-2.54c.59-.24 1.13-.56 1.62-.94l2.39.96c.22.08.47 0 .59-.22l1.92-3.32c.12-.22.07-.47-.12-.61l-2.01-1.58zM12 15.6c-1.98 0-3.6-1.62-3.6-3.6s1.62-3.6 3.6-3.6 3.6 1.62 3.6 3.6-1.62 3.6-3.6 3.6z"/></svg>
            <span>Settings</span>
        </div>
    </div>
    
    <!-- Text Chat Panel - outside container for proper positioning -->

    <div class="orb-container">
        <div class="orb" id="orb" title="Click to start listening">
            <div class="mic-overlay">
                <svg viewBox="0 0 24 24">
                    <path d="M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3zm-1-9c0-.55.45-1 1-1s1 .45 1 1v6c0 .55-.45 1-1 1s-1-.45-1-1V5zm6 6c0 2.76-2.24 5-5 5s-5-2.24-5-5H5c0 3.53 2.61 6.43 6 6.92V21h2v-3.08c3.39-.49 6-3.39 6-6.92h-2z"/>
                </svg>
            </div>
        </div>
        <div class="ring"></div>
        <div class="ring-secondary"></div>
        <div class="transcript-tooltip" id="transcript"></div>
    </div>
        
    <!-- Text Chat Panel -->
        <div class="text-chat-panel" id="textChatPanel">
            <div class="chat-header">
                <div class="chat-header-title">
                    <svg viewBox="0 0 24 24"><path d="M20 2H4c-1.1 0-2 .9-2 2v18l4-4h14c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm0 14H6l-2 2V4h16v12z"/></svg>
                    <span>Chat</span>
                </div>
                <button class="chat-close-btn" id="chatCloseBtn">
                    <svg viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
                </button>
            </div>
            <div class="chat-messages" id="chatMessages">
                <div class="chat-empty">
                    <svg viewBox="0 0 24 24"><path d="M20 2H4c-1.1 0-2 .9-2 2v18l4-4h14c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm0 14H6l-2 2V4h16v12z"/></svg>
                    <div class="chat-empty-text">
                        Type a command or ask a question.<br/>
                        Same as voice, just quieter.
                    </div>
                </div>
            </div>
            <div class="chat-input-container">
                <div class="chat-input-wrapper">
                    <input type="text" class="chat-input" id="chatInput" placeholder="Type a message..." autocomplete="off" />
                    <button class="chat-send-btn" id="chatSendBtn">
                        <svg viewBox="0 0 24 24"><path d="M2.01 21L23 12 2.01 3 2 10l15 2-15 2z"/></svg>
                    </button>
                </div>
            </div>
        </div>
    
    <script>
        // State
        let isListening = false;
        let isConnected = false;
        let isSessionReady = false;
        // NOTE: pendingSpeak removed - all TTS handled by backend
        let removeEventListener = null;
        let audioContext = null;
        let mediaStream = null;
        let processor = null;
        let ttsAudio = null;
        
        // Audio playback for OpenAI Realtime TTS
        let ttsAudioContext = null;
        let ttsAudioChunks = [];
        let isSpeaking = false;
        let speakingTimeoutId = null;
        const SPEAKING_SAFETY_TIMEOUT_MS = 30000; // Force-reset isSpeaking after 30s
        
        // Deduplication and feedback loop prevention
        let lastProcessedTranscript = '';
        let lastProcessedTime = 0;
        const DEDUP_WINDOW_MS = 2000; // Ignore same transcript within 2 seconds
        let ttsEndTime = 0;
        
        // ==================== STATE RECOVERY WATCHDOG ====================
        // Periodically checks for stuck states and auto-recovers
        let watchdogIntervalId = null;
        const WATCHDOG_INTERVAL_MS = 10000; // Check every 10 seconds
        
        function startWatchdog() {
            if (watchdogIntervalId) return;
            watchdogIntervalId = setInterval(() => {
                const now = Date.now();
                
                // CHECK 1: Speaking too long without audio_done
                if (isSpeaking && speakingTimeoutId === null) {
                    console.warn('[Orb] Watchdog: isSpeaking=true but no safety timeout - force-resetting');
                    forceResetSpeaking();
                }
                
                // CHECK 2: Listening but no timers running (stuck state)
                if (isListening && !isSpeaking && pendingSubmitCount === 0
                    && !silenceTimeoutId && !noSpeechTimeoutId && !isWaitingForUserInput) {
                    console.warn('[Orb] Watchdog: Listening with no active timers - restarting no-speech timeout');
                    hasSpokenThisSession = false;
                    startNoSpeechTimeout();
                }
                
                // CHECK 3: Connected but not listening and not speaking (orphaned connection)
                if (isConnected && !isListening && !isSpeaking && pendingSubmitCount === 0 && !isWaitingForUserInput) {
                    console.warn('[Orb] Watchdog: Orphaned connection detected - disconnecting');
                    finishAndDisconnect();
                    updateUI('idle');
                }
            }, WATCHDOG_INTERVAL_MS);
        }
        
        function stopWatchdog() {
            if (watchdogIntervalId) {
                clearInterval(watchdogIntervalId);
                watchdogIntervalId = null;
            }
        }
        
        // Force-reset speaking state (used by safety timeout and watchdog)
        function forceResetSpeaking() {
            console.warn('[Orb] Force-resetting speaking state');
            isSpeaking = false;
            ttsEndTime = Date.now();
            ttsAudioChunks = [];
            if (speakingTimeoutId) {
                clearTimeout(speakingTimeoutId);
                speakingTimeoutId = null;
            }
            
            // Restart listening timers
            hasSpokenThisSession = false;
            startNoSpeechTimeout();
            
            // If we should have disconnected, do it now
            if (!isListening && isConnected && pendingSubmitCount === 0 && !isWaitingForUserInput) {
                finishAndDisconnect();
                updateUI('idle');
            }
        }
        
        // Start speaking safety timeout (called whenever isSpeaking becomes true)
        function startSpeakingSafetyTimeout() {
            if (speakingTimeoutId) {
                clearTimeout(speakingTimeoutId);
            }
            speakingTimeoutId = setTimeout(() => {
                if (isSpeaking) {
                    console.warn('[Orb] Speaking safety timeout fired - audio_done never received');
                    forceResetSpeaking();
                }
                speakingTimeoutId = null;
            }, SPEAKING_SAFETY_TIMEOUT_MS);
        }
        
        // Start watchdog immediately
        startWatchdog();
        
        // Common noise/filler words to ignore (background noise often produces these)
        const NOISE_WORDS = new Set([
            'hmm', 'hm', 'um', 'uh', 'ah', 'oh', 'eh', 'er', 'mm',
            'mhm', 'uh-huh',
            'the', 'a', 'i', 'it', 'is', 'and', 'but', 'so', 'like',
            'right', 'alright', 'well', 'now', 'just',
            // Common misheard noise
            'you', 'me', 'we', 'he', 'she', 'they', 'that', 'this',
            'what', 'huh', 'wow', 'ooh', 'aah', 'ugh'
        ]);
        
        // Valid short commands that should NOT be filtered (agents can handle these)
        const VALID_SHORT_COMMANDS = new Set([
            // Greetings (smalltalk agent)
            'hi', 'hey', 'hello', 'yo', 'sup', 'bye', 'goodbye',
            // Confirmations (multi-turn conversations)
            'yes', 'yeah', 'yep', 'no', 'nope', 'ok', 'okay', 'sure',
            // Gratitude
            'thanks', 'thank you',
            // Playback controls (DJ agent)
            'stop', 'play', 'pause', 'skip', 'next', 'back', 'mute',
            // Actions
            'help', 'cancel', 'undo', 'done', 'repeat'
        ]);
        
        // Check if transcript is likely just noise
        // NOTE: We now let more through - smalltalk agent handles unclear inputs naturally
        function isLikelyNoise(text) {
            if (!text) return true;
            
            const normalized = text.toLowerCase().trim();
            const stripped = normalized.replace(/[.,!?;:'"]/g, '').trim();
            
            // Allow valid short commands - these are intentional user input
            if (VALID_SHORT_COMMANDS.has(stripped)) return false;
            
            // Allow anything 4+ characters - let agents decide what to do with it
            // (smalltalk agent will handle gibberish with "Hmm?" type responses)
            if (stripped.length >= 4) return false;
            
            // Only filter truly empty or single-char noise
            if (stripped.length <= 1) return true;
            
            // Filter common 2-3 char noise that's definitely not intentional
            const definiteNoise = new Set(['um', 'uh', 'ah', 'eh', 'er', 'mm', 'hm']);
            if (definiteNoise.has(stripped)) return true;
            
            // Let everything else through to agents
            return false;
        }
        
        // Track function call transcripts to avoid double-processing
        // When using function calling, we get the transcript via function_call_transcript
        // AND via the regular transcript event - we must skip the duplicate
        let lastFunctionCallTranscript = '';
        let lastFunctionCallTime = 0;
        const FUNCTION_CALL_DEDUP_MS = 5000; // Skip regular transcript if function call handled it within 5s
        const TTS_COOLDOWN_MS = 2500; // Ignore transcripts for 2.5s after TTS ends (prevent echo/ambient pickup)
        // NOTE: pendingSpeakQueue removed - all TTS handled by backend speechQueue
        
        // Store pending function call ID for multi-turn conversations
        // When agent returns needsInput, we use this to respond via the proper TTS channel
        let pendingFunctionCallId = null;
        
        // Multi-turn conversation state - when true, keep mic active after TTS
        // Set when agent returns needsInput, cleared when follow-up is processed
        let isWaitingForUserInput = false;
        
        // Track pending submit count - prevents disconnect while commands are being processed
        let pendingSubmitCount = 0;
        
        // Speech detection and silence timeout
        // The idea: detect when user STARTS speaking, then wait for silence AFTER speech
        let lastSpeechTime = 0;           // When we last detected user speaking (interim transcript)
        let hasSpokenThisSession = false; // Has the user said anything meaningful?
        let silenceTimeoutId = null;
        let noSpeechTimeoutId = null;
        
        const SILENCE_AFTER_SPEECH_MS = 5000;  // 5 seconds of silence after speech = done
        const NO_SPEECH_TIMEOUT_MS = 60000;    // 60 seconds with no speech at all = auto-stop
        
        // Called when we detect the user is actively speaking (interim transcripts)
        function onSpeechDetected() {
            lastSpeechTime = Date.now();
            hasSpokenThisSession = true;
            
            // Clear the silence timeout while they're speaking
            clearSilenceTimeout();
            
            // Clear the no-speech timeout since they are speaking
            clearNoSpeechTimeout();
        }
        
        // Called when speech appears to have ended (final transcript received or silence)
        function onSpeechEnded() {
            // Start silence timeout - if no more speech, consider them done
            clearSilenceTimeout();
            silenceTimeoutId = setTimeout(() => {
                // CRITICAL: Don't auto-stop if we're waiting for user input (multi-turn conversation)
                if (isWaitingForUserInput) {
                    console.log('[Orb] Silence timeout but waiting for user input - keeping mic active');
                    return;
                }
                if (isListening && !isSpeaking && hasSpokenThisSession) {
                    const timeSinceSpeech = Date.now() - lastSpeechTime;
                    if (timeSinceSpeech >= SILENCE_AFTER_SPEECH_MS) {
                        console.log('[Orb] Silence after speech - auto-stopping');
                        stopListening();
                    }
                }
            }, SILENCE_AFTER_SPEECH_MS);
        }
        
        function clearSilenceTimeout() {
            if (silenceTimeoutId) {
                clearTimeout(silenceTimeoutId);
                silenceTimeoutId = null;
            }
        }
        
        // No speech at all timeout - in case user activates but never speaks
        function startNoSpeechTimeout() {
            clearNoSpeechTimeout();
            noSpeechTimeoutId = setTimeout(() => {
                // CRITICAL: Don't auto-stop if we're waiting for user input (multi-turn conversation)
                if (isWaitingForUserInput) {
                    console.log('[Orb] No-speech timeout but waiting for user input - keeping mic active');
                    return;
                }
                if (isListening && !isSpeaking && !hasSpokenThisSession) {
                    console.log('[Orb] No speech detected - auto-stopping');
                    stopListening();
                }
            }, NO_SPEECH_TIMEOUT_MS);
        }
        
        function clearNoSpeechTimeout() {
            if (noSpeechTimeoutId) {
                clearTimeout(noSpeechTimeoutId);
                noSpeechTimeoutId = null;
            }
        }
        
        function clearAllSpeechTimers() {
            clearSilenceTimeout();
            clearNoSpeechTimeout();
        }
        
        // Initialize audio context for TTS playback
        let ttsAudioInitialized = false;
        
        function initTTSAudio() {
            if (!ttsAudioContext) {
                console.log('[Orb] Creating TTS AudioContext');
                ttsAudioContext = new AudioContext({ sampleRate: 24000 });
            }
            return ttsAudioContext;
        }
        
        // Pre-warm audio context on user interaction (required by browser autoplay policy)
        async function ensureTTSAudioReady() {
            if (ttsAudioInitialized) return true;
            
            try {
                const ctx = initTTSAudio();
                if (ctx.state === 'suspended') {
                    console.log('[Orb] Resuming suspended TTS AudioContext...');
                    await ctx.resume();
                }
                
                if (ctx.state === 'running') {
                    ttsAudioInitialized = true;
                    console.log('[Orb] TTS AudioContext ready');
                    return true;
                }
                
                console.warn('[Orb] TTS AudioContext state:', ctx.state);
                return false;
            } catch (e) {
                console.error('[Orb] Failed to initialize TTS AudioContext:', e);
                return false;
            }
        }
        
        // Initialize audio on first user interaction
        document.addEventListener('click', () => ensureTTSAudioReady(), { once: true });
        document.addEventListener('touchstart', () => ensureTTSAudioReady(), { once: true });
        document.addEventListener('keydown', () => ensureTTSAudioReady(), { once: true });
        
        // Convert base64 PCM16 to Float32 for playback
        function base64ToFloat32(base64) {
            const binaryString = atob(base64);
            const bytes = new Uint8Array(binaryString.length);
            for (let i = 0; i < binaryString.length; i++) {
                bytes[i] = binaryString.charCodeAt(i);
            }
            const pcm16 = new Int16Array(bytes.buffer);
            const float32 = new Float32Array(pcm16.length);
            for (let i = 0; i < pcm16.length; i++) {
                float32[i] = pcm16[i] / 32768;
            }
            return float32;
        }
        
        // Play WAV audio from base64 string (from TTS API)
        async function playWAVAudio(base64Audio) {
            try {
                console.log('[Orb] playWAVAudio: decoding WAV...');
                
                // Decode base64 to ArrayBuffer
                const binaryString = atob(base64Audio);
                const bytes = new Uint8Array(binaryString.length);
                for (let i = 0; i < binaryString.length; i++) {
                    bytes[i] = binaryString.charCodeAt(i);
                }
                
                // Get or create AudioContext
                const ctx = initTTSAudio();
                if (ctx.state === 'suspended') {
                    console.log('[Orb] playWAVAudio: resuming AudioContext...');
                    await ctx.resume();
                }
                
                // Decode WAV using Web Audio API
                const audioBuffer = await ctx.decodeAudioData(bytes.buffer.slice(0));
                
                console.log(`[Orb] playWAVAudio: decoded ${audioBuffer.duration.toFixed(2)}s of audio`);
                
                // Create source and play
                const source = ctx.createBufferSource();
                source.buffer = audioBuffer;
                source.connect(ctx.destination);
                
                source.onended = () => {
                    console.log('[Orb] playWAVAudio: playback complete');
                    isSpeaking = false;
                    ttsEndTime = Date.now();
                };
                
                source.start(0);
                console.log('[Orb] playWAVAudio: playing...');
                
            } catch (e) {
                console.error('[Orb] playWAVAudio error:', e);
                isSpeaking = false;
            }
        }
        
        // Play a pleasant ready chime using Web Audio API
        // This is a short tone that won't trigger function calling
        async function playReadyChime() {
            try {
                const ctx = new AudioContext();
                
                // Resume if suspended (autoplay policy)
                if (ctx.state === 'suspended') {
                    await ctx.resume();
                }
                
                const now = ctx.currentTime;
                
                // Create a pleasant two-tone ascending chime
                const frequencies = [523.25, 659.25]; // C5 and E5 - pleasant major third
                const duration = 0.12; // Very short
                const gap = 0.08;
                
                frequencies.forEach((freq, i) => {
                    const osc = ctx.createOscillator();
                    const gain = ctx.createGain();
                    
                    osc.type = 'sine';
                    osc.frequency.value = freq;
                    
                    // Gentle envelope - louder for audibility
                    const startTime = now + (i * (duration + gap));
                    gain.gain.setValueAtTime(0, startTime);
                    gain.gain.linearRampToValueAtTime(0.3, startTime + 0.02); // Louder
                    gain.gain.exponentialRampToValueAtTime(0.01, startTime + duration);
                    
                    osc.connect(gain);
                    gain.connect(ctx.destination);
                    
                    osc.start(startTime);
                    osc.stop(startTime + duration);
                });
                
                // Close context after chime finishes
                setTimeout(() => ctx.close(), 500);
                console.log('[Orb] Ready chime played');
            } catch (e) {
                console.warn('[Orb] Could not play ready chime:', e);
            }
        }
        
        // Play accumulated audio chunks
        async function playTTSAudio() {
            if (ttsAudioChunks.length === 0) {
                console.log('[Orb] playTTSAudio called but no chunks');
                return;
            }

            console.log('[Orb] playTTSAudio called with', ttsAudioChunks.length, 'chunks');

            try {
                const ctx = initTTSAudio();
                
                // Always try to resume - this is critical for audio to play
                if (ctx.state === 'suspended') {
                    console.log('[Orb] Attempting to resume AudioContext...');
                    try {
                        await ctx.resume();
                        console.log('[Orb] AudioContext resumed, state:', ctx.state);
                    } catch (resumeError) {
                        console.error('[Orb] Failed to resume AudioContext:', resumeError);
                        // Try creating a new context
                        ttsAudioContext = null;
                        ttsAudioInitialized = false;
                        const newCtx = initTTSAudio();
                        await newCtx.resume();
                    }
                }
                
                const currentCtx = ttsAudioContext;
                if (!currentCtx || currentCtx.state !== 'running') {
                    console.error('[Orb] AudioContext not running, state:', currentCtx?.state);
                    // Store chunks for later retry
                    return;
                }

                // Combine all chunks
                const totalLength = ttsAudioChunks.reduce((sum, chunk) => sum + chunk.length, 0);
                const combined = new Float32Array(totalLength);
                let offset = 0;
                for (const chunk of ttsAudioChunks) {
                    combined.set(chunk, offset);
                    offset += chunk.length;
                }

                // Create buffer and play
                const buffer = currentCtx.createBuffer(1, combined.length, 24000);
                buffer.copyToChannel(combined, 0);

                const source = currentCtx.createBufferSource();
                source.buffer = buffer;
                source.connect(currentCtx.destination);
                source.start();

                console.log('[Orb] Playing TTS audio, samples:', combined.length, 'duration:', (combined.length / 24000).toFixed(2) + 's');
                ttsAudioChunks = [];
                
            } catch (e) {
                console.error('[Orb] Error playing TTS audio:', e);
                ttsAudioChunks = []; // Clear to prevent memory buildup
            }
        }
        
        // ==================== PERSISTENT AUDIO LISTENER ====================
        // CRITICAL: This listener receives audio even when orb has "disconnected"
        // Handles the case where backend speaks after orb stopped listening
        (function initPersistentAudioListener() {
            if (window.orbAPI?.onEvent) {
                let firstAudioReceived = false;
                
                window.orbAPI.onEvent((event) => {
                    // Only handle audio events in this persistent listener
                    if (event.type === 'audio_delta' && event.audio) {
                        // Pre-warm audio context on first audio chunk
                        if (!firstAudioReceived) {
                            firstAudioReceived = true;
                            console.log('[Orb] Persistent: First audio received, ensuring AudioContext ready');
                            ensureTTSAudioReady();
                        }
                        
                        console.log('[Orb] Persistent: audio_delta received, chunk size:', event.audio.length);
                        const float32 = base64ToFloat32(event.audio);
                        ttsAudioChunks.push(float32);
                        if (!isSpeaking) {
                            isSpeaking = true;
                            startSpeakingSafetyTimeout();
                        }
                    } else if (event.type === 'audio_wav' && event.audio) {
                        // WAV format from TTS API - decode and play directly
                        console.log('[Orb] Persistent: audio_wav received, size:', event.audio.length);
                        if (!isSpeaking) {
                            isSpeaking = true;
                            startSpeakingSafetyTimeout();
                        }
                        window._agentResponseText = '';  // Clear for new response
                        
                        // NOTE: Mic gating is now handled centrally by HUD API
                        // (voice-speaker.js calls hudApi.speechStarted() which
                        //  causes voice-listener.js to drop audio during playback)
                        
                        playWAVAudio(event.audio);
                    } else if (event.type === 'audio_done') {
                        console.log('[Orb] Persistent: audio_done, playing', ttsAudioChunks.length, 'chunks');
                        if (ttsAudioChunks.length > 0) {
                            playTTSAudio();
                        }
                        isSpeaking = false;
                        ttsEndTime = Date.now();
                        // Clear safety timeout since audio_done arrived properly
                        if (speakingTimeoutId) {
                            clearTimeout(speakingTimeoutId);
                            speakingTimeoutId = null;
                        }
                    } else if (event.type === 'clear_audio_buffer') {
                        console.log('[Orb] Persistent: clearing audio buffer');
                        ttsAudioChunks = [];
                        isSpeaking = false;
                        if (speakingTimeoutId) {
                            clearTimeout(speakingTimeoutId);
                            speakingTimeoutId = null;
                        }
                    }
                });
                console.log('[Orb] Persistent audio listener registered');
            } else {
                console.error('[Orb] orbAPI.onEvent not available - audio will not work!');
            }
        })();
        // ============================================================
        
        // ==================== FRONTEND TTS REMOVED ====================
        // ALL TTS is now handled by the backend via exchange-bridge.js -> realtimeSpeech.speak()
        // The frontend only tracks TTS state for UI purposes (cooldown, etc.)
        // This eliminates duplicate TTS paths and race conditions.
        // ============================================================
        
        // ==================== DISAMBIGUATION STATE ====================
        let pendingDisambiguation = null;
        let disambiguationListenerRemove = null;
        
        /**
         * Handle disambiguation flow
         * @param {Object} result - Classification result with clarification data
         * @param {string} originalTranscript - The original user transcript
         */
        async function handleDisambiguation(result, originalTranscript) {
            console.log('[Orb] Starting disambiguation flow:', result);
            
            // Create disambiguation state
            const disambiguationState = {
                id: 'disamb_' + Date.now(),
                originalTranscript: originalTranscript,
                question: result.clarificationQuestion || 'What did you mean?',
                options: result.clarificationOptions || [],
                createdAt: Date.now(),
                expiresAt: Date.now() + 30000, // 30 second timeout
            };
            
            pendingDisambiguation = disambiguationState;
            
            // NOTE: Backend speaks via exchange-bridge.js
            console.log('[Orb] Disambiguation question (backend speaks):', disambiguationState.question);
            
            // Show transcript with question
            showTranscript(disambiguationState.question, false);
            
            // Set up listener for disambiguation response
            setupDisambiguationListeners();
        }
        
        /**
         * Set up listeners for disambiguation responses
         */
        function setupDisambiguationListeners() {
            // Clean up previous listener if any
            if (disambiguationListenerRemove) {
                disambiguationListenerRemove();
                disambiguationListenerRemove = null;
            }
            
            // Listen for option selection via HUD API disambiguation events
            // (The onDisambiguation listener in the main event section also handles this,
            //  but we keep a specific listener for the active disambiguation state here)
            if (window.agentHUD?.onDisambiguation) {
                disambiguationListenerRemove = window.agentHUD.onDisambiguation((state) => {
                    if (state.type === 'selected' || state.optionIndex !== undefined) {
                        console.log('[Orb] Disambiguation option selected via HUD API:', state);
                        resolveDisambiguation({
                            optionIndex: state.optionIndex,
                            option: state.option,
                            mergedTranscript: state.mergedTranscript,
                        });
                    }
                });
            }
            
            // Set timeout to cancel disambiguation
            setTimeout(() => {
                if (pendingDisambiguation) {
                    console.log('[Orb] Disambiguation timed out');
                    cancelDisambiguation();
                    // NOTE: Backend handles timeout feedback
                }
            }, 30000);
        }
        
        /**
         * Resolve disambiguation with selected option
         * @param {Object} selection - { optionIndex, option, mergedTranscript }
         */
        async function resolveDisambiguation(selection) {
            if (!pendingDisambiguation) return;
            
            console.log('[Orb] Resolving disambiguation:', selection);
            
            const state = pendingDisambiguation;
            pendingDisambiguation = null;
            
            // Clean up listeners
            if (disambiguationListenerRemove) {
                disambiguationListenerRemove();
                disambiguationListenerRemove = null;
            }
            
            // Hide transcript
            hideTranscript();
            
            // If we got a selected option with action, submit the clarified request
            if (selection.option && (selection.option.action || selection.mergedTranscript)) {
                console.log('[Orb] Disambiguation selected - submitting via HUD API');
                
                try {
                    // Submit the clarified transcript through the full pipeline
                    const clarifiedText = selection.mergedTranscript || 
                        `${state.originalTranscript} (${selection.option.label})`;
                    const result = await window.agentHUD.submitTask(clarifiedText, {
                        toolId: 'orb',
                        skipFilter: true, // Already validated by disambiguation
                        metadata: {
                            disambiguationResolved: true,
                            originalTranscript: state.originalTranscript,
                            clarification: selection.option.label,
                        },
                    });
                    
                    console.log('[Orb] Disambiguation action submitted via HUD API:', result);
                    // Backend speaks result via exchange-bridge.js
                } catch (err) {
                    console.error('[Orb] Error submitting disambiguation action:', err);
                    // NOTE: Backend handles error feedback
                }
            } else if (selection.mergedTranscript) {
                // Re-classify with merged transcript
                console.log('[Orb] Re-trying with merged transcript');
                await processVoiceCommand(selection.mergedTranscript);
            }
        }
        
        /**
         * Cancel pending disambiguation
         */
        function cancelDisambiguation() {
            if (!pendingDisambiguation) return;
            
            console.log('[Orb] Cancelling disambiguation');
            
            // Save stateId before nulling the reference
            const stateId = pendingDisambiguation.stateId;
            pendingDisambiguation = null;
            
            if (disambiguationListenerRemove) {
                disambiguationListenerRemove();
                disambiguationListenerRemove = null;
            }
            
            hideTranscript();
            
            // Cancel via HUD API if we have a stateId
            if (window.agentHUD?.cancelDisambiguation && stateId) {
                try {
                    window.agentHUD.cancelDisambiguation(stateId);
                } catch (e) {
                    console.warn('[Orb] HUD API cancel disambiguation error:', e);
                }
            }
        }
        
        /**
         * Check if we're waiting for disambiguation
         */
        function isAwaitingDisambiguation() {
            return pendingDisambiguation !== null && 
                   pendingDisambiguation.expiresAt > Date.now();
        }
        
        // ==================== VOICE COMMAND PROCESSING ====================
        
        // ==================== VOICE COMMAND PROCESSING (via HUD API) ====================
        
        let _processCallCount = 0;
        async function processVoiceCommand(transcript) {
            _processCallCount++;
            console.log('[Orb] Processing command:', transcript);
            
            // Check if Agent Composer is active - relay voice to it
            if (window.orbAPI?.isComposerActive) {
                try {
                    const composerActive = await window.orbAPI.isComposerActive();
                    if (composerActive) {
                        console.log('[Orb] Relaying to Agent Composer:', transcript);
                        if (window.orbAPI.cancelResponse) {
                            await window.orbAPI.cancelResponse();
                        }
                        const relayed = await window.orbAPI.relayToComposer(transcript);
                        if (relayed) return;
                    }
                } catch (e) {
                    console.warn('[Orb] Could not check composer status:', e);
                }
            }
            
            // Check if this is a response to a pending disambiguation
            if (isAwaitingDisambiguation()) {
                console.log('[Orb] Processing as disambiguation response');
                const state = pendingDisambiguation;
                const normalizedResponse = transcript.toLowerCase().trim();
                
                const numberWords = {
                    'one': 0, 'first': 0, '1': 0,
                    'two': 1, 'second': 1, '2': 1,
                    'three': 2, 'third': 2, '3': 2,
                    'four': 3, 'fourth': 3, '4': 3,
                    'five': 4, 'fifth': 4, '5': 4,
                };
                
                let matchedIndex = -1;
                for (const [word, index] of Object.entries(numberWords)) {
                    if (normalizedResponse.includes(word) && index < state.options.length) {
                        matchedIndex = index;
                        break;
                    }
                }
                
                if (matchedIndex === -1) {
                    matchedIndex = state.options.findIndex(opt =>
                        normalizedResponse.includes(opt.label.toLowerCase()) ||
                        opt.label.toLowerCase().includes(normalizedResponse)
                    );
                }
                
                if (matchedIndex >= 0) {
                    // Use HUD API to select disambiguation option
                    if (state.stateId && window.agentHUD) {
                        try {
                            await window.agentHUD.selectDisambiguationOption(state.stateId, matchedIndex);
                        } catch (e) {
                            console.warn('[Orb] HUD API disambiguation select failed, using legacy:', e);
                        }
                    }
                    resolveDisambiguation({
                        optionIndex: matchedIndex,
                        option: state.options[matchedIndex],
                        mergedTranscript: `${state.originalTranscript} (clarification: ${state.options[matchedIndex].label})`,
                    });
                } else {
                    resolveDisambiguation({
                        mergedTranscript: `${state.originalTranscript} (user clarified: ${transcript})`,
                    });
                }
                return;
            }
            
            try {
                // ==================== SUBMIT VIA HUD API ====================
                // The HUD API routes through the full exchange bridge pipeline:
                //   transcript filter -> dedup -> Router -> critical commands ->
                //   disambiguation -> exchange auction -> agent execution -> voice cues
                // Task-tool mapping is tracked so events route back to 'orb'.
                
                if (!window.agentHUD || !window.agentHUD.submitTask) {
                    console.error('[Orb] window.agentHUD not available! Using fallback.');
                    const fallback = await window.orbAPI.submit(transcript);
                    console.log('[Orb] Fallback result:', fallback);
                    return;
                }

                const result = await window.agentHUD.submitTask(transcript, {
                    toolId: 'orb',
                    skipFilter: false,
                });
                console.log('[Orb] HUD API submit result:', JSON.stringify(result));
                
                // Handle error returns
                if (!result || result.error) {
                    console.warn('[Orb] submitTask returned error:', result?.error);
                    return;
                }
                
                // Backend handled AND suppressed AI response (async exchange path)
                if (result.suppressAIResponse === true) {
                    console.log('[Orb] Response suppressed by backend (exchange events will speak)');
                    hideTranscript();
                    return;
                }
                
                // Backend handled but didn't suppress -- backend already spoke
                if (result.handled && result.message && result.suppressAIResponse === false) {
                    console.log('[Orb] Backend handled with message:', result.message);
                    hideTranscript();
                    return;
                }
                
                // Disambiguation needed
                if (result.clarificationNeeded && result.clarificationOptions?.length > 0) {
                    await handleDisambiguation(result, transcript);
                    return;
                }
                
                // Transcript was filtered as garbled
                if (result.needsClarification && result.filterReason) {
                    console.log('[Orb] Transcript filtered:', result.filterReason);
                    hideTranscript();
                    return;
                }
                
                if (result.queued && result.taskId) {
                    // Task successfully queued -- backend handles voice cues
                    console.log('[Orb] Task queued:', result.taskId);
                    
                } else if (result.handled) {
                    // Agent handled directly (follow-up, etc.)
                    console.log('[Orb] Task handled by agent, needsInput:', result.needsInput);
                    
                    if (result.needsInput) {
                        isWaitingForUserInput = true;
                        console.log('[Orb] Multi-turn continues: still waiting for input');
                    } else {
                        isWaitingForUserInput = false;
                        console.log('[Orb] Multi-turn complete');
                    }
                    
                } else {
                    // No action recognized -- backend already handled response
                    console.log('[Orb] No specific action, transcript recorded');
                }
                
            } catch (err) {
                console.error('[Orb] Submit error:', err?.message || err);
                
                // Speak error via backend TTS
                try {
                    await window.orbAPI.speak('Sorry, something went wrong. ' + (err?.message || ''));
                } catch (speakErr) {
                    console.warn('[Orb] Could not speak error:', speakErr);
                }
            }
        }
        
        // ==================== EVENT LISTENERS (HUD API only) ====================
        // All events come through the centralized HUD API.
        // The orb filters by toolId === 'orb' for tool-scoped events.
        if (window.agentHUD) {
            console.log('[Orb] Setting up HUD API event listeners');
            
            // Disambiguation events
            window.agentHUD.onDisambiguation((state) => {
                console.log('[Orb] HUD API disambiguation event:', state);
                if (!state.toolId || state.toolId === 'orb') {
                    if (typeof handleDisambiguation === 'function' && state.options?.length > 0) {
                        handleDisambiguation({
                            clarificationNeeded: true,
                            clarificationOptions: state.options.map((o, i) => ({
                                label: o.label,
                                description: o.description,
                                index: i,
                            })),
                            stateId: state.stateId,
                        }, '');
                    }
                }
            });
            
            // Needs-input events (agent follow-up questions)
            window.agentHUD.onNeedsInput((request) => {
                console.log('[Orb] HUD API needs-input:', request);
                if (!request.toolId || request.toolId === 'orb') {
                    isWaitingForUserInput = true;
                    console.log('[Orb] Multi-turn: waiting for user input, keeping mic active');
                    if (!isListening && !isSpeaking) {
                        console.log('[Orb] Restarting listening for multi-turn');
                        startListening();
                    }
                }
            });
            
            // Lifecycle events (task:queued, task:assigned, task:settled, etc.)
            window.agentHUD.onLifecycle((event) => {
                console.log('[Orb] HUD API lifecycle:', event.type, event.taskId || '');
                
                // Handle task completion
                if (event.type === 'task:settled' || event.type === 'task:completed') {
                    isWaitingForUserInput = false;
                }
                // Handle task failure / dead letter
                if (event.type === 'task:dead_letter' || event.type === 'task:route_to_error_agent') {
                    isWaitingForUserInput = false;
                }
            });
            
            // Result events (final result from agent execution)
            window.agentHUD.onResult((result) => {
                console.log('[Orb] HUD API result:', result.taskId, result.success);
                isWaitingForUserInput = false;
            });
            
            // Speech state changes (centralized mic gating)
            window.agentHUD.onSpeechState((state) => {
                isSpeaking = state.isSpeaking;
                if (!state.isSpeaking) {
                    ttsEndTime = Date.now();
                }
            });
        }
        
        // DOM elements
        const orb = document.getElementById('orb');
        const transcriptEl = document.getElementById('transcript');
        const contextMenu = document.getElementById('contextMenu');
        const textChatPanel = document.getElementById('textChatPanel');
        const chatMessages = document.getElementById('chatMessages');
        
        // Debug: check if elements exist
        console.log('[Orb] Elements found:', {
            orb: !!orb,
            contextMenu: !!contextMenu,
            textChatPanel: !!textChatPanel
        });
        const chatInput = document.getElementById('chatInput');
        const chatSendBtn = document.getElementById('chatSendBtn');
        const chatCloseBtn = document.getElementById('chatCloseBtn');
        
        // Text chat state
        let isTextChatOpen = false;
        let chatHistory = [];
        let chatInactivityTimer = null;
        const CHAT_INACTIVITY_TIMEOUT = 30000; // 30 seconds
        
        // Reset the chat inactivity timer
        function resetChatInactivityTimer() {
            if (chatInactivityTimer) {
                clearTimeout(chatInactivityTimer);
                chatInactivityTimer = null;
            }
            
            // Only set timer if chat is open
            if (isTextChatOpen) {
                chatInactivityTimer = setTimeout(() => {
                    console.log('[TextChat] Auto-closing due to inactivity');
                    closeTextChat();
                }, CHAT_INACTIVITY_TIMEOUT);
            }
        }
        
        // Clear inactivity timer
        function clearChatInactivityTimer() {
            if (chatInactivityTimer) {
                clearTimeout(chatInactivityTimer);
                chatInactivityTimer = null;
            }
        }
        
        // ==================== CONTEXT MENU ====================
        
        function showContextMenu(x, y) {
            console.log('[Orb] Showing context menu at window coords:', x, y);
            
            const MIN_MARGIN = 8;
            
            // Window dimensions (the BrowserWindow client area)
            const windowWidth = window.innerWidth;
            const windowHeight = window.innerHeight;
            
            // Window position on physical screen
            const windowX = window.screenX;
            const windowY = window.screenY;
            
            // Physical screen dimensions
            const screenWidth = window.screen.availWidth;
            const screenHeight = window.screen.availHeight;
            
            // Measure actual menu size (show off-screen first to get dimensions)
            contextMenu.style.left = '-9999px';
            contextMenu.style.top = '-9999px';
            contextMenu.classList.add('visible');
            
            const rect = contextMenu.getBoundingClientRect();
            const menuWidth = rect.width;
            const menuHeight = rect.height;
            
            // Calculate where menu would appear in screen coordinates
            let screenMenuX = windowX + x;
            let screenMenuY = windowY + y;
            
            console.log('[Orb] Screen coords - window:', windowX, windowY, 'click:', screenMenuX, screenMenuY, 'screen size:', screenWidth, screenHeight, 'menu size:', menuWidth, menuHeight);
            
            // Horizontal: check if menu would go off right edge of screen
            if (screenMenuX + menuWidth > screenWidth - MIN_MARGIN) {
                // Flip to left of cursor
                screenMenuX = Math.max(MIN_MARGIN, screenMenuX - menuWidth);
            }
            // Ensure not off left edge of screen
            if (screenMenuX < MIN_MARGIN) {
                screenMenuX = MIN_MARGIN;
            }
            
            // Vertical: check if menu would go off bottom edge of screen
            if (screenMenuY + menuHeight > screenHeight - MIN_MARGIN) {
                // Flip to above cursor
                screenMenuY = screenMenuY - menuHeight;
            }
            // Ensure not off top edge of screen
            if (screenMenuY < MIN_MARGIN) {
                screenMenuY = MIN_MARGIN;
            }
            
            // Convert back to window-relative coordinates
            let finalX = screenMenuX - windowX;
            let finalY = screenMenuY - windowY;
            
            // Also ensure menu stays within window bounds (for very edge cases)
            finalX = Math.max(0, Math.min(finalX, windowWidth - menuWidth));
            finalY = Math.max(0, Math.min(finalY, windowHeight - menuHeight));
            
            // Apply final position
            contextMenu.style.position = 'fixed';
            contextMenu.style.left = finalX + 'px';
            contextMenu.style.top = finalY + 'px';
            contextMenu.style.bottom = 'auto';
            contextMenu.style.right = 'auto';
            
            console.log('[Orb] Context menu final position:', finalX, finalY);
        }
        
        function hideContextMenu() {
            contextMenu.classList.remove('visible');
        }
        
        // Context menu handlers
        document.getElementById('menuTextChat').addEventListener('click', () => {
            hideContextMenu();
            openTextChat();
        });
        
        document.getElementById('menuVoice').addEventListener('click', () => {
            hideContextMenu();
            closeTextChat();
            if (!isListening) {
                startListening();
            }
        });
        
        document.getElementById('menuSettings').addEventListener('click', () => {
            hideContextMenu();
            // Open settings via IPC
            if (window.orbAPI?.openSettings) {
                window.orbAPI.openSettings();
            }
        });
        
        // Close menu on click outside
        document.addEventListener('click', (e) => {
            if (!contextMenu.contains(e.target) && e.target !== orb) {
                hideContextMenu();
            }
        });
        
        // ==================== TEXT CHAT ====================
        
        // Track current anchor for collapse
        let currentChatAnchor = 'bottom-right';
        
        async function openTextChat() {
            isTextChatOpen = true;
            
            // Determine which quadrant of the screen the orb is in
            const windowX = window.screenX;
            const windowY = window.screenY;
            const screenWidth = window.screen.availWidth;
            const screenHeight = window.screen.availHeight;
            
            // Calculate center points
            const windowCenterX = windowX + (window.outerWidth / 2);
            const windowCenterY = windowY + (window.outerHeight / 2);
            const screenCenterX = screenWidth / 2;
            const screenCenterY = screenHeight / 2;
            
            // Determine quadrant
            const isRight = windowCenterX > screenCenterX;
            const isBottom = windowCenterY > screenCenterY;
            
            // Remove existing position classes
            const orbContainer = document.querySelector('.orb-container');
            orbContainer.classList.remove('orb-bottom-right', 'orb-bottom-left', 'orb-top-right', 'orb-top-left');
            textChatPanel.classList.remove('pos-top-left', 'pos-top-right', 'pos-bottom-left', 'pos-bottom-right');
            
            // Determine anchor and apply classes based on quadrant
            // Chat panel appears opposite to orb position
            if (isBottom && isRight) {
                // Orb at bottom-right of screen: orb stays bottom-right, chat above
                currentChatAnchor = 'bottom-right';
                orbContainer.classList.add('orb-bottom-right');
                textChatPanel.classList.add('pos-top-left');
            } else if (isBottom && !isRight) {
                // Orb at bottom-left of screen: orb stays bottom-left, chat above
                currentChatAnchor = 'bottom-left';
                orbContainer.classList.add('orb-bottom-left');
                textChatPanel.classList.add('pos-top-right');
            } else if (!isBottom && isRight) {
                // Orb at top-right of screen: orb moves to top-right, chat below
                currentChatAnchor = 'top-right';
                orbContainer.classList.add('orb-top-right');
                textChatPanel.classList.add('pos-bottom-left');
            } else {
                // Orb at top-left of screen: orb moves to top-left, chat below
                currentChatAnchor = 'top-left';
                orbContainer.classList.add('orb-top-left');
                textChatPanel.classList.add('pos-bottom-right');
            }
            
            console.log('[TextChat] Position:', isRight ? 'right' : 'left', isBottom ? 'bottom' : 'top', 'anchor:', currentChatAnchor);
            
            // Expand the window to accommodate the chat panel
            if (window.orbAPI?.expandForChat) {
                try {
                    await window.orbAPI.expandForChat(currentChatAnchor);
                } catch (e) {
                    console.warn('[TextChat] Could not expand window:', e);
                }
            }
            
            // Show the chat panel
            textChatPanel.classList.add('visible');
            setTimeout(() => chatInput.focus(), 150);
            // Start inactivity timer
            resetChatInactivityTimer();
        }
        
        async function closeTextChat() {
            isTextChatOpen = false;
            textChatPanel.classList.remove('visible');
            // Clear inactivity timer
            clearChatInactivityTimer();
            
            // Collapse the window back to original size/position
            if (window.orbAPI?.collapseFromChat) {
                try {
                    await window.orbAPI.collapseFromChat();
                } catch (e) {
                    console.warn('[TextChat] Could not collapse window:', e);
                }
            }
            
            // Reset orb to default position after collapse
            const orbContainer = document.querySelector('.orb-container');
            orbContainer.classList.remove('orb-bottom-right', 'orb-bottom-left', 'orb-top-right', 'orb-top-left');
            textChatPanel.classList.remove('pos-top-left', 'pos-top-right', 'pos-bottom-left', 'pos-bottom-right');
            currentChatAnchor = 'bottom-right'; // Reset to default
        }
        
        chatCloseBtn.addEventListener('click', closeTextChat);
        
        // Reset inactivity timer on any chat panel interaction
        chatInput.addEventListener('input', resetChatInactivityTimer);
        chatInput.addEventListener('focus', resetChatInactivityTimer);
        chatMessages.addEventListener('scroll', resetChatInactivityTimer);
        textChatPanel.addEventListener('mousemove', resetChatInactivityTimer);
        
        function addChatMessage(type, text) {
            // Remove empty state if present
            const emptyState = chatMessages.querySelector('.chat-empty');
            if (emptyState) {
                emptyState.remove();
            }
            
            const msg = document.createElement('div');
            msg.className = `chat-message ${type}`;
            msg.textContent = text;
            chatMessages.appendChild(msg);
            chatMessages.scrollTop = chatMessages.scrollHeight;
            
            chatHistory.push({ type, text, timestamp: Date.now() });
            
            // Reset inactivity timer when messages are added
            resetChatInactivityTimer();
        }
        
        async function sendChatMessage() {
            const text = chatInput.value.trim();
            if (!text) return;
            
            // Reset inactivity timer on send
            resetChatInactivityTimer();
            
            // Clear input
            chatInput.value = '';
            chatSendBtn.disabled = true;
            
            // Add user message
            addChatMessage('user', text);
            
            // Process through the same command handler as voice
            try {
                // Show processing state
                addChatMessage('system', 'Processing...');
                
                // Use the same processVoiceCommand function
                await processVoiceCommand(text);
                
                // Remove processing message
                const processingMsg = chatMessages.querySelector('.chat-message.system:last-child');
                if (processingMsg && processingMsg.textContent === 'Processing...') {
                    processingMsg.remove();
                }
                
                // Add confirmation
                addChatMessage('assistant', 'Got it. Working on that.');
                
            } catch (error) {
                console.error('[TextChat] Error:', error);
                addChatMessage('assistant', 'Sorry, something went wrong.');
            }
            
            chatSendBtn.disabled = false;
        }
        
        chatSendBtn.addEventListener('click', sendChatMessage);
        
        chatInput.addEventListener('keydown', (e) => {
            if (e.key === 'Enter' && !e.shiftKey) {
                e.preventDefault();
                sendChatMessage();
            }
            if (e.key === 'Escape') {
                closeTextChat();
            }
        });
        
        // Explicit paste support for frameless window (Edit menu may not work in frameless windows)
        chatInput.addEventListener('paste', (e) => {
            console.log('[TextChat] Paste event received');
            resetChatInactivityTimer();
        });
        
        // Manual paste handler for frameless window where native Cmd+V may not work
        chatInput.addEventListener('keydown', async (e) => {
            if ((e.metaKey || e.ctrlKey) && e.key.toLowerCase() === 'v') {
                // Prevent default to handle manually (native paste often fails in frameless windows)
                e.preventDefault();
                
                let text = null;
                
                // Try Electron clipboard API first (most reliable in Electron)
                if (window.clipboardAPI && window.clipboardAPI.readText) {
                    try {
                        text = window.clipboardAPI.readText();
                        console.log('[TextChat] Got text from Electron clipboard API');
                    } catch (err) {
                        console.warn('[TextChat] Electron clipboard failed:', err.message);
                    }
                }
                
                // Fall back to browser Clipboard API
                if (!text && navigator.clipboard && navigator.clipboard.readText) {
                    try {
                        text = await navigator.clipboard.readText();
                        console.log('[TextChat] Got text from browser Clipboard API');
                    } catch (err) {
                        console.warn('[TextChat] Browser clipboard failed:', err.message);
                    }
                }
                
                if (text) {
                    const start = chatInput.selectionStart;
                    const end = chatInput.selectionEnd;
                    const value = chatInput.value;
                    chatInput.value = value.substring(0, start) + text + value.substring(end);
                    chatInput.selectionStart = chatInput.selectionEnd = start + text.length;
                    chatInput.dispatchEvent(new Event('input', { bubbles: true }));
                    console.log('[TextChat] Pasted text successfully');
                } else {
                    console.warn('[TextChat] Could not read clipboard');
                }
                
                resetChatInactivityTimer();
            }
        });
        
        // Update UI based on state
        function updateUI(state, text = '') {
            orb.classList.remove('listening', 'processing', 'error');
            
            switch (state) {
                case 'listening':
                    orb.classList.add('listening');
                    orb.title = 'Click to stop';
                    break;
                case 'processing':
                    orb.classList.add('processing');
                    break;
                case 'error':
                    orb.classList.add('error');
                    showTranscript(text || 'Error occurred', false);
                    setTimeout(() => {
                        orb.classList.remove('error');
                        hideTranscript();
                    }, 3000);
                    break;
                default:
                    orb.title = 'Click to start listening';
            }
        }
        
        let hideTranscriptTimeout = null;
        
        function showTranscript(text, isInterim = false) {
            // Clear any pending hide
            if (hideTranscriptTimeout) {
                clearTimeout(hideTranscriptTimeout);
                hideTranscriptTimeout = null;
            }
            
            // Remove fading state if it was fading
            transcriptEl.classList.remove('fading', 'below', 'left-of-orb');
            
            transcriptEl.textContent = text;
            transcriptEl.classList.toggle('interim', isInterim);
            
            // Auto-scroll to bottom for streaming text
            transcriptEl.scrollTop = transcriptEl.scrollHeight;

            // Get the orb's position within the window
            const orbContainer = document.querySelector('.orb-container');
            const orbRect = orbContainer.getBoundingClientRect();
            const windowWidth = window.innerWidth;
            const windowHeight = window.innerHeight;
            
            // Window position on physical screen (same approach as context menu)
            const windowX = window.screenX;
            const windowY = window.screenY;
            const screenWidth = window.screen.availWidth;
            const screenHeight = window.screen.availHeight;
            
            // Tooltip dimensions
            const tooltipMaxWidth = 320;
            const tooltipEstimatedHeight = Math.min(150, 50 + text.length * 0.5);
            const margin = 15;
            const gap = 15;
            
            // Orb's center in screen coordinates
            const orbScreenCenterX = windowX + orbRect.left + orbRect.width / 2;
            const isOrbOnRightHalf = orbScreenCenterX > screenWidth / 2;
            
            let top, bottom, left, right;
            
            if (isOrbOnRightHalf) {
                //  Orb is on the RIGHT half of screen 
                // Show tooltip to the LEFT of the orb, vertically centered
                transcriptEl.classList.add('left-of-orb');
                left = margin;
                right = windowWidth - orbRect.left + gap; // right edge stops before the orb
                // Vertically center on the orb, clamped to window bounds
                const centerTop = orbRect.top + orbRect.height / 2 - tooltipEstimatedHeight / 2;
                top = Math.max(margin, Math.min(centerTop, windowHeight - tooltipEstimatedHeight - margin));
                bottom = 'auto';
            } else {
                //  Orb is on the LEFT half of screen 
                // Show tooltip to the RIGHT of the orb, above it
                // (Window has ~300px to the left of the orb but that's off-screen here,
                //  so position above the orb, aligned to orb's left edge extending right)
                top = 'auto';
                bottom = windowHeight - orbRect.top + gap;
                // Align tooltip starting at the orb's left edge, extending right on screen
                let screenLeft = Math.max(margin, windowX + orbRect.left);
                if (screenLeft + tooltipMaxWidth > screenWidth - margin) {
                    screenLeft = screenWidth - margin - tooltipMaxWidth;
                }
                left = Math.max(0, screenLeft - windowX);
                right = 'auto';
            }
            
            // Apply position
            transcriptEl.style.top = typeof top === 'number' ? top + 'px' : top;
            transcriptEl.style.bottom = typeof bottom === 'number' ? bottom + 'px' : bottom;
            transcriptEl.style.left = typeof left === 'number' ? left + 'px' : left;
            transcriptEl.style.right = typeof right === 'number' ? right + 'px' : right;
            
            // Now show it
            transcriptEl.classList.add('visible');
        }
        
        function hideTranscript() {
            // Start elegant fade out
            transcriptEl.classList.add('fading');
            transcriptEl.classList.remove('visible');
            
            // Clean up classes and inline styles after animation completes
            hideTranscriptTimeout = setTimeout(() => {
                transcriptEl.classList.remove('fading', 'below', 'left-of-orb', 'interim');
                // Reset inline styles so CSS defaults take over
                transcriptEl.style.top = '';
                transcriptEl.style.bottom = '';
                transcriptEl.style.left = '';
                transcriptEl.style.right = '';
            }, 800);
        }
        
        // Convert Float32Array to base64 PCM16
        function floatTo16BitPCM(float32Array) {
            const buffer = new ArrayBuffer(float32Array.length * 2);
            const view = new DataView(buffer);
            for (let i = 0; i < float32Array.length; i++) {
                const s = Math.max(-1, Math.min(1, float32Array[i]));
                view.setInt16(i * 2, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
            }
            return btoa(String.fromCharCode.apply(null, new Uint8Array(buffer)));
        }
        
        // Start listening
        let _listenerGeneration = 0; // Track listener generations for debug

        async function startListening() {
            if (isListening) return;

            _listenerGeneration++;

            // FIX: Remove existing event listener BEFORE creating a new one
            // This prevents listener accumulation when startListening is called
            // after stopListening returned early during TTS playback
            if (removeEventListener) {
                console.log('[Orb] Removing orphaned event listener before re-creating');
                removeEventListener();
                removeEventListener = null;
            }
            
            // Reset dedup state for new session
            lastProcessedTranscript = '';
            lastProcessedTime = 0;
            ttsEndTime = 0;
            // NOTE: pendingSpeakQueue removed - all TTS handled by backend speechQueue
            
            try {
                // Request mic permission
                await window.orbAPI.requestMicPermission();
                
                // Connect to realtime speech API
                const result = await window.orbAPI.connect();
                if (!result.success) {
                    throw new Error(result.error || 'Failed to connect');
                }
                isConnected = true;
                
                // Set up event listener for transcripts and audio
                const _myGeneration = _listenerGeneration;
                removeEventListener = window.orbAPI.onEvent(async (event) => {
                    if (event.type === 'session_updated') {
                        // Session is ready
                        isSessionReady = true;
                        console.log('[Orb] Session ready');
                        // NOTE: pendingSpeak removed - all TTS handled by backend
                    } else if (event.type === 'transcript_delta') {
                        // User is actively speaking - reset speech detection
                        // BUT: during TTS cooldown, interim transcripts are likely mic echo
                        // Don't update speech state or it creates a stuck state
                        // (clears no-speech timer but final transcript gets rejected)
                        const timeSinceTTSDelta = Date.now() - ttsEndTime;
                        if (timeSinceTTSDelta < TTS_COOLDOWN_MS) {
                            // Still show the transcript visually so user sees feedback
                            showTranscript(event.text, true);
                            return;
                        }
                        onSpeechDetected();
                        showTranscript(event.text, true);
                    } else if (event.type === 'function_call_transcript') {
                        // FUNCTION CALLING: AI called our function with the transcript
                        // Process through our agents and respond
                        const transcript = event.transcript;
                        const callId = event.callId;
                        const now = Date.now();
                        
                        // ACTIVE TTS CHECK: Ignore function calls while TTS is playing
                        // Uses centralized speech state from HUD API
                        if (isSpeaking) {
                            console.log('[Orb] Ignoring function call during active TTS playback:', transcript);
                            try {
                                await window.orbAPI.respondToFunction(callId, "");
                            } catch (e) { /* ignore */ }
                            return;
                        }

                        // NOISE CHECK: Ignore background noise that triggered function call
                        if (isLikelyNoise(transcript)) {
                            console.log('[Orb] Ignoring noise in function call:', transcript);
                            try {
                                await window.orbAPI.respondToFunction(callId, "");
                            } catch (e) { /* ignore */ }
                            return;
                        }

                        // COOLDOWN CHECK: Ignore function calls right after TTS
                        const timeSinceTTS = now - ttsEndTime;
                        if (timeSinceTTS < TTS_COOLDOWN_MS) {
                            console.log('[Orb] Ignoring function call during TTS cooldown:', transcript, 'time since TTS:', timeSinceTTS);
                            try {
                                await window.orbAPI.respondToFunction(callId, "");
                            } catch (e) { /* ignore */ }
                            // Safety net: restart no-speech timeout so the orb doesn't get stuck
                            // (interim transcript_delta during cooldown may have cleared it)
                            hasSpokenThisSession = false;
                            startNoSpeechTimeout();
                            return;
                        }
                        
                        // Track this to prevent duplicate processing via regular transcript event
                        lastFunctionCallTranscript = transcript?.toLowerCase() || '';
                        lastFunctionCallTime = now;
                        
                        // Mark that speech ended (final transcript received)
                        onSpeechEnded();
                        
                        console.log('[Orb] Function call received:', transcript, 'callId:', callId);
                        
                        // Store callId for potential multi-turn conversation (needsInput)
                        pendingFunctionCallId = callId;
                        
                        // Show the transcript
                        showTranscript(transcript, false);
                        
                        // ==================== SUBMIT VIA HUD API ====================
                        pendingSubmitCount++;
                        try {
                            if (!window.agentHUD || !window.agentHUD.submitTask) {
                                console.error('[Orb] window.agentHUD not available! Falling back to orbAPI.submit');
                                const fallback = await window.orbAPI.submit(transcript);
                                const fbMsg = fallback?.message || fallback?.response || "I heard you, but couldn't process that.";
                                await window.orbAPI.respondToFunction(callId, fbMsg);
                                pendingFunctionCallId = null;
                                return;
                            }

                            const result = await window.agentHUD.submitTask(transcript, {
                                toolId: 'orb',
                                skipFilter: false,
                            });
                            console.log('[Orb] HUD API result:', JSON.stringify(result));
                            
                            // Handle error returns (submitTask returned but with error)
                            if (!result || result.error) {
                                const errMsg = result?.error || 'Unknown error';
                                console.warn('[Orb] submitTask returned error:', errMsg);
                                // If exchange not running, try the fallback pipeline
                                if (errMsg.includes('Exchange not running') || errMsg.includes('not available')) {
                                    console.log('[Orb] Exchange not ready, using fallback submit');
                                    const fallback = await window.orbAPI.submit(transcript);
                                    const fbMsg = fallback?.message || fallback?.response || "I heard you, but the exchange isn't ready yet.";
                                    await window.orbAPI.respondToFunction(callId, fbMsg);
                                } else {
                                    await window.orbAPI.respondToFunction(callId, result?.message || "I had trouble processing that. Try again.");
                                }
                                pendingFunctionCallId = null;
                                return;
                            }
                            
                            // Async exchange path: events will handle TTS
                            if (result.suppressAIResponse && result.queued) {
                                console.log('[Orb] Async exchange path - events will speak result');
                                await window.orbAPI.respondToFunction(callId, "");
                                pendingFunctionCallId = null;
                                return;
                            }
                            
                            // Transcript was filtered
                            if (result.needsClarification && result.filterReason) {
                                console.log('[Orb] Transcript filtered:', result.filterReason);
                                await window.orbAPI.respondToFunction(callId, result.message || "");
                                pendingFunctionCallId = null;
                                return;
                            }
                            
                            // Multi-turn: agent needs input
                            if (result.needsInput) {
                                console.log('[Orb] Agent needs input (sync), keeping mic active');
                                isWaitingForUserInput = true;
                                const prompt = result.message || result.needsInput?.prompt || "What would you like?";
                                await window.orbAPI.respondToFunction(callId, prompt);
                                pendingFunctionCallId = null;
                            } else {
                                // Standard response
                                let responseText = result.message || "I'm not sure how to help with that.";
                                await window.orbAPI.respondToFunction(callId, responseText);
                                pendingFunctionCallId = null;
                            }
                        } catch (err) {
                            console.error('[Orb] Error processing function call:', err?.message || err);
                            // Log to main process so it's visible in terminal
                            try { window.orbAPI.speak(''); } catch(_){} // no-op to trigger any lazy init
                            await window.orbAPI.respondToFunction(callId, "Sorry, something went wrong. " + (err?.message || ''));
                            pendingFunctionCallId = null;
                        } finally {
                            pendingSubmitCount--;
                            if (pendingSubmitCount === 0 && !isListening && isConnected && !isSpeaking && !isWaitingForUserInput) {
                                console.log('[Orb] All submits resolved, no TTS pending - disconnecting');
                                finishAndDisconnect();
                                updateUI('idle');
                            }
                        }
                        
                        // Hide transcript after a delay
                        setTimeout(hideTranscript, 3000);
                        return; // Don't process as regular transcript
                    } else if (event.type === 'transcript') {
                        const text = event.text?.trim();
                        const now = Date.now();
                        
                        // Skip if TTS is actively playing (prevents echo)
                        if (isSpeaking) {
                            console.log('[Orb] Ignoring transcript during active TTS playback:', text);
                            return;
                        }
                        
                        // Skip if empty or likely background noise
                        if (isLikelyNoise(text)) {
                            console.log('[Orb] Ignoring noise/filler:', text);
                            return;
                        }
                        
                        // CRITICAL: Skip if already processed via function call
                        // With function calling enabled, we receive transcripts twice:
                        // 1. Via function_call_transcript (which we handle)
                        // 2. Via regular transcript (which we must skip)
                        // Normalize by removing punctuation for comparison (e.g., "Play music." vs "play music")
                        const stripPunctuation = (s) => s.replace(/[.,!?;:'"]/g, '').trim();
                        const normalizedFuncCall = stripPunctuation(lastFunctionCallTranscript);
                        const normalizedText = stripPunctuation(text.toLowerCase());
                        if (normalizedText === normalizedFuncCall && (now - lastFunctionCallTime) < FUNCTION_CALL_DEDUP_MS) {
                            console.log('[Orb] Skipping transcript - already handled via function call:', text);
                            return;
                        }
                        
                        // Skip if within TTS cooldown (prevents feedback loop)
                        if (now - ttsEndTime < TTS_COOLDOWN_MS) {
                            console.log('[Orb] Ignoring transcript during TTS cooldown:', text);
                            // Safety net: restart no-speech timeout so the orb doesn't get stuck
                            hasSpokenThisSession = false;
                            startNoSpeechTimeout();
                            return;
                        }
                        
                        // Skip if duplicate within dedup window
                        // normalizedText already defined above
                        const normalizedLast = lastProcessedTranscript.toLowerCase();
                        if (normalizedText === normalizedLast && (now - lastProcessedTime) < DEDUP_WINDOW_MS) {
                            console.log('[Orb] Ignoring duplicate transcript:', text);
                            return;
                        }
                        
                        // Update dedup tracking
                        lastProcessedTranscript = text;
                        lastProcessedTime = now;
                        
                        // Mark that speech ended (final transcript received)
                        onSpeechEnded();
                        
                        showTranscript(text, false);
                        // Submit for classification and show HUD
                        processVoiceCommand(text);
                        // Hide transcript after a delay
                        setTimeout(hideTranscript, 3000);
                    } else if (event.type === 'speech_text_delta') {
                        // Agent is speaking - show text word by word in real-time
                        if (!window._agentResponseText) {
                            window._agentResponseText = '';
                        }
                        window._agentResponseText += event.text;
                        
                        // Show accumulated agent response (different style for agent speech)
                        transcriptEl.classList.add('agent-speaking');
                        showTranscript(window._agentResponseText, false);
                    } else if (event.type === 'speech_text') {
                        // Full agent response text (final)
                        window._agentResponseText = '';  // Reset for next response
                        showTranscript(event.text, false);
                        transcriptEl.classList.add('agent-speaking');
                        // Keep visible longer for agent responses
                        setTimeout(() => {
                            hideTranscript();
                            transcriptEl.classList.remove('agent-speaking');
                        }, 4000);
                    } else if (event.type === 'response_cancelled' || event.type === 'clear_audio_buffer') {
                        // FIX: When a response is cancelled OR we're about to speak something new,
                        // clear any accumulated audio chunks to prevent mixing responses
                        console.log('[Orb] Clearing audio buffer, reason:', event.type);
                        ttsAudioChunks = [];
                        
                        // Clear safety timeout
                        if (speakingTimeoutId) {
                            clearTimeout(speakingTimeoutId);
                            speakingTimeoutId = null;
                        }
                        
                        // FIX: Also reset speaking state when response is cancelled
                        // This prevents getting stuck in speaking state
                        if (isSpeaking) {
                            console.log('[Orb] Resetting speaking state after cancellation');
                            isSpeaking = false;
                            ttsEndTime = Date.now();
                            
                            // NOTE: Backend speech queue handles queuing - no frontend queue
                            
                            // If listening was stopped, now disconnect
                            // BUT if we're in a multi-turn conversation, keep listening instead
                            // AND if there are pending submits, keep alive
                            if (!isListening && isConnected) {
                                if (isWaitingForUserInput) {
                                    console.log('[Orb] Response cancelled but waiting for user input - restarting listening');
                                    startListening();
                                } else if (pendingSubmitCount > 0) {
                                    console.log('[Orb] Response cancelled but', pendingSubmitCount, 'submit(s) pending - keeping connection alive');
                                    updateUI('processing');
                                } else {
                                    console.log('[Orb] Response cancelled, now disconnecting...');
                                    finishAndDisconnect();
                                    updateUI('idle');
                                }
                            }
                        }
                    } else if (event.type === 'audio_delta') {
                        // AUDIO IS HANDLED BY PERSISTENT LISTENER (initPersistentAudioListener)
                        // Skip here to avoid duplicate audio chunks
                        // The persistent listener handles all audio_delta events
                    } else if (event.type === 'audio_done') {
                        // AUDIO PLAYBACK handled by persistent listener
                        // But we still need to update state here
                        
                        // Clear agent response text buffer
                        window._agentResponseText = '';
                        
                        // ALWAYS update ttsEndTime when audio finishes
                        // This handles speech from backend (exchange-bridge) via realtimeSpeech.speak()
                        ttsEndTime = Date.now();
                        console.log('[Orb] TTS ended (state update), cooldown active until:', ttsEndTime + TTS_COOLDOWN_MS);
                        
                        // Reset speaking state
                        if (isSpeaking) {
                            isSpeaking = false;
                            
                            // After TTS, start listening for user's next speech
                            // Reset hasSpokenThisSession so we wait for new speech
                            hasSpokenThisSession = false;
                            startNoSpeechTimeout();
                            
                            // NOTE: Backend speech queue handles queuing - no frontend queue
                        }
                        
                        // If listening was stopped while TTS was playing, now disconnect
                        // BUT if we're in a multi-turn conversation, keep listening instead
                        // AND if there are pending submits (back-to-back commands), keep alive
                        if (!isListening && isConnected) {
                            if (isWaitingForUserInput) {
                                console.log('[Orb] TTS complete but waiting for user input - restarting listening');
                                startListening();
                            } else if (pendingSubmitCount > 0) {
                                console.log('[Orb] TTS complete but', pendingSubmitCount, 'submit(s) pending - keeping connection alive');
                                updateUI('processing');
                            } else {
                                console.log('[Orb] TTS complete, now disconnecting...');
                                finishAndDisconnect();
                                updateUI('idle');
                            }
                        }
                    } else if (event.type === 'reconnecting') {
                        // Backend is attempting to reconnect
                        console.log('[Orb] Backend reconnecting, attempt:', event.attempt, 'delay:', event.delay);
                        updateUI('processing');
                        showTranscript('Reconnecting...', false);
                    } else if (event.type === 'reconnected') {
                        // Backend reconnected successfully - resume listening
                        console.log('[Orb] Backend reconnected successfully');
                        isConnected = true;
                        isSessionReady = false; // Will be set when session_updated fires
                        hideTranscript();
                        updateUI('listening');
                        // Reset speech tracking for the new session
                        hasSpokenThisSession = false;
                        startNoSpeechTimeout();
                    } else if (event.type === 'disconnected') {
                        // Backend WebSocket closed (permanent - reconnects exhausted or user-initiated)
                        console.warn('[Orb] Backend disconnected, code:', event.code, 'permanent:', event.permanent || false);
                        
                        // Force full state reset
                        isConnected = false;
                        isSessionReady = false;
                        isSpeaking = false;
                        isWaitingForUserInput = false;
                        pendingSubmitCount = 0;
                        pendingFunctionCallId = null;
                        pendingDisambiguation = null;
                        ttsAudioChunks = [];
                        hasSpokenThisSession = false;
                        clearAllSpeechTimers();
                        if (speakingTimeoutId) {
                            clearTimeout(speakingTimeoutId);
                            speakingTimeoutId = null;
                        }
                        
                        // Clean up audio resources
                        if (processor) { try { processor.disconnect(); } catch(_){} processor = null; }
                        if (audioContext) { try { audioContext.close(); } catch(_){} audioContext = null; }
                        if (mediaStream) { mediaStream.getTracks().forEach(t => t.stop()); mediaStream = null; }
                        
                        isListening = false;
                        updateUI('idle');
                        console.log('[Orb] State fully reset after backend disconnect');
                    } else if (event.type === 'error') {
                        updateUI('error', event.message || 'Speech error');
                        stopListening();
                    }
                });
                
                // Set up audio capture
                mediaStream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        channelCount: 1,
                        sampleRate: 24000,
                        echoCancellation: true,
                        noiseSuppression: true
                    }
                });
                
                audioContext = new AudioContext({ sampleRate: 24000 });
                const source = audioContext.createMediaStreamSource(mediaStream);
                processor = audioContext.createScriptProcessor(4096, 1, 1);
                
                processor.onaudioprocess = (e) => {
                    if (!isListening) return;
                    const inputData = e.inputBuffer.getChannelData(0);
                    const base64Audio = floatTo16BitPCM(inputData);
                    window.orbAPI.sendAudio(base64Audio);
                };
                
                source.connect(processor);
                processor.connect(audioContext.destination);
                
                isListening = true;
                updateUI('listening');
                console.log('[Orb] Started listening');
                
                // Reset speech tracking for this session
                lastSpeechTime = 0;
                hasSpokenThisSession = false;
                
                // Start no-speech timeout (in case user never speaks)
                startNoSpeechTimeout();
                
                // Play ready chime - a short tone that won't trigger function calling
                // (unlike spoken "Ready" which caused feedback loops)
                playReadyChime();
                
            } catch (error) {
                console.error('[Orb] Start error:', error);
                updateUI('error', error.message);
                stopListening();
            }
        }
        
        // Helper to finish cleanup and disconnect
        async function finishAndDisconnect() {
            // Clean up event listener
            if (removeEventListener) {
                removeEventListener();
                removeEventListener = null;
            }
            
            // Disconnect from API
            if (isConnected) {
                try {
                    await window.orbAPI.disconnect();
                } catch (e) {
                    console.error('[Orb] Disconnect error:', e);
                }
                isConnected = false;
            }
            
            isSessionReady = false;
            
            // Full state cleanup to prevent any lingering state
            pendingFunctionCallId = null;
            isWaitingForUserInput = false;
            pendingDisambiguation = null;
            if (speakingTimeoutId) {
                clearTimeout(speakingTimeoutId);
                speakingTimeoutId = null;
            }
        }
        
        // Stop listening
        async function stopListening() {
            if (!isListening && !isConnected) return;
            
            isListening = false;
            // NOTE: pendingSpeak and pendingSpeakQueue removed - all TTS handled by backend
            
            // Clear all timers
            clearAllSpeechTimers();
            
            // Clean up audio input (microphone)
            if (processor) {
                processor.disconnect();
                processor = null;
            }
            if (audioContext) {
                audioContext.close();
                audioContext = null;
            }
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
                mediaStream = null;
            }
            
            // If TTS is in progress or submits are pending, delay disconnect
            // But still update UI immediately to show we're not listening anymore
            if (isSpeaking || pendingSubmitCount > 0) {
                console.log('[Orb] Delaying disconnect - TTS:', isSpeaking, 'pendingSubmits:', pendingSubmitCount);
                // Event listener stays active to receive audio_done / submit result
                // Disconnect will happen via finishAndDisconnect() after TTS/submit completes
                updateUI('processing');
                
                // Safety: force disconnect after 60s if TTS/submits never resolve
                setTimeout(() => {
                    if (isConnected && !isListening) {
                        console.warn('[Orb] Forced disconnect after delayed stop timeout');
                        isSpeaking = false;
                        pendingSubmitCount = 0;
                        if (speakingTimeoutId) {
                            clearTimeout(speakingTimeoutId);
                            speakingTimeoutId = null;
                        }
                        finishAndDisconnect();
                        updateUI('idle');
                    }
                }, 60000);
                return;
            }
            
            // Clean up event listener and disconnect
            finishAndDisconnect();
            
            updateUI('idle');
            console.log('[Orb] Stopped listening');
        }
        
        // ==========================================================================
        // DRAG SUPPORT
        // ==========================================================================
        
        let isDragging = false;
        let dragStartX = 0;
        let dragStartY = 0;
        let windowStartX = 0;
        let windowStartY = 0;
        let hasMoved = false;
        
        orb.addEventListener('mousedown', (e) => {
            // Only drag on left-click (button 0), not right-click (button 2)
            if (e.button !== 0) return;
            
            isDragging = true;
            hasMoved = false;
            dragStartX = e.screenX;
            dragStartY = e.screenY;
            // Capture window position at start of drag
            windowStartX = window.screenX;
            windowStartY = window.screenY;
            orb.style.cursor = 'grabbing';
            e.preventDefault();
        });
        
        document.addEventListener('mousemove', (e) => {
            if (!isDragging) return;
            
            // Calculate total delta from drag start
            const deltaX = e.screenX - dragStartX;
            const deltaY = e.screenY - dragStartY;
            
            // Only start moving if we've moved more than 5px (to distinguish from clicks)
            if (Math.abs(deltaX) > 5 || Math.abs(deltaY) > 5) {
                hasMoved = true;
                
                // Calculate new position based on initial window position + total delta
                const newX = windowStartX + deltaX;
                const newY = windowStartY + deltaY;
                
                window.orbAPI.setPosition(newX, newY);
            }
        });
        
        document.addEventListener('mouseup', () => {
            if (isDragging) {
                isDragging = false;
                orb.style.cursor = 'pointer';
            }
        });
        
        // Toggle listening on click (only if we didn't drag)
        orb.addEventListener('click', () => {
            if (hasMoved) {
                hasMoved = false;
                return;
            }
            
            // If text chat is open, close it and start voice
            if (isTextChatOpen) {
                closeTextChat();
            }
            
            if (isListening) {
                stopListening();
            } else {
                startListening();
            }
        });
        
        // Right-click to show context menu (only on the orb or its container)
        const orbContainer = document.querySelector('.orb-container');
        orbContainer.addEventListener('contextmenu', (e) => {
            e.preventDefault();
            e.stopPropagation();
            showContextMenu(e.clientX, e.clientY);
        });
        
        // Prevent default context menu on the rest of the document (but don't show our menu)
        document.addEventListener('contextmenu', (e) => {
            e.preventDefault();
        });
        
        // Double-click as alternative to open text chat directly
        orb.addEventListener('dblclick', (e) => {
            console.log('[Orb] Double-click detected');
            e.preventDefault();
            openTextChat();
        });
        
        // Handle window close
        window.addEventListener('beforeunload', () => {
            stopListening();
        });
        
        // Listen for plan summary from Agent Composer
        if (window.orbAPI?.onPlanSummary) {
            window.orbAPI.onPlanSummary(async (data) => {
                console.log('[Orb] Received plan summary:', data.type);
                
                if (data.type === 'plan-ready' && data.summary) {
                    // Speak the plan summary via TTS (using backend)
                    try {
                        await window.orbAPI.speak(data.summary);
                    } catch (e) {
                        console.warn('[Orb] Could not speak plan summary:', e);
                    }
                } else if (data.type === 'creation-complete' && data.agentName) {
                    // Announce agent creation complete - conversational
                    try {
                        await window.orbAPI.speak(`All set! ${data.agentName} is ready to use.`);
                    } catch (e) {
                        console.warn('[Orb] Could not speak completion:', e);
                    }
                }
            });
            console.log('[Orb] Agent Composer plan listener registered');
        }
        
        // =====================================================================
        // Click-through toggle: make transparent areas pass through to windows
        // behind (e.g. clipboard viewer) while keeping the orb itself clickable.
        // setIgnoreMouseEvents(true, { forward: true }) is set in main process
        // at window creation. Here we toggle it off when the cursor enters any
        // interactive element so clicks register, then back on when it leaves.
        // =====================================================================
        (function setupClickThrough() {
            if (!window.orbAPI?.setClickThrough) return;
            
            const interactiveSelectors = ['.orb-container', '.orb-context-menu', '.text-chat-panel'];
            let insideCount = 0;
            
            interactiveSelectors.forEach(sel => {
                const el = document.querySelector(sel);
                if (!el) return;
                el.addEventListener('mouseenter', () => {
                    if (insideCount === 0) {
                        window.orbAPI.setClickThrough(false);
                    }
                    insideCount++;
                });
                el.addEventListener('mouseleave', () => {
                    insideCount--;
                    if (insideCount <= 0) {
                        insideCount = 0;
                        window.orbAPI.setClickThrough(true);
                    }
                });
            });
            
            console.log('[Orb] Click-through toggle initialized');
        })();
        
        // Log ready state
        console.log('[Orb] Voice Orb initialized with drag support and Agent Composer integration');
    </script>
</body>
</html>
